{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Spoc","text":"<p>Spoc is a toolbox for analyzing higher-order contact data from pore-c experiments. It is capable of handling sister-specific contact information to use in the context of sister-sensitive pore-c experiments.</p>"},{"location":"cli/","title":"CLI","text":"<p>Console script for spoc.</p>"},{"location":"cli/#spoc.cli.annotate","title":"<code>annotate(fragments_path, label_library_path, labelled_fragments_path)</code>","text":"<p>Functionality to annotate porec fragments. Adds annotating labels and sister identity of mapped read fragments.</p> <p>Parameters:</p> Name Type Description Default <code>fragments_path</code> <code>str</code> <p>Path to the input fragments file.</p> required <code>label_library_path</code> <code>str</code> <p>Path to the label library file.</p> required <code>labelled_fragments_path</code> <code>str</code> <p>Path to the output labelled fragments file.</p> required Source code in <code>spoc/cli.py</code> <pre><code>@click.command()\n@click.argument(\"fragments_path\")\n@click.argument(\"label_library_path\")\n@click.argument(\"labelled_fragments_path\")\ndef annotate(fragments_path, label_library_path, labelled_fragments_path):\n    \"\"\"\n    Functionality to annotate porec fragments. Adds annotating labels and sister identity of mapped read fragments.\n\n    Args:\n        fragments_path (str): Path to the input fragments file.\n        label_library_path (str): Path to the label library file.\n        labelled_fragments_path (str): Path to the output labelled fragments file.\n\n    \"\"\"\n    file_manager = FileManager()\n    label_library = file_manager.load_label_library(label_library_path)\n    annotator = FragmentAnnotator(label_library)\n    input_fragments = file_manager.load_fragments(fragments_path)\n    result = annotator.annotate_fragments(input_fragments)\n    file_manager.write_fragments(labelled_fragments_path, result)\n</code></pre>"},{"location":"cli/#spoc.cli.bin_contacts","title":"<code>bin_contacts(contact_path, pixel_path, bin_size, same_chromosome)</code>","text":"<p>Functionality to bin contacts.  Bins higher order contacts into genomic bins of fixed size. Contacts path should be an URI.</p> <p>Parameters:</p> Name Type Description Default <code>contact_path</code> <code>str</code> <p>Path to the input contact file.</p> required <code>pixel_path</code> <code>str</code> <p>Path to the output pixel file.</p> required <code>bin_size</code> <code>int</code> <p>Size of the bins. Defaults to 10000.</p> required <code>same_chromosome</code> <code>bool</code> <p>Only bin contacts on the same chromosome. Defaults to False.</p> required Source code in <code>spoc/cli.py</code> <pre><code>@click.command()\n@click.argument(\"contact_path\")\n@click.argument(\"pixel_path\")\n@click.option(\"-b\", \"--bin_size\", default=10_000, type=int)\n@click.option(\"-c\", \"--same_chromosome\", is_flag=True)\ndef bin_contacts(\n    contact_path,\n    pixel_path,\n    bin_size,\n    same_chromosome,\n):\n    \"\"\"\n    Functionality to bin contacts.  Bins higher order contacts into genomic bins of fixed size.\n    Contacts path should be an URI.\n\n    Args:\n        contact_path (str): Path to the input contact file.\n        pixel_path (str): Path to the output pixel file.\n        bin_size (int, optional): Size of the bins. Defaults to 10000.\n        same_chromosome (bool, optional): Only bin contacts on the same chromosome. Defaults to False.\n\n    \"\"\"\n    # load data from disk\n    file_manager = FileManager(DataMode.DASK)\n    contacts = Contacts.from_uri(contact_path)\n    # binning\n    binner = GenomicBinner(bin_size=bin_size)\n    pixels = binner.bin_contacts(contacts, same_chromosome=same_chromosome)\n    # persisting\n    file_manager.write_pixels(pixel_path, pixels)\n</code></pre>"},{"location":"cli/#spoc.cli.expand","title":"<code>expand(fragments_path, expanded_contacts_path, n_fragments)</code>","text":"<p>Functionality to expand labelled fragments to contacts. Expands n-way fragments over sequencing reads to yield contacts.</p> <p>Parameters:</p> Name Type Description Default <code>fragments_path</code> <code>str</code> <p>Path to the labelled fragments file.</p> required <code>expanded_contacts_path</code> <code>str</code> <p>Path to the output contacts file.</p> required <code>n_fragments</code> <code>int</code> <p>Number of fragments per read to expand. Defaults to 3.</p> required Source code in <code>spoc/cli.py</code> <pre><code>@click.command()\n@click.argument(\"fragments_path\")\n@click.argument(\"expanded_contacts_path\")\n@click.option(\n    \"-n\",\n    \"--n_fragments\",\n    default=3,\n    help=\"Number of fragments per read to expand\",\n)\ndef expand(fragments_path, expanded_contacts_path, n_fragments):\n    \"\"\"\n    Functionality to expand labelled fragments to contacts. Expands n-way fragments over sequencing reads\n    to yield contacts.\n\n    Args:\n        fragments_path (str): Path to the labelled fragments file.\n        expanded_contacts_path (str): Path to the output contacts file.\n        n_fragments (int, optional): Number of fragments per read to expand. Defaults to 3.\n\n    \"\"\"\n    expander = FragmentExpander(number_fragments=n_fragments)\n    file_manager = FileManager()\n    input_fragments = file_manager.load_fragments(fragments_path)\n    expanded = expander.expand(input_fragments)\n    file_manager.write_contacts(expanded_contacts_path, expanded)\n</code></pre>"},{"location":"cli/#spoc.cli.main","title":"<code>main()</code>","text":"<p>Console script for spoc.</p> Source code in <code>spoc/cli.py</code> <pre><code>@click.group()\ndef main():\n    \"\"\"Console script for spoc.\"\"\"\n</code></pre>"},{"location":"cli/#spoc.cli.merge","title":"<code>merge()</code>","text":"<p>Functionality to merge files</p> Source code in <code>spoc/cli.py</code> <pre><code>@click.group()\ndef merge():\n    \"\"\"Functionality to merge files\"\"\"\n</code></pre>"},{"location":"cli/#spoc.cli.merge_contacts","title":"<code>merge_contacts(contact_paths, output)</code>","text":"<p>Functionality to merge annotated fragments. Concatenates contacts with the same configuration together and copies contacts with different configurations.</p> <p>Parameters:</p> Name Type Description Default <code>contact_paths</code> <code>tuple</code> <p>Paths to the input contact files.</p> required <code>output</code> <code>str</code> <p>Path to the output merged contact file.</p> required Source code in <code>spoc/cli.py</code> <pre><code>@click.command(name=\"contacts\")\n@click.argument(\"contact_paths\", nargs=-1)\n@click.option(\"-o\", \"--output\", help=\"output path\")\ndef merge_contacts(contact_paths, output):\n    \"\"\"Functionality to merge annotated fragments. Concatenates contacts with the same\n    configuration together and copies contacts with different configurations.\n\n    Args:\n        contact_paths (tuple): Paths to the input contact files.\n        output (str, optional): Path to the output merged contact file.\n    \"\"\"\n    file_manager = FileManager(DataMode.DASK)\n    # get list of parameters\n    parameters = [file_manager.list_contacts(p) for p in contact_paths]\n    # get parameter counts -&gt; if count &gt; 1 then we need to concatenate\n    parameter_counter = {}\n    for file_index, parameter_list in enumerate(parameters):\n        for parameter in parameter_list:\n            if parameter not in parameter_counter:\n                parameter_counter[parameter] = [file_index]\n            else:\n                parameter_counter[parameter].append(file_index)\n    # iterate over parameters and write\n    for parameter, file_indices in parameter_counter.items():\n        if len(file_indices) == 1:\n            file_index = file_indices[0]\n            contacts = file_manager.load_contacts(contact_paths[file_index], parameter)\n            file_manager.write_contacts(output, contacts)\n        else:\n            contact_files = [\n                file_manager.load_contacts(contact_paths[i], parameter)\n                for i in file_indices\n            ]\n            manipulator = ContactManipulator()\n            merged = manipulator.merge_contacts(contact_files)\n            file_manager.write_contacts(output, merged)\n</code></pre>"},{"location":"contacts/","title":"Contacts","text":"<p>Managing multi-way contacts.</p>"},{"location":"contacts/#spoc.contacts.ContactManipulator","title":"<code>ContactManipulator</code>","text":"<p>Responsible for performing operations on contact data such as merging, splitting and subsetting.</p> Source code in <code>spoc/contacts.py</code> <pre><code>class ContactManipulator:\n    \"\"\"Responsible for performing operations on\n    contact data such as merging, splitting and subsetting.\"\"\"\n\n    def merge_contacts(self, merge_list: List[Contacts]) -&gt; Contacts:\n        \"\"\"Merge contacts\n\n        Args:\n            merge_list (List[Contacts]): List of Contacts objects to merge.\n\n        Returns:\n            Contacts: Merged Contacts object.\n        \"\"\"\n        # validate that merge is possible\n        if len({i.number_fragments for i in merge_list}) != 1:\n            raise ValueError(\"All contacts need to have the same order!\")\n        if len({i.data_mode for i in merge_list}) != 1:\n            raise ValueError(\"Mixture of dataframes is not supported!\")\n        # TODO: assert all have same labelling state\n        number_fragments = merge_list[0].number_fragments\n        if merge_list[0].data_mode == DataMode.DASK:\n            return Contacts(\n                dd.concat([i.data for i in merge_list]),\n                number_fragments=number_fragments,\n            )\n        elif merge_list[0].data_mode == DataMode.PANDAS:\n            return Contacts(\n                pd.concat([i.data for i in merge_list]),\n                number_fragments=number_fragments,\n            )\n        else:\n            raise ValueError(\"Merging duckdb relations is not supported!\")\n\n    @staticmethod\n    def _generate_rename_columns(order, start_index=1):\n        columns = [\n            \"chrom\",\n            \"start\",\n            \"end\",\n            \"mapping_quality\",\n            \"align_score\",\n            \"align_base_qscore\",\n            \"metadata\",\n        ]\n        rename_columns = {}\n        for i in range(len(order)):\n            for column in columns:\n                current_name = f\"{column}_{i+start_index}\"\n                new_name = f\"{column}_{order.index(i+start_index) + start_index}\"\n                rename_columns[current_name] = new_name\n        return rename_columns\n\n    @staticmethod\n    def _get_label_combinations(labels, order):\n        sorted_labels = sorted(labels)\n        combinations = set(\n            tuple(sorted(i)) for i in product(sorted_labels, repeat=order)\n        )\n        return combinations\n\n    @staticmethod\n    def _get_combination_splits(combination):\n        splits = []\n        for index, (i, j) in enumerate(zip(combination[:-1], combination[1:])):\n            if i != j:\n                splits.append(index + 2)\n        return [1] + splits + [len(combination) + 1]\n\n    def _flip_unlabelled_contacts(\n        self,\n        df: DataFrame,\n        start_index: Optional[int] = None,\n        end_index: Optional[int] = None,\n    ) -&gt; DataFrame:\n        \"\"\"Flips contacts\"\"\"\n        fragment_order = max(int(i.split(\"_\")[1]) for i in df.columns if \"start\" in i)\n        if start_index is None:\n            start_index = 1\n        if end_index is None:\n            end_index = fragment_order + 1\n        subsets = []\n        for perm in permutations(range(start_index, end_index)):\n            query = \"&lt;=\".join([f\"start_{i}\" for i in perm])\n            subsets.append(\n                df.query(query).rename(\n                    columns=self._generate_rename_columns(perm, start_index)\n                )\n            )\n        # determine which method to use for concatenation\n        if isinstance(df, pd.DataFrame):\n            result = pd.concat(subsets).sort_index()\n            # this is needed if there are reads with equal start positions\n            result = result.loc[~result.index.duplicated(keep=\"first\")]\n        else:\n            result = (\n                dd.concat(subsets)\n                .reset_index()\n                .sort_values(\"index\")\n                .drop_duplicates(subset=[\"index\"])\n                .set_index(\"index\")\n            )\n        return result\n\n    def _flip_labelled_contacts(\n        self, df: DataFrame, label_values: List[str]\n    ) -&gt; DataFrame:\n        \"\"\"Flips labelled contacts\"\"\"\n        fragment_order = max(int(i.split(\"_\")[1]) for i in df.columns if \"start\" in i)\n        label_combinations = self._get_label_combinations(label_values, fragment_order)\n        subsets = []\n        for combination in label_combinations:\n            splits = self._get_combination_splits(combination)\n            # separate out name constanc_columns\n            query = \" and \".join(\n                [f\"metadata_{i} == '{j}'\" for i, j in enumerate(combination, 1)]\n            )\n            candidate_frame = df.query(query)\n            if len(candidate_frame) == 0:\n                continue\n            constant_df, variable_df = candidate_frame[\n                [\"read_name\", \"read_length\"]\n            ], candidate_frame.drop([\"read_name\", \"read_length\"], axis=1)\n            split_frames = [constant_df]\n            for start, end in zip(splits, splits[1:]):\n                # get all columns wiht nubmer between start and\n                subset_columns = [\n                    i\n                    for i in variable_df.columns\n                    if start &lt;= int(i.split(\"_\")[-1]) &lt; end\n                ]\n                # if only columns is present, no need for flipping\n                if start + 1 == end:\n                    split_frame = variable_df[subset_columns]\n                else:\n                    split_frame = self._flip_unlabelled_contacts(\n                        variable_df[subset_columns], start, end\n                    )\n                split_frames.append(split_frame)\n            # concatenate split frames\n            if isinstance(df, pd.DataFrame):\n                subset = pd.concat(split_frames, axis=1)\n            else:\n                subset = dd.concat(split_frames, axis=1)\n            subsets.append(subset)\n        # determine which method to use for concatenation\n        if isinstance(df, pd.DataFrame):\n            result = pd.concat(subsets).sort_index()\n            # this is needed if there are reads with equal start positions\n            result = result.loc[~result.index.duplicated(keep=\"first\")]\n        else:\n            result = (\n                dd.concat(subsets)\n                .reset_index()\n                .sort_values(\"index\")\n                .drop_duplicates(subset=[\"index\"])\n                .set_index(\"index\")\n            )\n        return result\n\n    def sort_labels(self, contacts: Contacts) -&gt; Contacts:\n        \"\"\"Sorts labels in ascending, alphabetical order\n\n        Args:\n            contacts (Contacts): Contacts object to sort.\n\n        Returns:\n            Contacts: Sorted Contacts object.\n        \"\"\"\n        if not contacts.contains_metadata:\n            raise ValueError(\n                \"Sorting labels for unlabelled contacts is not implemented.\"\n            )\n        # get label values.\n        label_values = contacts.get_label_values()\n        # iterate over all permutations of label values\n        subsets = []\n        for perm in product(label_values, repeat=contacts.number_fragments):\n            query = \" and \".join(\n                [f\"metadata_{i+1} == '{j}'\" for i, j in enumerate(perm)]\n            )\n            desired_order = [i + 1 for i in np.argsort(perm)]\n            subsets.append(\n                contacts.data.query(query).rename(\n                    columns=self._generate_rename_columns(desired_order)\n                )\n            )\n        # determine which method to use for concatenation\n        if contacts.data_mode == DataMode.DASK:\n            # this is a bit of a hack to get the index sorted. Dask does not support index sorting\n            result = (\n                dd.concat(subsets).reset_index().sort_values(\"index\").set_index(\"index\")\n            )\n        elif contacts.data_mode == DataMode.PANDAS:\n            result = pd.concat(subsets).sort_index()\n        else:\n            raise ValueError(\"Sorting labels for duckdb relations is not implemented.\")\n        return Contacts(\n            result, number_fragments=contacts.number_fragments, label_sorted=True\n        )\n\n    def _sort_chromosomes(self, df: DataFrame, number_fragments: int) -&gt; DataFrame:\n        \"\"\"Sorts chromosomes in ascending, alphabetical order\"\"\"\n        # iterate over all permutations of chromosomes that exist\n        subsets = []\n        if isinstance(df, dd.DataFrame):\n            chromosome_conbinations = (\n                df[[f\"chrom_{i}\" for i in range(1, number_fragments + 1)]]\n                .drop_duplicates()\n                .compute()\n                .values.tolist()\n            )\n        else:\n            chromosome_conbinations = (\n                df[[f\"chrom_{i}\" for i in range(1, number_fragments + 1)]]\n                .drop_duplicates()\n                .values.tolist()\n            )\n        for perm in chromosome_conbinations:\n            query = \" and \".join([f\"chrom_{i+1} == '{j}'\" for i, j in enumerate(perm)])\n            desired_order = [i + 1 for i in np.argsort(perm, kind=\"stable\")]\n            sorted_frame = df.query(query).rename(\n                columns=self._generate_rename_columns(desired_order)\n            )\n            # ensure correct column order\n            subsets.append(sorted_frame)\n        # determine which method to use for concatenation\n        if isinstance(df, dd.DataFrame):\n            # this is a bit of a hack to get the index sorted. Dask does not support index sorting\n            result = (\n                dd.concat(subsets).reset_index().sort_values(\"index\").set_index(\"index\")\n            )\n        else:\n            result = pd.concat(subsets).sort_index()\n        return result\n\n    def _generate_binary_label_mapping(\n        self, label_values: List[str], number_fragments: int\n    ) -&gt; Dict[Tuple[str, ...], Tuple[str, ...]]:\n        sorted_labels = sorted(label_values)\n        mapping = {}\n        for i in range(number_fragments + 1):\n            target = [sorted_labels[0]] * (number_fragments - i) + [\n                sorted_labels[-1]\n            ] * (i)\n            source = [sorted_labels[0]] * (i) + [sorted_labels[-1]] * (\n                number_fragments - i\n            )\n            if i &lt;= (number_fragments // 2):\n                mapping[tuple(source)] = tuple(target)\n            else:\n                mapping[tuple(source)] = ()\n        return mapping\n\n    def equate_binary_labels(self, contacts: Contacts) -&gt; Contacts:\n        \"\"\"\n        Equate binary labels.\n\n        Binary labels often only carry information about whether\n        they happen between the same or different fragments. This\n        method equates these labels be replacing all equivalent binary labels with\n        the alphabetically first label.\n        For example, if we have a contact between two fragments\n        that are labelled B and B, the label will be replaced with AA.\n\n        Args:\n            contacts (Contacts): Contacts object to equate binary labels.\n\n        Returns:\n            Contacts: Contacts object with equated binary labels.\n\n        \"\"\"\n        assert contacts.contains_metadata, \"Contacts do not contain metadata!\"\n        if not contacts.label_sorted:\n            contacts = self.sort_labels(contacts)\n        # get label values\n        label_values = contacts.get_label_values()\n        assert (\n            len(label_values) == 2\n        ), \"Equate binary labels only works for binary labels!\"\n        # generate mapping diectionary\n        mapping = self._generate_binary_label_mapping(\n            label_values, contacts.number_fragments\n        )\n        subsets = []\n        for source, target in mapping.items():\n            query = \" and \".join(\n                [f\"metadata_{i+1} == '{j}'\" for i, j in enumerate(source)]\n            )\n            subset = contacts.data.query(query)\n            # assign target labels to dataframe\n            for i, j in enumerate(target):\n                subset[f\"metadata_{i+1}\"] = j\n            subsets.append(subset)\n        # determine which method to use for concatenation\n        if contacts.data_mode == DataMode.DASK:\n            # this is a bit of a hack to get the index sorted. Dask does not support index sorting\n            result = (\n                dd.concat(subsets).reset_index().sort_values(\"index\").set_index(\"index\")\n            )\n        elif contacts.data_mode == DataMode.PANDAS:\n            result = pd.concat(subsets).sort_index()\n        else:\n            raise ValueError(\n                \"Equate binary labels for duckdb relations is not implemented.\"\n            )\n        return Contacts(\n            result,\n            number_fragments=contacts.number_fragments,\n            label_sorted=True,\n            binary_labels_equal=True,\n        )\n\n    def subset_on_metadata(\n        self, contacts: Contacts, metadata_combi: List[str]\n    ) -&gt; Contacts:\n        \"\"\"Subset contacts based on metadata\n\n        Args:\n            contacts (Contacts): Contacts object to subset.\n            metadata_combi (List[str]): List of metadata combinations to subset on.\n\n        Returns:\n            Contacts: Subsetted Contacts object.\n\n        \"\"\"\n        # check if metadata is present\n        if not contacts.contains_metadata:\n            raise ValueError(\"Contacts do not contain metadata!\")\n        # check if metadata_combi has the correct length\n        if not len(metadata_combi) == contacts.number_fragments:\n            raise ValueError(\"Metadata combination does not match number of fragments!\")\n        # get label values\n        label_values = contacts.get_label_values()\n        # check if metadata_combi is compatible with label values\n        assert all(\n            i in label_values for i in metadata_combi\n        ), \"Metadata combination is not compatible with label values!\"\n        # subset contacts\n        query = \" and \".join(\n            [f\"metadata_{i+1} == '{j}'\" for i, j in enumerate(metadata_combi)]\n        )\n        result = contacts.data.query(query)\n        return Contacts(\n            result,\n            number_fragments=contacts.number_fragments,\n            metadata_combi=metadata_combi,\n            label_sorted=contacts.label_sorted,\n            binary_labels_equal=contacts.binary_labels_equal,\n            symmetry_flipped=contacts.symmetry_flipped,\n        )\n\n    def flip_symmetric_contacts(\n        self, contacts: Contacts, sort_chromosomes: bool = False\n    ) -&gt; Contacts:\n        \"\"\"Flips contacts based on inherent symmetry\n\n        Args:\n            contacts (Contacts): Contacts object to flip symmetric contacts.\n            sort_chromosomes (bool, optional): Whether to sort chromosomes. Defaults to False.\n\n        Returns:\n            Contacts: Contacts object with flipped symmetric contacts.\n\n        \"\"\"\n        if contacts.contains_metadata:\n            if not contacts.label_sorted:\n                contacts = self.sort_labels(contacts)\n            label_values = contacts.get_label_values()\n            result = self._flip_labelled_contacts(contacts.data, label_values)\n            if sort_chromosomes:\n                result = self._sort_chromosomes(result, contacts.number_fragments)\n            return Contacts(\n                result,\n                number_fragments=contacts.number_fragments,\n                label_sorted=True,\n                binary_labels_equal=contacts.binary_labels_equal,\n                symmetry_flipped=True,\n            )\n        result = self._flip_unlabelled_contacts(contacts.data)\n        if sort_chromosomes:\n            result = self._sort_chromosomes(result, contacts.number_fragments)\n        return Contacts(\n            result,\n            number_fragments=contacts.number_fragments,\n            symmetry_flipped=True,\n        )\n</code></pre>"},{"location":"contacts/#spoc.contacts.ContactManipulator.equate_binary_labels","title":"<code>equate_binary_labels(contacts)</code>","text":"<p>Equate binary labels.</p> <p>Binary labels often only carry information about whether they happen between the same or different fragments. This method equates these labels be replacing all equivalent binary labels with the alphabetically first label. For example, if we have a contact between two fragments that are labelled B and B, the label will be replaced with AA.</p> <p>Parameters:</p> Name Type Description Default <code>contacts</code> <code>Contacts</code> <p>Contacts object to equate binary labels.</p> required <p>Returns:</p> Name Type Description <code>Contacts</code> <code>Contacts</code> <p>Contacts object with equated binary labels.</p> Source code in <code>spoc/contacts.py</code> <pre><code>def equate_binary_labels(self, contacts: Contacts) -&gt; Contacts:\n    \"\"\"\n    Equate binary labels.\n\n    Binary labels often only carry information about whether\n    they happen between the same or different fragments. This\n    method equates these labels be replacing all equivalent binary labels with\n    the alphabetically first label.\n    For example, if we have a contact between two fragments\n    that are labelled B and B, the label will be replaced with AA.\n\n    Args:\n        contacts (Contacts): Contacts object to equate binary labels.\n\n    Returns:\n        Contacts: Contacts object with equated binary labels.\n\n    \"\"\"\n    assert contacts.contains_metadata, \"Contacts do not contain metadata!\"\n    if not contacts.label_sorted:\n        contacts = self.sort_labels(contacts)\n    # get label values\n    label_values = contacts.get_label_values()\n    assert (\n        len(label_values) == 2\n    ), \"Equate binary labels only works for binary labels!\"\n    # generate mapping diectionary\n    mapping = self._generate_binary_label_mapping(\n        label_values, contacts.number_fragments\n    )\n    subsets = []\n    for source, target in mapping.items():\n        query = \" and \".join(\n            [f\"metadata_{i+1} == '{j}'\" for i, j in enumerate(source)]\n        )\n        subset = contacts.data.query(query)\n        # assign target labels to dataframe\n        for i, j in enumerate(target):\n            subset[f\"metadata_{i+1}\"] = j\n        subsets.append(subset)\n    # determine which method to use for concatenation\n    if contacts.data_mode == DataMode.DASK:\n        # this is a bit of a hack to get the index sorted. Dask does not support index sorting\n        result = (\n            dd.concat(subsets).reset_index().sort_values(\"index\").set_index(\"index\")\n        )\n    elif contacts.data_mode == DataMode.PANDAS:\n        result = pd.concat(subsets).sort_index()\n    else:\n        raise ValueError(\n            \"Equate binary labels for duckdb relations is not implemented.\"\n        )\n    return Contacts(\n        result,\n        number_fragments=contacts.number_fragments,\n        label_sorted=True,\n        binary_labels_equal=True,\n    )\n</code></pre>"},{"location":"contacts/#spoc.contacts.ContactManipulator.flip_symmetric_contacts","title":"<code>flip_symmetric_contacts(contacts, sort_chromosomes=False)</code>","text":"<p>Flips contacts based on inherent symmetry</p> <p>Parameters:</p> Name Type Description Default <code>contacts</code> <code>Contacts</code> <p>Contacts object to flip symmetric contacts.</p> required <code>sort_chromosomes</code> <code>bool</code> <p>Whether to sort chromosomes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Contacts</code> <code>Contacts</code> <p>Contacts object with flipped symmetric contacts.</p> Source code in <code>spoc/contacts.py</code> <pre><code>def flip_symmetric_contacts(\n    self, contacts: Contacts, sort_chromosomes: bool = False\n) -&gt; Contacts:\n    \"\"\"Flips contacts based on inherent symmetry\n\n    Args:\n        contacts (Contacts): Contacts object to flip symmetric contacts.\n        sort_chromosomes (bool, optional): Whether to sort chromosomes. Defaults to False.\n\n    Returns:\n        Contacts: Contacts object with flipped symmetric contacts.\n\n    \"\"\"\n    if contacts.contains_metadata:\n        if not contacts.label_sorted:\n            contacts = self.sort_labels(contacts)\n        label_values = contacts.get_label_values()\n        result = self._flip_labelled_contacts(contacts.data, label_values)\n        if sort_chromosomes:\n            result = self._sort_chromosomes(result, contacts.number_fragments)\n        return Contacts(\n            result,\n            number_fragments=contacts.number_fragments,\n            label_sorted=True,\n            binary_labels_equal=contacts.binary_labels_equal,\n            symmetry_flipped=True,\n        )\n    result = self._flip_unlabelled_contacts(contacts.data)\n    if sort_chromosomes:\n        result = self._sort_chromosomes(result, contacts.number_fragments)\n    return Contacts(\n        result,\n        number_fragments=contacts.number_fragments,\n        symmetry_flipped=True,\n    )\n</code></pre>"},{"location":"contacts/#spoc.contacts.ContactManipulator.merge_contacts","title":"<code>merge_contacts(merge_list)</code>","text":"<p>Merge contacts</p> <p>Parameters:</p> Name Type Description Default <code>merge_list</code> <code>List[Contacts]</code> <p>List of Contacts objects to merge.</p> required <p>Returns:</p> Name Type Description <code>Contacts</code> <code>Contacts</code> <p>Merged Contacts object.</p> Source code in <code>spoc/contacts.py</code> <pre><code>def merge_contacts(self, merge_list: List[Contacts]) -&gt; Contacts:\n    \"\"\"Merge contacts\n\n    Args:\n        merge_list (List[Contacts]): List of Contacts objects to merge.\n\n    Returns:\n        Contacts: Merged Contacts object.\n    \"\"\"\n    # validate that merge is possible\n    if len({i.number_fragments for i in merge_list}) != 1:\n        raise ValueError(\"All contacts need to have the same order!\")\n    if len({i.data_mode for i in merge_list}) != 1:\n        raise ValueError(\"Mixture of dataframes is not supported!\")\n    # TODO: assert all have same labelling state\n    number_fragments = merge_list[0].number_fragments\n    if merge_list[0].data_mode == DataMode.DASK:\n        return Contacts(\n            dd.concat([i.data for i in merge_list]),\n            number_fragments=number_fragments,\n        )\n    elif merge_list[0].data_mode == DataMode.PANDAS:\n        return Contacts(\n            pd.concat([i.data for i in merge_list]),\n            number_fragments=number_fragments,\n        )\n    else:\n        raise ValueError(\"Merging duckdb relations is not supported!\")\n</code></pre>"},{"location":"contacts/#spoc.contacts.ContactManipulator.sort_labels","title":"<code>sort_labels(contacts)</code>","text":"<p>Sorts labels in ascending, alphabetical order</p> <p>Parameters:</p> Name Type Description Default <code>contacts</code> <code>Contacts</code> <p>Contacts object to sort.</p> required <p>Returns:</p> Name Type Description <code>Contacts</code> <code>Contacts</code> <p>Sorted Contacts object.</p> Source code in <code>spoc/contacts.py</code> <pre><code>def sort_labels(self, contacts: Contacts) -&gt; Contacts:\n    \"\"\"Sorts labels in ascending, alphabetical order\n\n    Args:\n        contacts (Contacts): Contacts object to sort.\n\n    Returns:\n        Contacts: Sorted Contacts object.\n    \"\"\"\n    if not contacts.contains_metadata:\n        raise ValueError(\n            \"Sorting labels for unlabelled contacts is not implemented.\"\n        )\n    # get label values.\n    label_values = contacts.get_label_values()\n    # iterate over all permutations of label values\n    subsets = []\n    for perm in product(label_values, repeat=contacts.number_fragments):\n        query = \" and \".join(\n            [f\"metadata_{i+1} == '{j}'\" for i, j in enumerate(perm)]\n        )\n        desired_order = [i + 1 for i in np.argsort(perm)]\n        subsets.append(\n            contacts.data.query(query).rename(\n                columns=self._generate_rename_columns(desired_order)\n            )\n        )\n    # determine which method to use for concatenation\n    if contacts.data_mode == DataMode.DASK:\n        # this is a bit of a hack to get the index sorted. Dask does not support index sorting\n        result = (\n            dd.concat(subsets).reset_index().sort_values(\"index\").set_index(\"index\")\n        )\n    elif contacts.data_mode == DataMode.PANDAS:\n        result = pd.concat(subsets).sort_index()\n    else:\n        raise ValueError(\"Sorting labels for duckdb relations is not implemented.\")\n    return Contacts(\n        result, number_fragments=contacts.number_fragments, label_sorted=True\n    )\n</code></pre>"},{"location":"contacts/#spoc.contacts.ContactManipulator.subset_on_metadata","title":"<code>subset_on_metadata(contacts, metadata_combi)</code>","text":"<p>Subset contacts based on metadata</p> <p>Parameters:</p> Name Type Description Default <code>contacts</code> <code>Contacts</code> <p>Contacts object to subset.</p> required <code>metadata_combi</code> <code>List[str]</code> <p>List of metadata combinations to subset on.</p> required <p>Returns:</p> Name Type Description <code>Contacts</code> <code>Contacts</code> <p>Subsetted Contacts object.</p> Source code in <code>spoc/contacts.py</code> <pre><code>def subset_on_metadata(\n    self, contacts: Contacts, metadata_combi: List[str]\n) -&gt; Contacts:\n    \"\"\"Subset contacts based on metadata\n\n    Args:\n        contacts (Contacts): Contacts object to subset.\n        metadata_combi (List[str]): List of metadata combinations to subset on.\n\n    Returns:\n        Contacts: Subsetted Contacts object.\n\n    \"\"\"\n    # check if metadata is present\n    if not contacts.contains_metadata:\n        raise ValueError(\"Contacts do not contain metadata!\")\n    # check if metadata_combi has the correct length\n    if not len(metadata_combi) == contacts.number_fragments:\n        raise ValueError(\"Metadata combination does not match number of fragments!\")\n    # get label values\n    label_values = contacts.get_label_values()\n    # check if metadata_combi is compatible with label values\n    assert all(\n        i in label_values for i in metadata_combi\n    ), \"Metadata combination is not compatible with label values!\"\n    # subset contacts\n    query = \" and \".join(\n        [f\"metadata_{i+1} == '{j}'\" for i, j in enumerate(metadata_combi)]\n    )\n    result = contacts.data.query(query)\n    return Contacts(\n        result,\n        number_fragments=contacts.number_fragments,\n        metadata_combi=metadata_combi,\n        label_sorted=contacts.label_sorted,\n        binary_labels_equal=contacts.binary_labels_equal,\n        symmetry_flipped=contacts.symmetry_flipped,\n    )\n</code></pre>"},{"location":"contacts/#spoc.contacts.Contacts","title":"<code>Contacts</code>","text":"<p>N-way genomic contacts</p> <p>Parameters:</p> Name Type Description Default <code>contact_frame</code> <code>DataFrame</code> <p>DataFrame containing the contact data.</p> required <code>number_fragments</code> <code>int</code> <p>Number of fragments. Defaults to None.</p> <code>None</code> <code>metadata_combi</code> <code>List[str]</code> <p>List of metadata combinations. Defaults to None.</p> <code>None</code> <code>label_sorted</code> <code>bool</code> <p>Whether the labels are sorted. Defaults to False.</p> <code>False</code> <code>binary_labels_equal</code> <code>bool</code> <p>Whether the binary labels are equal. Defaults to False.</p> <code>False</code> <code>symmetry_flipped</code> <code>bool</code> <p>Whether the symmetry is flipped. Defaults to False.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>contains_metadata</code> <code>bool</code> <p>Whether the contact data contains metadata.</p> <code>number_fragments</code> <code>int</code> <p>Number of fragments.</p> <code>is_dask</code> <code>bool</code> <p>Whether the contact data is a Dask DataFrame.</p> <code>metadata_combi</code> <code>List[str]</code> <p>List of metadata combinations.</p> <code>label_sorted</code> <code>bool</code> <p>Whether the labels are sorted.</p> <code>binary_labels_equal</code> <code>bool</code> <p>Whether the binary labels are equal.</p> <code>symmetry_flipped</code> <code>bool</code> <p>Whether the symmetry is flipped.</p> Source code in <code>spoc/contacts.py</code> <pre><code>class Contacts:\n    \"\"\"N-way genomic contacts\n\n    Args:\n        contact_frame (DataFrame): DataFrame containing the contact data.\n        number_fragments (int, optional): Number of fragments. Defaults to None.\n        metadata_combi (List[str], optional): List of metadata combinations. Defaults to None.\n        label_sorted (bool, optional): Whether the labels are sorted. Defaults to False.\n        binary_labels_equal (bool, optional): Whether the binary labels are equal. Defaults to False.\n        symmetry_flipped (bool, optional): Whether the symmetry is flipped. Defaults to False.\n\n    Attributes:\n        contains_metadata (bool): Whether the contact data contains metadata.\n        number_fragments (int): Number of fragments.\n        is_dask (bool): Whether the contact data is a Dask DataFrame.\n        metadata_combi (List[str]): List of metadata combinations.\n        label_sorted (bool): Whether the labels are sorted.\n        binary_labels_equal (bool): Whether the binary labels are equal.\n        symmetry_flipped (bool): Whether the symmetry is flipped.\n    \"\"\"\n\n    def __init__(\n        self,\n        contact_frame: DataFrame,\n        number_fragments: Optional[int] = None,\n        metadata_combi: Optional[List[str]] = None,\n        label_sorted: bool = False,\n        binary_labels_equal: bool = False,\n        symmetry_flipped: bool = False,\n    ) -&gt; None:\n        self.contains_metadata = (\n            \"metadata_1\" in contact_frame.columns\n        )  # All contacts contain at least one fragment\n        if number_fragments is None:\n            self.number_fragments = self._guess_number_fragments(contact_frame)\n        else:\n            self.number_fragments = number_fragments\n        self._schema = ContactSchema(\n            number_fragments=self.number_fragments,\n            contains_metadata=self.contains_metadata,\n        )\n        # TODO: make this work for duckdb pyrelation -&gt; switch to mode\n        if isinstance(contact_frame, pd.DataFrame):\n            self.data_mode = DataMode.PANDAS\n        elif isinstance(contact_frame, dd.DataFrame):\n            self.data_mode = DataMode.DASK\n        elif isinstance(contact_frame, duckdb.DuckDBPyRelation):\n            self.data_mode = DataMode.DUCKDB\n        else:\n            raise ValueError(\"Unknown data mode!\")\n        self._data = self._schema.validate(contact_frame)\n        self.metadata_combi = metadata_combi\n        self.label_sorted = label_sorted\n        self.binary_labels_equal = binary_labels_equal\n        self.symmetry_flipped = symmetry_flipped\n\n    @staticmethod\n    def from_uri(uri, mode=DataMode.PANDAS):\n        \"\"\"Construct contacts from uri.\n        Will match parameters based on the following order:\n\n        PATH::number_fragments::metadata_combi::binary_labels_equal::symmetry_flipped::label_sorted\n\n        Path, number_fragments are required. The rest is optional\n        and will be tried to match to the available contacts. If no match is found, or there is no\n         unique match, an error is raised.\n        Mode can be one of pandas|dask, which corresponds to the type of the pixel source.\n        \"\"\"\n        # import here to avoid circular imports\n        from spoc.io import FileManager\n\n        return FileManager(mode).load_contacts(uri)\n\n    def get_global_parameters(self) -&gt; ContactsParameters:\n        \"\"\"Returns global parameters\"\"\"\n        return ContactsParameters(\n            number_fragments=self.number_fragments,\n            metadata_combi=self.metadata_combi,\n            label_sorted=self.label_sorted,\n            binary_labels_equal=self.binary_labels_equal,\n            symmetry_flipped=self.symmetry_flipped,\n        )\n\n    def get_schema(self) -&gt; GenomicDataSchema:\n        \"\"\"Returns the schema of the underlying data\"\"\"\n        return self._schema\n\n    def _guess_number_fragments(self, contact_frame: DataFrame) -&gt; int:\n        \"\"\"Guesses the number of fragments from the contact frame\"\"\"\n        return max(int(i.split(\"_\")[1]) for i in contact_frame.columns if \"start\" in i)\n\n    def get_label_values(self) -&gt; List[str]:\n        \"\"\"Returns all label values\"\"\"\n        # TODO: This could be put in global metadata of parquet file\n        if not self.contains_metadata:\n            raise ValueError(\"Contacts do not contain metadata!\")\n        output = set()\n        for i in range(self.number_fragments):\n            if self.data_mode == DataMode.DASK:\n                output.update(self.data[f\"metadata_{i+1}\"].unique().compute())\n            elif self.data_mode == DataMode.PANDAS:\n                output.update(self.data[f\"metadata_{i+1}\"].unique())\n            else:\n                raise ValueError(\"Label values not supported for duckdb!\")\n        return list(output)\n\n    def get_chromosome_values(self) -&gt; List[str]:\n        \"\"\"Returns all chromosome values\"\"\"\n        # TODO: This could be put in global metadata of parquet file\n        output = set()\n        for i in range(self.number_fragments):\n            if self.data_mode == DataMode.DASK:\n                output.update(self.data[f\"chrom_{i+1}\"].unique().compute())\n            elif self.data_mode == DataMode.PANDAS:\n                output.update(self.data[f\"chrom_{i+1}\"].unique())\n            else:\n                raise ValueError(\"Chromosome values not supported for duckdb!\")\n        return list(output)\n\n    @property\n    def data(self):\n        \"\"\"Returns the contact data\"\"\"\n        return self._data\n\n    @data.setter\n    def data(self, contact_frame):\n        \"\"\"Sets the contact data\n\n        Args:\n            contact_frame (DataFrame): DataFrame containing the contact data.\n        \"\"\"\n        self._data = self._schema.validate(contact_frame)\n\n    def __repr__(self) -&gt; str:\n        return f\"&lt;Contacts | order: {self.number_fragments} | contains metadata: {self.contains_metadata}&gt;\"\n</code></pre>"},{"location":"contacts/#spoc.contacts.Contacts.data","title":"<code>data</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the contact data</p>"},{"location":"contacts/#spoc.contacts.Contacts.from_uri","title":"<code>from_uri(uri, mode=DataMode.PANDAS)</code>  <code>staticmethod</code>","text":"<p>Construct contacts from uri. Will match parameters based on the following order:</p> <p>PATH::number_fragments::metadata_combi::binary_labels_equal::symmetry_flipped::label_sorted</p> <p>Path, number_fragments are required. The rest is optional and will be tried to match to the available contacts. If no match is found, or there is no  unique match, an error is raised. Mode can be one of pandas|dask, which corresponds to the type of the pixel source.</p> Source code in <code>spoc/contacts.py</code> <pre><code>@staticmethod\ndef from_uri(uri, mode=DataMode.PANDAS):\n    \"\"\"Construct contacts from uri.\n    Will match parameters based on the following order:\n\n    PATH::number_fragments::metadata_combi::binary_labels_equal::symmetry_flipped::label_sorted\n\n    Path, number_fragments are required. The rest is optional\n    and will be tried to match to the available contacts. If no match is found, or there is no\n     unique match, an error is raised.\n    Mode can be one of pandas|dask, which corresponds to the type of the pixel source.\n    \"\"\"\n    # import here to avoid circular imports\n    from spoc.io import FileManager\n\n    return FileManager(mode).load_contacts(uri)\n</code></pre>"},{"location":"contacts/#spoc.contacts.Contacts.get_chromosome_values","title":"<code>get_chromosome_values()</code>","text":"<p>Returns all chromosome values</p> Source code in <code>spoc/contacts.py</code> <pre><code>def get_chromosome_values(self) -&gt; List[str]:\n    \"\"\"Returns all chromosome values\"\"\"\n    # TODO: This could be put in global metadata of parquet file\n    output = set()\n    for i in range(self.number_fragments):\n        if self.data_mode == DataMode.DASK:\n            output.update(self.data[f\"chrom_{i+1}\"].unique().compute())\n        elif self.data_mode == DataMode.PANDAS:\n            output.update(self.data[f\"chrom_{i+1}\"].unique())\n        else:\n            raise ValueError(\"Chromosome values not supported for duckdb!\")\n    return list(output)\n</code></pre>"},{"location":"contacts/#spoc.contacts.Contacts.get_global_parameters","title":"<code>get_global_parameters()</code>","text":"<p>Returns global parameters</p> Source code in <code>spoc/contacts.py</code> <pre><code>def get_global_parameters(self) -&gt; ContactsParameters:\n    \"\"\"Returns global parameters\"\"\"\n    return ContactsParameters(\n        number_fragments=self.number_fragments,\n        metadata_combi=self.metadata_combi,\n        label_sorted=self.label_sorted,\n        binary_labels_equal=self.binary_labels_equal,\n        symmetry_flipped=self.symmetry_flipped,\n    )\n</code></pre>"},{"location":"contacts/#spoc.contacts.Contacts.get_label_values","title":"<code>get_label_values()</code>","text":"<p>Returns all label values</p> Source code in <code>spoc/contacts.py</code> <pre><code>def get_label_values(self) -&gt; List[str]:\n    \"\"\"Returns all label values\"\"\"\n    # TODO: This could be put in global metadata of parquet file\n    if not self.contains_metadata:\n        raise ValueError(\"Contacts do not contain metadata!\")\n    output = set()\n    for i in range(self.number_fragments):\n        if self.data_mode == DataMode.DASK:\n            output.update(self.data[f\"metadata_{i+1}\"].unique().compute())\n        elif self.data_mode == DataMode.PANDAS:\n            output.update(self.data[f\"metadata_{i+1}\"].unique())\n        else:\n            raise ValueError(\"Label values not supported for duckdb!\")\n    return list(output)\n</code></pre>"},{"location":"contacts/#spoc.contacts.Contacts.get_schema","title":"<code>get_schema()</code>","text":"<p>Returns the schema of the underlying data</p> Source code in <code>spoc/contacts.py</code> <pre><code>def get_schema(self) -&gt; GenomicDataSchema:\n    \"\"\"Returns the schema of the underlying data\"\"\"\n    return self._schema\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at spoc's issue page.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>spoc could always use more documentation, whether as part of the official spoc docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue here.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make implementation easier.</li> <li>Remember that this is a volunteer-driven project and that contributions   are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>spoc</code> for local development.</p> <p>TODO</p>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into the relevant markdown file in the docs folder.</li> </ol>"},{"location":"data_structures/","title":"Spoc data structures","text":"<p>This notebook explains the data structures that are availabel within spoc and shows how they relate to each other. On a high level, spoc provides data structures for all parts of the transformation pipline from raw reads to aggregated pixels. </p> <p>Often, these data structures (with the exception of the pixels class) will not be used within every day analysis tasks, but rather within analysis pipelines.</p>"},{"location":"data_structures/#data-frame-schemas","title":"Data frame schemas","text":"<p>Spoc data structures are wrappers around tabular data containers such as <code>panda.DataFrame</code> or <code>dask.dataframe.DataFrame</code>. To ensure that the underlying data complies with the format that spoc expects, spoc implements dataframe validation using <code>pandera</code>. The underlying schemas reside in the <code>spoc.dataframe_models</code> file.</p>"},{"location":"data_structures/#io","title":"I/O","text":"<p>Reading and writing of spoc data structures is managed by the <code>spoc.io</code> package, specifically by the <code>FileManager</code> class. Examples of using the FileManager can be found with the specific data structure.</p>"},{"location":"data_structures/#fragments","title":"Fragments","text":"<p>Fragments encapsulate a data structure that can hold a dynamic number of aligned fragments per sequencing unit. In a Pore-C experiment, a sequencing unit is the sequencing read that holds multiple fragments per read. In theory, this structure can also be used for other experiment types that generate aligned fragments that are grouped together by an id, for exapmle SPRITE</p> <p>Reading fragments using <code>FileManager</code></p> <pre><code>from spoc.io import FileManager\n</code></pre> <pre><code>fragments = FileManager().load_fragments(\"../tests/test_files/good_porec.parquet\")\n</code></pre> <p>Fragments class has data accessor for fragments</p> <pre><code>fragments.data.head()\n</code></pre> chrom start end strand read_name read_start read_end read_length mapping_quality align_score align_base_qscore pass_filter 0 chr1 1 4 True dummy 1 4 1 1 1 1 True 1 chr1 2 5 True dummy 2 5 1 2 2 2 True 2 chr1 3 6 True dummy 3 6 1 3 3 3 True <p>The fragments class constructor validates the underlying data structure using pandera and the dataframe schemas in <code>spoc.dataframe_models</code></p> <pre><code>from pandera.errors import SchemaError\n</code></pre> <pre><code>try:\n    FileManager().load_fragments(\"../tests/test_files/bad_porec.parquet\")\nexcept SchemaError as e:\n    print(str(e).split(\"\\n\")[0])\n</code></pre> <p>Fragments class also supports reading as dask dataframe</p> <pre><code>from spoc.io import DataMode\nfragments = FileManager(DataMode.DASK).load_fragments(\"../tests/test_files/good_porec.parquet\")\n</code></pre> <pre><code>fragments.data\n</code></pre> Dask DataFrame Structure: chrom start end strand read_name read_start read_end read_length mapping_quality align_score align_base_qscore pass_filter npartitions=1 object int64 int64 bool object int64 int64 int64 int64 int64 int64 bool ... ... ... ... ... ... ... ... ... ... ... ... Dask Name: validate, 2 graph layers <p>Note that if reading from dask dataframes, schema evaluation is deferred until the dask taskgraph is evaluated</p> <pre><code>fragments = FileManager(DataMode.DASK).load_fragments(\"../tests/test_files/bad_porec.parquet\")\n</code></pre> <pre><code>try:\n    fragments.data.compute()\nexcept SchemaError as e:\n    print(str(e).split(\"\\n\")[0])\n</code></pre> <pre><code>column 'chrom' not in dataframe\n</code></pre>"},{"location":"data_structures/#annotating-fragments","title":"Annotating fragments","text":"<p>Fragments can carry metadata that add additional information, which can be propagated in the analysis pipeline. <code>FragmentAnnotator</code> uses a dictionary called label library that contains compound fragment ids and metainformation to annotate fragments. These ids are concatenations of the read_id, chromosome, start and end of the mapping.</p> <pre><code>fragments = FileManager().load_fragments(\"../tests/test_files/good_porec.parquet\")\n</code></pre> <pre><code>label_library = FileManager().load_label_library(\"../tests/test_files/ll1.pickle\")\n</code></pre> <pre><code>label_library\n</code></pre> <pre><code>{'dummy_chr1_1_4': True, 'dummy_chr1_2_5': False}\n</code></pre> <pre><code>from spoc.fragments import FragmentAnnotator\n</code></pre> <pre><code>annotated_fragments = FragmentAnnotator(label_library).annotate_fragments(fragments)\n</code></pre> <pre><code>annotated_fragments.data.head()\n</code></pre> chrom start end strand read_name read_start read_end read_length mapping_quality align_score align_base_qscore pass_filter metadata 0 chr1 1 4 True dummy 1 4 1 1 1 1 True SisterB 1 chr1 2 5 True dummy 2 5 1 2 2 2 True SisterA"},{"location":"data_structures/#contacts","title":"Contacts","text":"<p>While the fragment representation retains flexibility, it is often not practical to have contacts of multiple orders and types in different rows of the same file. To this end, we employ the contact representation, where each row contains one contact of a defined order, e.g. a duplet, or a triplet. The <code>Contact</code> class is a wrapper around the data structure that holds this representation. The <code>Contacts</code> class is a generic interface that can represent different orders. The class that creates contacts from fragments is called <code>FragmentExpander</code>, which can be used to generate contacts of arbitrary order.</p> <pre><code>import pandas as pd\nfrom spoc.fragments import FragmentExpander\n</code></pre> <pre><code>fragments = FileManager().load_fragments(\"../tests/test_files/fragments_unlabelled.parquet\")\n</code></pre> <pre><code>fragments.data.head()\n</code></pre> chrom start end strand read_name read_start read_end read_length mapping_quality align_score align_base_qscore pass_filter 0 chr1 1 4 True dummy 1 4 1 1 1 1 True 1 chr1 2 5 True dummy 2 5 1 2 2 2 True 2 chr1 3 6 True dummy 3 6 1 3 3 3 True 3 chr1 4 7 True dummy 4 7 1 4 4 4 True 4 chr1 5 8 True dummy2 5 8 1 5 5 5 True <pre><code>contacts = FragmentExpander(number_fragments=2).expand(fragments)\n</code></pre> <pre><code>contacts.data.head()\n</code></pre> read_name read_length chrom_1 start_1 end_1 mapping_quality_1 align_score_1 align_base_qscore_1 chrom_2 start_2 end_2 mapping_quality_2 align_score_2 align_base_qscore_2 0 dummy 1 chr1 1 4 1 1 1 chr1 2 5 2 2 2 1 dummy 1 chr1 1 4 1 1 1 chr1 3 6 3 3 3 2 dummy 1 chr1 1 4 1 1 1 chr1 4 7 4 4 4 3 dummy 1 chr1 2 5 2 2 2 chr1 3 6 3 3 3 4 dummy 1 chr1 2 5 2 2 2 chr1 4 7 4 4 4 <p>Fragment expander also allows us to deal with metadata that is associated with fragments</p> <pre><code>fragments_labelled = FileManager().load_fragments(\"../tests/test_files/fragments_labelled.parquet\")\n</code></pre> <pre><code>contacts_labelled = FragmentExpander(number_fragments=2).expand(fragments_labelled)\n</code></pre> <pre><code>contacts_labelled.data.head()\n</code></pre> read_name read_length chrom_1 start_1 end_1 mapping_quality_1 align_score_1 align_base_qscore_1 metadata_1 chrom_2 start_2 end_2 mapping_quality_2 align_score_2 align_base_qscore_2 metadata_2 0 dummy 1 chr1 1 4 1 1 1 SisterA chr1 2 5 2 2 2 SisterB 1 dummy 1 chr1 1 4 1 1 1 SisterA chr1 3 6 3 3 3 SisterA 2 dummy 1 chr1 1 4 1 1 1 SisterA chr1 4 7 4 4 4 SisterB 3 dummy 1 chr1 2 5 2 2 2 SisterB chr1 3 6 3 3 3 SisterA 4 dummy 1 chr1 2 5 2 2 2 SisterB chr1 4 7 4 4 4 SisterB <p>The contact class retains the information as to whether the expanded contacts contain metadata</p> <pre><code>contacts_labelled.contains_metadata\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"data_structures/#persisting-contacts","title":"Persisting contacts","text":"<p>Contacts can be persisted using the file manager, which writes the data as well as the gobal parameters to a parquet file. Loading the file restores the global parameters.</p> <pre><code>contacts_labelled.get_global_parameters()\n</code></pre> <pre><code>ContactsParameters(number_fragments=2, metadata_combi=None, label_sorted=False, binary_labels_equal=False, symmetry_flipped=False)\n</code></pre> <pre><code>FileManager().write_contacts(\"test.parquet\", contacts_labelled)\n</code></pre> <p>The contacts fileformat is a directory containing different specific contact instances such as contacts with different contact order etc. We can specify which of those to load by bassing the contacts parameters</p> <pre><code>contacts = FileManager().load_contacts(\"test.parquet\", contacts_labelled.get_global_parameters())\n</code></pre> <pre><code>contacts.get_global_parameters()\n</code></pre> <pre><code>ContactsParameters(number_fragments=2, metadata_combi=None, label_sorted=False, binary_labels_equal=False, symmetry_flipped=False)\n</code></pre> <p>Additionally, contacts can be loaded using a URI that specifies the parameters seperated by <code>::</code>. This method of loading requires the parameters path and number_fragments but will match the rest to the available pixels.</p> <pre><code>from spoc.contacts import Contacts\n</code></pre> <pre><code>loaded_contacts = Contacts.from_uri('test.parquet::2')\n</code></pre> <pre><code>loaded_contacts.get_global_parameters()\n</code></pre> <pre><code>ContactsParameters(number_fragments=2, metadata_combi=None, label_sorted=False, binary_labels_equal=False, symmetry_flipped=False)\n</code></pre>"},{"location":"data_structures/#symmetry","title":"Symmetry","text":""},{"location":"data_structures/#unlabelled-contacts","title":"Unlabelled contacts","text":"<p>The quantification of genomic interactions in conventional (2-way) Hi-C assumes that there is no difference in the order of interactions. This means that whether a genomic location is measured in the first read or second read of a paired-end sequencing experiment carries the same information. This means that during preprocessing, conventional Hi-C data is flipped based on some convention (often that the first read has a smaller genomic location based on some sort order) and then only the upper triangular interaction matrix is stored.</p> <p>When we talk about higher genomic order, a similar reasoning can apply (except for special use-cases) and we thus can flip genomic contacts such that genomic coordinates are monotonically increasing from lower to higher order (we mean this order if we refer to flipping below). This produces a symmetric, high-dimensional tensor, meaning that every permutation of dimensions does not change the associated value.</p> <p>In <code>spoc</code>, this logic is implemented in the <code>ContactManipulator</code> class, in the <code>.flip_symmetric_contacts</code> method.</p> <pre><code>from spoc.contacts import Contacts\n</code></pre> <pre><code>contacts = Contacts.from_uri(\"../tests/test_files/contacts_unlabelled_2d_v2.parquet::2\")\n</code></pre> <pre><code>contacts.data.head().filter(regex=\"(read_name|start|end)\")\n</code></pre> read_name start_1 end_1 start_2 end_2 0 read1 100 200 1000 2000 1 read2 2000 3000 200 300 2 read3 3000 4000 300 400 <p>This particular contacts dataframe has one contact that conforms with the convention that the first contact should be smaller than the second (<code>read1</code>), whereas the other two contacts don't conform with that convention. Using the <code>.flip_symmetric_contacts</code> method we can fix this:</p> <pre><code>from spoc.contacts import ContactManipulator\n</code></pre> <pre><code>flipped_contacts = ContactManipulator().flip_symmetric_contacts(contacts)\n</code></pre> <pre><code>flipped_contacts.data.head().filter(regex=\"(read_name|start|end)\")\n</code></pre> read_name start_1 end_1 start_2 end_2 0 read1 100 200 1000 2000 1 read2 200 300 2000 3000 2 read3 300 400 3000 4000 <p>Symmetry flipped contacts have the flag <code>symmetry_flipped</code> set to true</p> <pre><code>flipped_contacts.symmetry_flipped\n</code></pre> <pre><code>True\n</code></pre> <p>These operations are available for arbitrary contact cardinalities:</p> <pre><code>contacts = Contacts.from_uri(\"../tests/test_files/contacts_unlabelled_3d_v2.parquet::3\")\n</code></pre> <pre><code>contacts.data.head().filter(regex=\"(read_name|start|end)\")\n</code></pre> read_name start_1 end_1 start_2 end_2 start_3 end_3 0 read1 100 200 1000 2000 250 300 1 read2 2000 3000 200 300 400 500 2 read3 3000 4000 300 400 100 200 <pre><code>flipped_contacts = ContactManipulator().flip_symmetric_contacts(contacts)\nflipped_contacts.data.head().filter(regex=\"(read_name|start|end)\")\n</code></pre> read_name start_1 end_1 start_2 end_2 start_3 end_3 0 read1 100 200 250 300 1000 2000 1 read2 200 300 400 500 2000 3000 2 read3 100 200 300 400 3000 4000"},{"location":"data_structures/#labelled-contacts","title":"Labelled contacts","text":"<p>For labelled contacts, the situation is more complex as we have to deal with different flavours of symmetry. If we take the example of triplets, that is genomic contacts of order 3, and binary labels (denoted as A or B), there are 8 possible contact orders: <pre><code> (AAA, BBB, BAA, ABA, AAB, ABB, BAB, BBA)\n</code></pre></p> <p>If we extend the argument that order of interactions is unimportant, this reduces to 4 possible arrangements of labels:</p> <pre><code> (AAA, BBB, ABB, BAA)\n</code></pre> <p>It is often the case that for binary contacts, the specific label type is unimportant, the only important information is whether the labels where different (e.g. for sister specific labels or homologous chromosome labels). In such a situation, the possible arrangements reduce further to 2 possible label types:</p> <pre><code> (AAA, AAB)\n</code></pre> <p>For those contact types, two different \u201crules\u201d for symmetry apply, for the situation of all similar labels (AAA or BBB), the same rules apply as for unlabelled contacts as there is no difference in order. For the other situation (ABB or BBA, which we denote as ABB from now on), only permutations within one label type produce the same value, meaning that if we have the label state ABB, and denote permutations as tuples of length three with (0,1,2) being the identity permutation then only the permutations (0,1,2) and (0,2,1) are identical. Practically, this means that we can flip contacts that are related by these permutations such that their genomic coordinates are monotonically increasing, but we cannot do this for contacts that are not related through a symmetry relation. </p> <p>This reasoning can be generalized to higher dimensions, where contacts can be flipped if they can be related via a symmetry relation. Practically speaking, this means that we have a higher-order contact with two possible label states of order n with k labels of type A and (n-k) labels of type B, we can flip the labels within type A and within type B. For example, if we have a contact of order 4 with the configuration AABB, we can flip within A and within B based on genomic coordinates, but not within them. This reasoning also applies to the situation where we have more than one possible label state. Also here, we can flip within one label state, but not between them.</p> <p>This logic is implemented in <code>spoc</code> in the <code>ContactManipulator</code> class, with to methods:</p> <ul> <li>The <code>.equate_binary_labels</code> method can be used to specify whether in a binary label situation, the labels shold be different or not (e.g. whether AA is equivaltent to BB) or not. This method is optional and can be used prior to the flipping procedure.</li> <li>The <code>.flip_symmetric_contacts</code> method flips symmetric contacts base don the rules specified above</li> </ul> <p>Note that all symmetry operations require the contact metadata to be alphabetically sorted, this can be either done explicitely via the <code>.sort_labels</code> method, or is performed automatically within the other methods. For example, the metadat order of <code>ABA</code> will be converted to <code>AAB</code> by the <code>.sort_labels</code> operation.</p> <p>Let's look at an example! Here, we have 3d contacts that have binary labels</p> <pre><code>contacts = Contacts.from_uri(\"../tests/test_files/contacts_labelled_3d_v2.parquet::3\")\n</code></pre> <pre><code>contacts.data.head().filter(regex=\"(read_name|start|end|metadata)\")\n</code></pre> read_name start_1 end_1 metadata_1 start_2 end_2 metadata_2 start_3 end_3 metadata_3 0 read1 100 200 A 200 300 B 1000 2000 B 1 read2 5000 5500 A 2000 3000 A 200 300 B 2 read3 800 900 B 3000 3200 B 3000 4000 B <pre><code>contacts.number_fragments, contacts.get_label_values()\n</code></pre> <pre><code>(3, {'A', 'B'})\n</code></pre> <p>This dataframe contains three contacts, with the following label state: <pre><code>(ABB)\n(AAB)\n(BBB)\n</code></pre> In this analysis use case, we want to equate the binary labels since we don't have a biological reason to belive that there is any difference between the labels. The only information that is important for us is whether the contacts happened between different label states or the same label state. Therefore, we use the <code>.equate_binary_labels</code> method to replace all occurences of the same contact combination with their alphabetically first example. For example, the label state <code>(ABB)</code> is a contact, where two parts come from one label state and one part comes from another. In our logic, this equivalent to <code>(AAB)</code>, which is the alphabetically first label, which we therefore use to replace it. Following the same logic, <code>(BBB)</code> will be replaced by <code>(AAA)</code>.</p> <pre><code>equated_contacts = ContactManipulator().equate_binary_labels(contacts)\n</code></pre> <pre><code>/users/michael.mitter/.conda/envs/spoc-dev/lib/python3.8/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  self._set_item(key, value)\n</code></pre> <pre><code>equated_contacts.data.head().filter(regex=\"(read_name|start|end|metadata)\")\n</code></pre> read_name start_1 end_1 metadata_1 start_2 end_2 metadata_2 start_3 end_3 metadata_3 0 read1 100 200 A 200 300 A 1000 2000 B 1 read2 5000 5500 A 2000 3000 A 200 300 B 2 read3 800 900 A 3000 3200 A 3000 4000 A <p>As you can see, the occurence of <code>(ABB)</code> has been replaced by <code>(AAB)</code> and the occurence of <code>(BBB)</code> has been replaced by <code>(AAA)</code>. We can now reduce the symmetry of these contacts based on the logic explained above using the <code>.flip_symmetric_contacts</code> method.</p> <pre><code>flipped_labelled_contacts = ContactManipulator().flip_symmetric_contacts(equated_contacts)\n</code></pre> <pre><code>flipped_labelled_contacts.data.head().filter(regex=\"(read_name|start|end|metadata)\")\n</code></pre> read_name start_1 end_1 metadata_1 start_2 end_2 metadata_2 start_3 end_3 metadata_3 0 read1 100 200 A 200 300 A 1000 2000 B 1 read2 2000 3000 A 5000 5500 A 200 300 B 2 read3 800 900 A 3000 3200 A 3000 4000 A"},{"location":"data_structures/#pixels","title":"Pixels","text":""},{"location":"data_structures/#concept","title":"Concept","text":"<p>Pixels represent aggregated information that count the number of genomic contacts per genomic bin. In this context, a genomic bin is a genomic interval with a fixed size (for example 10 kb) and a genomic contact is defined as above as an interaction between the corresponding bins. Pixels represent a single contact order, binsize and labelling state (if metadata is provided) and are thought to be the central datastructure that an analyst will use to interact with multiway genomic data to answer questions about genomic structure.</p>"},{"location":"data_structures/#implementation","title":"Implementation","text":"<p>Within <code>spoc</code>, pixel instances can be generated from genomic contacts through the <code>GenomicBinner</code> class.</p> <pre><code>from spoc.pixels import GenomicBinner\n</code></pre> <pre><code>contacts = Contacts.from_uri(\"../tests/test_files/contacts_for_pixels_3d_v2.parquet::3\")\n</code></pre> <pre><code>contacts.data.head().filter(regex=\"(read_name|chrom|start|end|metadata)\")\n</code></pre> read_name chrom_1 start_1 end_1 metadata_1 chrom_2 start_2 end_2 metadata_2 chrom_3 start_3 end_3 metadata_3 0 a chr1 100010 100015 SisterA chr1 500010 500050 SisterB chr1 600100 600200 SisterB 1 b chr1 5000010 5000050 SisterB chr1 7000050 7000070 SisterA chr4 2000300 2000400 SisterA 2 c chr1 10000010 10000050 SisterA chr1 25000800 25000900 SisterB chr1 6000050 6000600 SisterA 3 d chr1 10000010 10000050 SisterA chr1 25001000 25002000 SisterA chr1 6000010 6000700 SisterB <p><code>GenomicBinner</code> takes a binsize and aggregates contacts by counting the number of contacts that fall within a genomic bin. As with all the other preprocessing functionalities, <code>GenomicBinner</code> can take arguments as either pandas dataframes or dask dataframes. The output is consistent, meaning that when passing a pandas dataframe, a pandas dataframe is returned and if passing a dask dataframe, a dask dataframe is returned.</p> <pre><code>pixels = GenomicBinner(bin_size=100_000).bin_contacts(contacts)\n</code></pre> <p>The default behavior of genomic binner is to filter for contacts that are on the same chromosome and produce an intrachromosomal pixels instance. This behavior allows us to create a compact representation of pixels, which only store one chromosome field per pixel.</p> <pre><code>pixels.data.head()\n</code></pre> chrom start_1 start_2 start_3 count 0 chr1 100000 500000 600000 1 1 chr1 10000000 25000000 6000000 2 <p>The pixels class additionally contains all information associated with the data stored. This data is taken from the contacts object passed to genomic binner.</p> <pre><code>pixels.number_fragments, pixels.binsize, pixels.binary_labels_equal, pixels.symmetry_flipped, pixels.metadata_combi\n</code></pre> <pre><code>(3, 100000, False, False, None)\n</code></pre> <p>If interchromosomal contacts are needed, this can be passed to the <code>.bin_contacts</code> method of genomic binner and will cause the pixel schema to incorporate chromosome columns for each of the corresponding pixel dimensions.</p> <pre><code>pixels_w_inter = GenomicBinner(bin_size=100_000).bin_contacts(contacts, same_chromosome=False)\n</code></pre> <pre><code>pixels_w_inter.data.head()\n</code></pre> chrom_1 start_1 chrom_2 start_2 chrom_3 start_3 count 0 chr1 100000 chr1 500000 chr1 600000 1 1 chr1 10000000 chr1 25000000 chr1 6000000 2 2 chr1 5000000 chr1 7000000 chr4 2000000 1"},{"location":"data_structures/#lablled-contacts","title":"Lablled contacts","text":"<p>The concept of pixels representing a single labeling state allows simplification of the interfaces to downstream processing functionality, but offloads responsibiltiy to the analyst to ensure that the biological question at hand is adequately adressed by the chosen labelling state. The pixels class does not perform any labelling state checks and forwards the combination(s) of labelling states present in the contacts class used for their construction. This allows for greater flexibility with regards to the questions that can be answered. </p> <p>The <code>ContactManipulator</code> class contains functionality to filter contacts for a given labelling state to perform downstream aggregation of pixels.</p> <pre><code>contacts.data.head().filter(regex=\"(read_name|chrom|start|end|metadata)\")\n</code></pre> read_name chrom_1 start_1 end_1 metadata_1 chrom_2 start_2 end_2 metadata_2 chrom_3 start_3 end_3 metadata_3 0 a chr1 100010 100015 SisterA chr1 500010 500050 SisterB chr1 600100 600200 SisterB 1 b chr1 5000010 5000050 SisterB chr1 7000050 7000070 SisterA chr4 2000300 2000400 SisterA 2 c chr1 10000010 10000050 SisterA chr1 25000800 25000900 SisterB chr1 6000050 6000600 SisterA 3 d chr1 10000010 10000050 SisterA chr1 25001000 25002000 SisterA chr1 6000010 6000700 SisterB <pre><code>filtered_contacts = ContactManipulator().subset_on_metadata(contacts, metadata_combi=['SisterA', 'SisterB', 'SisterB'])\n</code></pre> <pre><code>filtered_contacts.data.head().filter(regex=\"(read_name|chrom|start|end|metadata)\")\n</code></pre> read_name chrom_1 start_1 end_1 metadata_1 chrom_2 start_2 end_2 metadata_2 chrom_3 start_3 end_3 metadata_3 0 a chr1 100010 100015 SisterA chr1 500010 500050 SisterB chr1 600100 600200 SisterB <p>The resulting contacts carry the filtered metadata combination as an attribute:</p> <pre><code>filtered_contacts.metadata_combi\n</code></pre> <pre><code>['SisterA', 'SisterB', 'SisterB']\n</code></pre> <p><code>GenomicBinner</code> carries this information forward and adds it to the pixels object</p> <pre><code>pixels_filtered = GenomicBinner(bin_size=100_000).bin_contacts(filtered_contacts)\n</code></pre> <pre><code>pixels_filtered.data.head()\n</code></pre> chrom start_1 start_2 start_3 count 0 chr1 100000 500000 600000 1 <pre><code>pixels_filtered.metadata_combi\n</code></pre> <pre><code>['SisterA', 'SisterB', 'SisterB']\n</code></pre>"},{"location":"data_structures/#persisting-pixels","title":"Persisting pixels","text":"<p>Pixels come in a multitude of different flavors and it would thus be cumbsersome to have to keep track of pixels of different binsize, labelling state or contact order that belong to the same source fragments. We thus developed a pixel file format that exposes a unified name to pixels that belong together as well as functoinality to load the exact pixels that we want.</p>"},{"location":"data_structures/#the-pixel-file-format","title":"The pixel file format","text":"<p>The pixel file format is a directory that contains a number of parquet files as well as a config file that holds information about what specific pixels are available. As with the other io-operations, the <code>FileManager</code> object is responsible for saving pixels.</p> <pre><code>import os\n</code></pre> <pre><code>FileManager().write_pixels(\"test_pixels.parquet\", pixels)\n</code></pre> <pre><code>os.listdir('test_pixels.parquet')\n</code></pre> <pre><code>['5f0e4d6a6c0e5afb82c3a6ec4bce1635.parquet', 'metadata.json']\n</code></pre> <ul> <li>The pixel foler at <code>test.parquet</code> contains a single specific pixel file as well as a metadata file</li> </ul> <p>The <code>FileManager</code> class contains functionality to read the metadata file and list available pixels</p> <pre><code>FileManager().list_pixels(\"test_pixels.parquet\")\n</code></pre> <pre><code>[PixelParameters(number_fragments=3, binsize=100000, metadata_combi=None, label_sorted=False, binary_labels_equal=False, symmetry_flipped=False, same_chromosome=True)]\n</code></pre> <ul> <li>The <code>list_pixels</code> method lists all available pixels in the form of <code>PixelParameters</code>, a pydantic dataclass that holds the parameters that describe a specific pixel file</li> </ul> <p>Writing additional pixels updates the metadata file</p> <pre><code>pixels_filtered._metadata_combi = ['A', 'B', 'B'] # make metadata shorter\n</code></pre> <pre><code>FileManager().write_pixels(\"test_pixels.parquet\", pixels_filtered)\n</code></pre> <pre><code>FileManager().list_pixels(\"test_pixels.parquet\")\n</code></pre> <pre><code>[PixelParameters(number_fragments=3, binsize=100000, metadata_combi=None, label_sorted=False, binary_labels_equal=False, symmetry_flipped=False, same_chromosome=True),\n PixelParameters(number_fragments=3, binsize=100000, metadata_combi=['A', 'B', 'B'], label_sorted=False, binary_labels_equal=False, symmetry_flipped=False, same_chromosome=True)]\n</code></pre>"},{"location":"data_structures/#loading-pixels","title":"Loading pixels","text":"<p>Pixel files can be loaded using the parameter pydantic class</p> <pre><code>from spoc.file_parameter_models import PixelParameters\n</code></pre> <pre><code>loaded_pixels = FileManager().load_pixels(\"test_pixels.parquet\", PixelParameters(number_fragments=3, binsize=100_000, same_chromosome=True))\n</code></pre> <p>Additionally, pixels can be loaded using a URI that specifies the parameters seperated by <code>::</code>. This method of loading requires the parameters path, number_fragments and binsize, but will match the rest to the available pixels.</p> <pre><code>from spoc.pixels import Pixels\n</code></pre> <pre><code>loaded_pixels = Pixels.from_uri('test_pixels.parquet::3::100000::ABB')\n</code></pre> <pre><code>loaded_pixels.get_global_parameters()\n</code></pre> <pre><code>PixelParameters(number_fragments=3, binsize=100000, metadata_combi=['A', 'B', 'B'], label_sorted=False, binary_labels_equal=False, symmetry_flipped=False, same_chromosome=True)\n</code></pre> <p>If the specified uri is not specific enough and multiple pixels match, an error is raised</p>"},{"location":"dataframe_models/","title":"Dataframe models","text":"<p>Dataframe models</p>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema","title":"<code>ContactSchema</code>","text":"<p>Dynamic schema for N-way contacts</p> <p>Parameters:</p> Name Type Description Default <code>number_fragments</code> <code>int</code> <p>Number of fragments. Defaults to 3.</p> <code>3</code> <code>contains_metadata</code> <code>bool</code> <p>Whether the contact data contains metadata. Defaults to True.</p> <code>True</code> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>class ContactSchema:\n    \"\"\"Dynamic schema for N-way contacts\n\n    Args:\n        number_fragments (int, optional): Number of fragments. Defaults to 3.\n        contains_metadata (bool, optional): Whether the contact data contains metadata. Defaults to True.\n    \"\"\"\n\n    # field groups\n\n    common_fields = {\n        \"read_name\": pa.Column(str),\n        \"read_length\": pa.Column(int),\n    }\n\n    contact_fields = {\n        \"chrom\": pa.Column(str),\n        \"start\": pa.Column(int),\n        \"end\": pa.Column(int),\n        \"mapping_quality\": pa.Column(int),\n        \"align_score\": pa.Column(int),\n        \"align_base_qscore\": pa.Column(int),\n        \"metadata\": pa.Column(str, required=False),\n    }\n\n    def __init__(\n        self, number_fragments: int = 3, contains_metadata: bool = True\n    ) -&gt; None:\n        self._number_fragments = number_fragments\n        self._schema = pa.DataFrameSchema(\n            dict(\n                self.common_fields,\n                **self._expand_contact_fields(\n                    range(1, number_fragments + 1), contains_metadata\n                ),\n            ),\n            coerce=True,\n        )\n\n    @classmethod\n    def get_contact_fields(cls, contains_metadata: bool) -&gt; Dict:\n        \"\"\"returns contact fields\n\n        Args:\n            contains_metadata (bool): Whether the contact data contains metadata.\n\n        Returns:\n            Dict: Dictionary containing the contact fields.\n        \"\"\"\n        if contains_metadata:\n            return copy.deepcopy(cls.contact_fields)\n        return {\n            key: value\n            for key, value in copy.deepcopy(cls.contact_fields).items()\n            if key not in [\"metadata\"]\n        }\n\n    def _expand_contact_fields(\n        self, expansions: Iterable = (1, 2, 3), contains_metadata: bool = True\n    ) -&gt; dict:\n        \"\"\"adds suffixes to fields\"\"\"\n        output = {}\n        for i in expansions:\n            for key, value in self.get_contact_fields(contains_metadata).items():\n                output[key + f\"_{i}\"] = value\n        return output\n\n    def validate_header(self, data_frame: DataFrame) -&gt; None:\n        \"\"\"Validates only header, needed to validate that dask taskgraph can be built before\n        evaluation.\n\n        Args:\n            data_frame (DataFrame): The DataFrame to validate.\n        \"\"\"\n        for column in data_frame.columns:\n            if column not in self._schema.columns:\n                raise pa.errors.SchemaError(\n                    self._schema, data_frame, \"Header is invalid!\"\n                )\n\n    def get_schema(self) -&gt; pa.DataFrameSchema:\n        \"\"\"\n        Get the schema of the DataFrame.\n\n        Returns:\n            pa.DataFrameSchema: The schema of the DataFrame.\n        \"\"\"\n        return self._schema\n\n    def get_position_fields(self) -&gt; Dict[int, List[str]]:\n        \"\"\"Returns the position fields as a dictionary\n        of framgent index to the respective fields\"\"\"\n        return {\n            i: [f\"chrom_{i}\", f\"start_{i}\", f\"end_{i}\"]\n            for i in range(1, self._number_fragments + 1)\n        }\n\n    def get_contact_order(self) -&gt; int:\n        \"\"\"Returns the order of the genomic data\"\"\"\n        return self._number_fragments\n\n    def validate(self, data_frame: DataFrame) -&gt; DataFrame:\n        \"\"\"Validate multiway contact dataframe\n\n        Args:\n            data_frame (DataFrame): The DataFrame to validate.\n        \"\"\"\n        self.validate_header(data_frame)\n        if isinstance(data_frame, duckdb.DuckDBPyRelation):\n            # duckdb does not support schema validation\n            return data_frame\n        return self._schema.validate(data_frame)\n\n    def get_binsize(self) -&gt; Optional[int]:\n        \"\"\"Returns the binsize of the genomic data\"\"\"\n        return None\n\n    def get_region_number(self) -&gt; Optional[int]:\n        \"\"\"Returns the number of regions in the genomic data\n        if present.\"\"\"\n        return None\n\n    def get_half_window_size(self) -&gt; Optional[int]:\n        \"\"\"Returns the window size of the genomic data\n        if present.\"\"\"\n        return None\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.get_binsize","title":"<code>get_binsize()</code>","text":"<p>Returns the binsize of the genomic data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_binsize(self) -&gt; Optional[int]:\n    \"\"\"Returns the binsize of the genomic data\"\"\"\n    return None\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.get_contact_fields","title":"<code>get_contact_fields(contains_metadata)</code>  <code>classmethod</code>","text":"<p>returns contact fields</p> <p>Parameters:</p> Name Type Description Default <code>contains_metadata</code> <code>bool</code> <p>Whether the contact data contains metadata.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Dictionary containing the contact fields.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>@classmethod\ndef get_contact_fields(cls, contains_metadata: bool) -&gt; Dict:\n    \"\"\"returns contact fields\n\n    Args:\n        contains_metadata (bool): Whether the contact data contains metadata.\n\n    Returns:\n        Dict: Dictionary containing the contact fields.\n    \"\"\"\n    if contains_metadata:\n        return copy.deepcopy(cls.contact_fields)\n    return {\n        key: value\n        for key, value in copy.deepcopy(cls.contact_fields).items()\n        if key not in [\"metadata\"]\n    }\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.get_contact_order","title":"<code>get_contact_order()</code>","text":"<p>Returns the order of the genomic data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_contact_order(self) -&gt; int:\n    \"\"\"Returns the order of the genomic data\"\"\"\n    return self._number_fragments\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.get_half_window_size","title":"<code>get_half_window_size()</code>","text":"<p>Returns the window size of the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_half_window_size(self) -&gt; Optional[int]:\n    \"\"\"Returns the window size of the genomic data\n    if present.\"\"\"\n    return None\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.get_position_fields","title":"<code>get_position_fields()</code>","text":"<p>Returns the position fields as a dictionary of framgent index to the respective fields</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_position_fields(self) -&gt; Dict[int, List[str]]:\n    \"\"\"Returns the position fields as a dictionary\n    of framgent index to the respective fields\"\"\"\n    return {\n        i: [f\"chrom_{i}\", f\"start_{i}\", f\"end_{i}\"]\n        for i in range(1, self._number_fragments + 1)\n    }\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.get_region_number","title":"<code>get_region_number()</code>","text":"<p>Returns the number of regions in the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_region_number(self) -&gt; Optional[int]:\n    \"\"\"Returns the number of regions in the genomic data\n    if present.\"\"\"\n    return None\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.get_schema","title":"<code>get_schema()</code>","text":"<p>Get the schema of the DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrameSchema</code> <p>pa.DataFrameSchema: The schema of the DataFrame.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_schema(self) -&gt; pa.DataFrameSchema:\n    \"\"\"\n    Get the schema of the DataFrame.\n\n    Returns:\n        pa.DataFrameSchema: The schema of the DataFrame.\n    \"\"\"\n    return self._schema\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.validate","title":"<code>validate(data_frame)</code>","text":"<p>Validate multiway contact dataframe</p> <p>Parameters:</p> Name Type Description Default <code>data_frame</code> <code>DataFrame</code> <p>The DataFrame to validate.</p> required Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def validate(self, data_frame: DataFrame) -&gt; DataFrame:\n    \"\"\"Validate multiway contact dataframe\n\n    Args:\n        data_frame (DataFrame): The DataFrame to validate.\n    \"\"\"\n    self.validate_header(data_frame)\n    if isinstance(data_frame, duckdb.DuckDBPyRelation):\n        # duckdb does not support schema validation\n        return data_frame\n    return self._schema.validate(data_frame)\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.ContactSchema.validate_header","title":"<code>validate_header(data_frame)</code>","text":"<p>Validates only header, needed to validate that dask taskgraph can be built before evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>data_frame</code> <code>DataFrame</code> <p>The DataFrame to validate.</p> required Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def validate_header(self, data_frame: DataFrame) -&gt; None:\n    \"\"\"Validates only header, needed to validate that dask taskgraph can be built before\n    evaluation.\n\n    Args:\n        data_frame (DataFrame): The DataFrame to validate.\n    \"\"\"\n    for column in data_frame.columns:\n        if column not in self._schema.columns:\n            raise pa.errors.SchemaError(\n                self._schema, data_frame, \"Header is invalid!\"\n            )\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.DataMode","title":"<code>DataMode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enum for data mode</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>class DataMode(Enum):\n    \"\"\"Enum for data mode\"\"\"\n\n    PANDAS = auto()\n    DASK = auto()\n    DUCKDB = auto()\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.GenomicDataSchema","title":"<code>GenomicDataSchema</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Protocol for genomic data schema to be used in the query engine</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>class GenomicDataSchema(Protocol):\n    \"\"\"Protocol for genomic data schema\n    to be used in the query engine\"\"\"\n\n    def get_position_fields(self) -&gt; Dict[int, List[str]]:\n        \"\"\"Returns the position fields as a dictionary\n        of framgent index to the respective fields\"\"\"\n\n    def get_contact_order(self) -&gt; int:\n        \"\"\"Returns the order of the genomic data\"\"\"\n\n    def get_schema(self) -&gt; pa.DataFrameSchema:\n        \"\"\"Return the schema of the underlying data\"\"\"\n\n    def get_binsize(self) -&gt; Optional[int]:\n        \"\"\"Returns the binsize of the genomic data\"\"\"\n\n    def get_region_number(self) -&gt; Optional[int]:\n        \"\"\"Returns the number of regions in the genomic data\n        if present.\"\"\"\n\n    def get_half_window_size(self) -&gt; Optional[int]:\n        \"\"\"Returns the window size of the genomic data\n        if present.\"\"\"\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.GenomicDataSchema.get_binsize","title":"<code>get_binsize()</code>","text":"<p>Returns the binsize of the genomic data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_binsize(self) -&gt; Optional[int]:\n    \"\"\"Returns the binsize of the genomic data\"\"\"\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.GenomicDataSchema.get_contact_order","title":"<code>get_contact_order()</code>","text":"<p>Returns the order of the genomic data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_contact_order(self) -&gt; int:\n    \"\"\"Returns the order of the genomic data\"\"\"\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.GenomicDataSchema.get_half_window_size","title":"<code>get_half_window_size()</code>","text":"<p>Returns the window size of the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_half_window_size(self) -&gt; Optional[int]:\n    \"\"\"Returns the window size of the genomic data\n    if present.\"\"\"\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.GenomicDataSchema.get_position_fields","title":"<code>get_position_fields()</code>","text":"<p>Returns the position fields as a dictionary of framgent index to the respective fields</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_position_fields(self) -&gt; Dict[int, List[str]]:\n    \"\"\"Returns the position fields as a dictionary\n    of framgent index to the respective fields\"\"\"\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.GenomicDataSchema.get_region_number","title":"<code>get_region_number()</code>","text":"<p>Returns the number of regions in the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_region_number(self) -&gt; Optional[int]:\n    \"\"\"Returns the number of regions in the genomic data\n    if present.\"\"\"\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.GenomicDataSchema.get_schema","title":"<code>get_schema()</code>","text":"<p>Return the schema of the underlying data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_schema(self) -&gt; pa.DataFrameSchema:\n    \"\"\"Return the schema of the underlying data\"\"\"\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema","title":"<code>PixelSchema</code>","text":"<p>Dynamic schema for N-way pixels</p> <p>Parameters:</p> Name Type Description Default <code>number_fragments</code> <code>int</code> <p>Number of fragments. Defaults to 3.</p> <code>3</code> <code>same_chromosome</code> <code>bool</code> <p>Whether the fragments are on the same chromosome. Defaults to True.</p> <code>True</code> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>class PixelSchema:\n    \"\"\"Dynamic schema for N-way pixels\n\n    Args:\n        number_fragments (int, optional): Number of fragments. Defaults to 3.\n        same_chromosome (bool, optional): Whether the fragments are on the same chromosome. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        number_fragments: int = 3,\n        same_chromosome: bool = True,\n        binsize: Optional[int] = None,\n    ) -&gt; None:\n        self._number_fragments = number_fragments\n        self._same_chromosome = same_chromosome\n        self._binsize = binsize\n        self._schema = pa.DataFrameSchema(\n            dict(\n                self._get_constant_fields(),\n                **self._expand_contact_fields(range(1, number_fragments + 1)),\n            ),\n            coerce=True,\n        )\n\n    def _get_contact_fields(self):\n        if self._same_chromosome:\n            return {\n                \"start\": pa.Column(int),\n            }\n        return {\"chrom\": pa.Column(str), \"start\": pa.Column(int)}\n\n    def _get_constant_fields(self):\n        if self._same_chromosome:\n            return {\n                \"chrom\": pa.Column(str),\n                \"count\": pa.Column(int),\n                \"corrected_count\": pa.Column(float, required=False),\n            }\n        return {\n            \"count\": pa.Column(int),\n            \"corrected_count\": pa.Column(float, required=False),\n        }\n\n    def _expand_contact_fields(self, expansions: Iterable = (1, 2, 3)) -&gt; dict:\n        \"\"\"adds suffixes to fields\"\"\"\n        output = {}\n        for i in expansions:\n            for key, value in self._get_contact_fields().items():\n                output[key + f\"_{i}\"] = value\n        return output\n\n    def validate_header(self, data_frame: DataFrame) -&gt; None:\n        \"\"\"Validates only header, needed to validate that dask taskgraph can be built before\n        evaluation\n\n        Args:\n            data_frame (DataFrame): The DataFrame to validate.\n        \"\"\"\n        for column in data_frame.columns:\n            if column not in self._schema.columns:\n                raise pa.errors.SchemaError(\n                    self._schema, data_frame, \"Header is invalid!\"\n                )\n\n    def get_schema(self) -&gt; pa.DataFrameSchema:\n        \"\"\"\n        Get the schema of the DataFrame.\n\n        Returns:\n            pa.DataFrameSchema: The schema of the DataFrame.\n        \"\"\"\n        return self._schema\n\n    def get_position_fields(self) -&gt; Dict[int, List[str]]:\n        \"\"\"Returns the position fields as a dictionary\n        of framgent index to the respective fields\"\"\"\n        if self._same_chromosome:\n            return {\n                i: [\"chrom\", f\"start_{i}\"] for i in range(1, self._number_fragments + 1)\n            }\n        else:\n            return {\n                i: [f\"chrom_{i}\", f\"start_{i}\"]\n                for i in range(1, self._number_fragments + 1)\n            }\n\n    def get_binsize(self) -&gt; Optional[int]:\n        \"\"\"Returns the binsize of the genomic data\"\"\"\n        return self._binsize\n\n    def get_region_number(self) -&gt; Optional[int]:\n        \"\"\"Returns the number of regions in the genomic data\n        if present.\"\"\"\n        return None\n\n    def get_contact_order(self) -&gt; int:\n        \"\"\"Returns the order of the genomic data\"\"\"\n        return self._number_fragments\n\n    def get_half_window_size(self) -&gt; Optional[int]:\n        \"\"\"Returns the window size of the genomic data\n        if present.\"\"\"\n        return None\n\n    def validate(self, data_frame: DataFrame) -&gt; DataFrame:\n        \"\"\"Validate multiway contact dataframe\n\n        Args:\n            data_frame (DataFrame): The DataFrame to validate.\n\n        \"\"\"\n        self.validate_header(data_frame)\n        if isinstance(data_frame, duckdb.DuckDBPyRelation):\n            # duckdb does not support schema validation\n            return data_frame\n        return self._schema.validate(data_frame)\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.get_binsize","title":"<code>get_binsize()</code>","text":"<p>Returns the binsize of the genomic data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_binsize(self) -&gt; Optional[int]:\n    \"\"\"Returns the binsize of the genomic data\"\"\"\n    return self._binsize\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.get_contact_order","title":"<code>get_contact_order()</code>","text":"<p>Returns the order of the genomic data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_contact_order(self) -&gt; int:\n    \"\"\"Returns the order of the genomic data\"\"\"\n    return self._number_fragments\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.get_half_window_size","title":"<code>get_half_window_size()</code>","text":"<p>Returns the window size of the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_half_window_size(self) -&gt; Optional[int]:\n    \"\"\"Returns the window size of the genomic data\n    if present.\"\"\"\n    return None\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.get_position_fields","title":"<code>get_position_fields()</code>","text":"<p>Returns the position fields as a dictionary of framgent index to the respective fields</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_position_fields(self) -&gt; Dict[int, List[str]]:\n    \"\"\"Returns the position fields as a dictionary\n    of framgent index to the respective fields\"\"\"\n    if self._same_chromosome:\n        return {\n            i: [\"chrom\", f\"start_{i}\"] for i in range(1, self._number_fragments + 1)\n        }\n    else:\n        return {\n            i: [f\"chrom_{i}\", f\"start_{i}\"]\n            for i in range(1, self._number_fragments + 1)\n        }\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.get_region_number","title":"<code>get_region_number()</code>","text":"<p>Returns the number of regions in the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_region_number(self) -&gt; Optional[int]:\n    \"\"\"Returns the number of regions in the genomic data\n    if present.\"\"\"\n    return None\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.get_schema","title":"<code>get_schema()</code>","text":"<p>Get the schema of the DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrameSchema</code> <p>pa.DataFrameSchema: The schema of the DataFrame.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_schema(self) -&gt; pa.DataFrameSchema:\n    \"\"\"\n    Get the schema of the DataFrame.\n\n    Returns:\n        pa.DataFrameSchema: The schema of the DataFrame.\n    \"\"\"\n    return self._schema\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.validate","title":"<code>validate(data_frame)</code>","text":"<p>Validate multiway contact dataframe</p> <p>Parameters:</p> Name Type Description Default <code>data_frame</code> <code>DataFrame</code> <p>The DataFrame to validate.</p> required Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def validate(self, data_frame: DataFrame) -&gt; DataFrame:\n    \"\"\"Validate multiway contact dataframe\n\n    Args:\n        data_frame (DataFrame): The DataFrame to validate.\n\n    \"\"\"\n    self.validate_header(data_frame)\n    if isinstance(data_frame, duckdb.DuckDBPyRelation):\n        # duckdb does not support schema validation\n        return data_frame\n    return self._schema.validate(data_frame)\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.PixelSchema.validate_header","title":"<code>validate_header(data_frame)</code>","text":"<p>Validates only header, needed to validate that dask taskgraph can be built before evaluation</p> <p>Parameters:</p> Name Type Description Default <code>data_frame</code> <code>DataFrame</code> <p>The DataFrame to validate.</p> required Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def validate_header(self, data_frame: DataFrame) -&gt; None:\n    \"\"\"Validates only header, needed to validate that dask taskgraph can be built before\n    evaluation\n\n    Args:\n        data_frame (DataFrame): The DataFrame to validate.\n    \"\"\"\n    for column in data_frame.columns:\n        if column not in self._schema.columns:\n            raise pa.errors.SchemaError(\n                self._schema, data_frame, \"Header is invalid!\"\n            )\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.QueryStepDataSchema","title":"<code>QueryStepDataSchema</code>","text":"<p>Implements GenomicDataSchema for query steps with generic columns</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>class QueryStepDataSchema:\n    \"\"\"Implements GenomicDataSchema for query steps\n    with generic columns\"\"\"\n\n    # pylint: disable=too-many-arguments\n    # arguments needed to define the schema\n    def __init__(\n        self,\n        columns: List[str],\n        position_fields: Dict[int, List[str]],\n        contact_order: int,\n        binsize: Optional[int] = None,\n        region_number: Optional[int] = None,\n        half_window_size: Optional[int] = None,\n    ) -&gt; None:\n        self._columns = columns\n        self._contact_order = contact_order\n        self._position_fields = position_fields\n        self._binsize = binsize\n        self._region_number = region_number\n        self._half_window_size = half_window_size\n        self._schema = pa.DataFrameSchema(\n            {column: pa.Column() for column in columns},\n            coerce=True,\n        )\n\n    def get_position_fields(self) -&gt; Dict[int, List[str]]:\n        \"\"\"\n        Returns the position fields as a dictionary.\n\n        Returns:\n            A dictionary where the keys are integers representing positions\n            and the values are lists of strings representing the fields.\n        \"\"\"\n        return self._position_fields\n\n    def get_contact_order(self) -&gt; int:\n        \"\"\"\n        Returns the contact order of the object.\n        \"\"\"\n        return self._contact_order\n\n    def get_schema(self) -&gt; pa.DataFrameSchema:\n        \"\"\"Return the schema of the underlying data\"\"\"\n        return self._schema\n\n    def get_binsize(self) -&gt; Optional[int]:\n        \"\"\"Returns the binsize of the genomic data\"\"\"\n        return self._binsize\n\n    def get_region_number(self) -&gt; Optional[int]:\n        \"\"\"Returns the number of regions in the genomic data\n        if present.\"\"\"\n        return self._region_number\n\n    def get_half_window_size(self) -&gt; Optional[int]:\n        \"\"\"Returns the half window size of the genomic data\n        if present.\"\"\"\n        return self._half_window_size\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.QueryStepDataSchema.get_binsize","title":"<code>get_binsize()</code>","text":"<p>Returns the binsize of the genomic data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_binsize(self) -&gt; Optional[int]:\n    \"\"\"Returns the binsize of the genomic data\"\"\"\n    return self._binsize\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.QueryStepDataSchema.get_contact_order","title":"<code>get_contact_order()</code>","text":"<p>Returns the contact order of the object.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_contact_order(self) -&gt; int:\n    \"\"\"\n    Returns the contact order of the object.\n    \"\"\"\n    return self._contact_order\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.QueryStepDataSchema.get_half_window_size","title":"<code>get_half_window_size()</code>","text":"<p>Returns the half window size of the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_half_window_size(self) -&gt; Optional[int]:\n    \"\"\"Returns the half window size of the genomic data\n    if present.\"\"\"\n    return self._half_window_size\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.QueryStepDataSchema.get_position_fields","title":"<code>get_position_fields()</code>","text":"<p>Returns the position fields as a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[int, List[str]]</code> <p>A dictionary where the keys are integers representing positions</p> <code>Dict[int, List[str]]</code> <p>and the values are lists of strings representing the fields.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_position_fields(self) -&gt; Dict[int, List[str]]:\n    \"\"\"\n    Returns the position fields as a dictionary.\n\n    Returns:\n        A dictionary where the keys are integers representing positions\n        and the values are lists of strings representing the fields.\n    \"\"\"\n    return self._position_fields\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.QueryStepDataSchema.get_region_number","title":"<code>get_region_number()</code>","text":"<p>Returns the number of regions in the genomic data if present.</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_region_number(self) -&gt; Optional[int]:\n    \"\"\"Returns the number of regions in the genomic data\n    if present.\"\"\"\n    return self._region_number\n</code></pre>"},{"location":"dataframe_models/#spoc.models.dataframe_models.QueryStepDataSchema.get_schema","title":"<code>get_schema()</code>","text":"<p>Return the schema of the underlying data</p> Source code in <code>spoc/models/dataframe_models.py</code> <pre><code>def get_schema(self) -&gt; pa.DataFrameSchema:\n    \"\"\"Return the schema of the underlying data\"\"\"\n    return self._schema\n</code></pre>"},{"location":"file_parameters_models/","title":"File parameter models","text":"<p>This file contains data classes for parameters of spoc data structures</p>"},{"location":"file_parameters_models/#spoc.models.file_parameter_models.ContactsParameters","title":"<code>ContactsParameters</code>","text":"<p>             Bases: <code>GlobalParameters</code></p> <p>Parameters for multiway contacts</p> Source code in <code>spoc/models/file_parameter_models.py</code> <pre><code>class ContactsParameters(GlobalParameters):\n    \"\"\"Parameters for multiway contacts\"\"\"\n\n    @classmethod\n    def get_uri_fields(cls) -&gt; List[str]:\n        \"\"\"Returns the fields that should be included in the URI\"\"\"\n        # Specific parameters needed to enforce order\n        return [\n            \"number_fragments\",\n            \"metadata_combi\",\n            \"binary_labels_equal\",\n            \"symmetry_flipped\",\n            \"label_sorted\",\n        ]\n</code></pre>"},{"location":"file_parameters_models/#spoc.models.file_parameter_models.ContactsParameters.get_uri_fields","title":"<code>get_uri_fields()</code>  <code>classmethod</code>","text":"<p>Returns the fields that should be included in the URI</p> Source code in <code>spoc/models/file_parameter_models.py</code> <pre><code>@classmethod\ndef get_uri_fields(cls) -&gt; List[str]:\n    \"\"\"Returns the fields that should be included in the URI\"\"\"\n    # Specific parameters needed to enforce order\n    return [\n        \"number_fragments\",\n        \"metadata_combi\",\n        \"binary_labels_equal\",\n        \"symmetry_flipped\",\n        \"label_sorted\",\n    ]\n</code></pre>"},{"location":"file_parameters_models/#spoc.models.file_parameter_models.GlobalParameters","title":"<code>GlobalParameters</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Base class for global file parameters</p> Source code in <code>spoc/models/file_parameter_models.py</code> <pre><code>class GlobalParameters(BaseModel):\n    \"\"\"Base class for global file parameters\"\"\"\n\n    number_fragments: Optional[int] = None\n    metadata_combi: Optional[Tuple[str, ...]] = None\n    label_sorted: bool = False\n    binary_labels_equal: bool = False\n    symmetry_flipped: bool = False\n\n    @classmethod\n    def get_uri_fields(cls) -&gt; List[str]:\n        \"\"\"Returns the fields that should be included in the URI\"\"\"\n        raise NotImplementedError\n\n    def __hash__(self) -&gt; int:\n        # get metadata hash\n        return hash(\n            (\n                self.number_fragments,\n                self.metadata_combi,\n                self.label_sorted,\n                self.binary_labels_equal,\n                self.symmetry_flipped,\n            )\n        )\n</code></pre>"},{"location":"file_parameters_models/#spoc.models.file_parameter_models.GlobalParameters.get_uri_fields","title":"<code>get_uri_fields()</code>  <code>classmethod</code>","text":"<p>Returns the fields that should be included in the URI</p> Source code in <code>spoc/models/file_parameter_models.py</code> <pre><code>@classmethod\ndef get_uri_fields(cls) -&gt; List[str]:\n    \"\"\"Returns the fields that should be included in the URI\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"file_parameters_models/#spoc.models.file_parameter_models.PixelParameters","title":"<code>PixelParameters</code>","text":"<p>             Bases: <code>GlobalParameters</code></p> <p>Parameters for genomic pixels</p> Source code in <code>spoc/models/file_parameter_models.py</code> <pre><code>class PixelParameters(GlobalParameters):\n    \"\"\"Parameters for genomic pixels\"\"\"\n\n    binsize: Optional[int] = None\n    same_chromosome: bool = True\n\n    @classmethod\n    def get_uri_fields(cls) -&gt; List[str]:\n        \"\"\"Returns the fields that should be included in the URI\"\"\"\n        # Specific parameters needed to enforce order\n        return [\n            \"number_fragments\",\n            \"binsize\",\n            \"metadata_combi\",\n            \"binary_labels_equal\",\n            \"symmetry_flipped\",\n            \"label_sorted\",\n            \"same_chromosome\",\n        ]\n\n    def __hash__(self) -&gt; int:\n        return hash(\n            (\n                self.number_fragments,\n                self.binsize,\n                self.metadata_combi,\n                self.label_sorted,\n                self.binary_labels_equal,\n                self.symmetry_flipped,\n                self.same_chromosome,\n            )\n        )\n</code></pre>"},{"location":"file_parameters_models/#spoc.models.file_parameter_models.PixelParameters.get_uri_fields","title":"<code>get_uri_fields()</code>  <code>classmethod</code>","text":"<p>Returns the fields that should be included in the URI</p> Source code in <code>spoc/models/file_parameter_models.py</code> <pre><code>@classmethod\ndef get_uri_fields(cls) -&gt; List[str]:\n    \"\"\"Returns the fields that should be included in the URI\"\"\"\n    # Specific parameters needed to enforce order\n    return [\n        \"number_fragments\",\n        \"binsize\",\n        \"metadata_combi\",\n        \"binary_labels_equal\",\n        \"symmetry_flipped\",\n        \"label_sorted\",\n        \"same_chromosome\",\n    ]\n</code></pre>"},{"location":"fragments/","title":"Fragments","text":"<p>This part of spoc is responsible for dealing aligned fragments that have not yet been converted to contacts. It deals with label information as well as expanding fragments to contacts.</p>"},{"location":"fragments/#spoc.fragments.FragmentAnnotator","title":"<code>FragmentAnnotator</code>","text":"<p>Responsible for annotating labels and sister identity of mapped read fragments.</p> <p>Parameters:</p> Name Type Description Default <code>label_library</code> <code>Dict[str, bool]</code> <p>Dictionary containing the label library.</p> required Source code in <code>spoc/fragments.py</code> <pre><code>class FragmentAnnotator:\n    \"\"\"Responsible for annotating labels and sister identity of mapped read fragments.\n\n    Args:\n        label_library (Dict[str, bool]): Dictionary containing the label library.\n    \"\"\"\n\n    def __init__(self, label_library: Dict[str, bool]) -&gt; None:\n        self._label_library = label_library\n\n    def _is_read_labelled(self, read_code: str) -&gt; Union[bool, None]:\n        \"\"\"If a read is in the label dict, return its labeling state.\n        If it is not in there, return None.\"\"\"\n        return self._label_library.get(read_code, None)\n\n    @staticmethod\n    def _assign_sister(data_frame) -&gt; pd.Series:\n        \"\"\"assigns sister identity for a given row\"\"\"\n        return pd.Series(\n            np.select(\n                [data_frame.strand.astype(bool) != data_frame.is_labelled.astype(bool)],\n                [\"SisterA\"],\n                default=\"SisterB\",\n            )\n        )\n\n    def _assign_label_state(self, data_frame: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"helper method that annotates a fragment data frame\"\"\"\n        read_codes = data_frame.read_name.str.cat(\n            [\n                data_frame.chrom,\n                data_frame.start.astype(\"str\"),\n                data_frame.end.astype(\"str\"),\n            ],\n            sep=\"_\",\n        )\n        return pd.Series(read_codes.apply(self._is_read_labelled))\n\n    def annotate_fragments(self, fragments: Fragments) -&gt; Fragments:\n        \"\"\"Takes fragment dataframe and returns a copy of it with its labelling state in a separate\n        column with name `is_labelled`. If drop_uninformative is true, drops fragments that\n        are not in label library.\n\n        Args:\n            fragments (Fragments): Fragments object containing the fragment data.\n\n        Returns:\n            Fragments: Fragments object with annotated fragment data.\n\n        \"\"\"\n        return Fragments(\n            fragments.data.assign(is_labelled=self._assign_label_state)\n            .dropna(subset=[\"is_labelled\"])\n            .assign(metadata=self._assign_sister)\n            .drop(\"is_labelled\", axis=1)\n        )\n</code></pre>"},{"location":"fragments/#spoc.fragments.FragmentAnnotator.annotate_fragments","title":"<code>annotate_fragments(fragments)</code>","text":"<p>Takes fragment dataframe and returns a copy of it with its labelling state in a separate column with name <code>is_labelled</code>. If drop_uninformative is true, drops fragments that are not in label library.</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>Fragments object containing the fragment data.</p> required <p>Returns:</p> Name Type Description <code>Fragments</code> <code>Fragments</code> <p>Fragments object with annotated fragment data.</p> Source code in <code>spoc/fragments.py</code> <pre><code>def annotate_fragments(self, fragments: Fragments) -&gt; Fragments:\n    \"\"\"Takes fragment dataframe and returns a copy of it with its labelling state in a separate\n    column with name `is_labelled`. If drop_uninformative is true, drops fragments that\n    are not in label library.\n\n    Args:\n        fragments (Fragments): Fragments object containing the fragment data.\n\n    Returns:\n        Fragments: Fragments object with annotated fragment data.\n\n    \"\"\"\n    return Fragments(\n        fragments.data.assign(is_labelled=self._assign_label_state)\n        .dropna(subset=[\"is_labelled\"])\n        .assign(metadata=self._assign_sister)\n        .drop(\"is_labelled\", axis=1)\n    )\n</code></pre>"},{"location":"fragments/#spoc.fragments.FragmentExpander","title":"<code>FragmentExpander</code>","text":"<p>Expands n-way fragments over sequencing reads to yield contacts.</p> <p>Parameters:</p> Name Type Description Default <code>number_fragments</code> <code>int</code> <p>Number of fragments.</p> required <code>contains_metadata</code> <code>bool</code> <p>Whether the fragment data contains metadata. Defaults to True.</p> <code>True</code> Source code in <code>spoc/fragments.py</code> <pre><code>class FragmentExpander:\n    \"\"\"Expands n-way fragments over sequencing reads\n    to yield contacts.\n\n    Args:\n        number_fragments (int): Number of fragments.\n        contains_metadata (bool, optional): Whether the fragment data contains metadata. Defaults to True.\n\n    \"\"\"\n\n    def __init__(self, number_fragments: int, contains_metadata: bool = True) -&gt; None:\n        self._number_fragments = number_fragments\n        self._contains_metadata = contains_metadata\n        self._schema = ContactSchema(number_fragments, contains_metadata)\n\n    @staticmethod\n    def _add_suffix(row, suffix: int, contains_metadata: bool) -&gt; Dict:\n        \"\"\"expands contact fields\"\"\"\n        output = {}\n        for key in ContactSchema.get_contact_fields(contains_metadata):\n            output[key + f\"_{suffix}\"] = getattr(row, key)\n        return output\n\n    def _get_expansion_output_structure(self) -&gt; pd.DataFrame:\n        \"\"\"returns expansion output dataframe structure for dask\"\"\"\n        return pd.DataFrame(\n            columns=list(self._schema._schema.columns.keys()) + [\"level_2\"]\n        ).set_index([\"read_name\", \"read_length\", \"level_2\"])\n\n    def _expand_single_read(\n        self, read_df: pd.DataFrame, contains_metadata: bool\n    ) -&gt; pd.DataFrame:\n        \"\"\"Expands a single read\"\"\"\n        if len(read_df) &lt; self._number_fragments:\n            return pd.DataFrame()\n\n        rows = list(\n            read_df.sort_values([\"read_start\"], ascending=True)\n            .assign(pos_on_read=lambda x: np.arange(len(x)))\n            .itertuples()\n        )\n\n        result = []\n        for alignments in combinations(rows, self._number_fragments):\n            contact = {}\n            # add reads\n            for index, align in enumerate(alignments, start=1):\n                contact.update(self._add_suffix(align, index, contains_metadata))\n            result.append(contact)\n        return pd.DataFrame(result)\n\n    def expand(self, fragments: Fragments) -&gt; Contacts:\n        \"\"\"expand contacts n-ways\n\n        Args:\n            fragments (Fragments): Fragments object containing the fragment data.\n\n        Returns:\n            Contacts: Contacts object containing the expanded contact data.\n        \"\"\"\n        # construct dataframe type specific kwargs\n        if fragments.is_dask:\n            kwargs = dict(meta=self._get_expansion_output_structure())\n        else:\n            kwargs = {}\n        # expand\n        contact_df = (\n            fragments.data.groupby([\"read_name\", \"read_length\"])\n            .apply(\n                self._expand_single_read,\n                contains_metadata=fragments.contains_metadata,\n                **kwargs,\n            )\n            .reset_index()\n            .drop(\"level_2\", axis=1)\n        )\n        # return contact_df\n        return Contacts(contact_df, number_fragments=self._number_fragments)\n</code></pre>"},{"location":"fragments/#spoc.fragments.FragmentExpander.expand","title":"<code>expand(fragments)</code>","text":"<p>expand contacts n-ways</p> <p>Parameters:</p> Name Type Description Default <code>fragments</code> <code>Fragments</code> <p>Fragments object containing the fragment data.</p> required <p>Returns:</p> Name Type Description <code>Contacts</code> <code>Contacts</code> <p>Contacts object containing the expanded contact data.</p> Source code in <code>spoc/fragments.py</code> <pre><code>def expand(self, fragments: Fragments) -&gt; Contacts:\n    \"\"\"expand contacts n-ways\n\n    Args:\n        fragments (Fragments): Fragments object containing the fragment data.\n\n    Returns:\n        Contacts: Contacts object containing the expanded contact data.\n    \"\"\"\n    # construct dataframe type specific kwargs\n    if fragments.is_dask:\n        kwargs = dict(meta=self._get_expansion_output_structure())\n    else:\n        kwargs = {}\n    # expand\n    contact_df = (\n        fragments.data.groupby([\"read_name\", \"read_length\"])\n        .apply(\n            self._expand_single_read,\n            contains_metadata=fragments.contains_metadata,\n            **kwargs,\n        )\n        .reset_index()\n        .drop(\"level_2\", axis=1)\n    )\n    # return contact_df\n    return Contacts(contact_df, number_fragments=self._number_fragments)\n</code></pre>"},{"location":"fragments/#spoc.fragments.Fragments","title":"<code>Fragments</code>","text":"<p>Genomic fragments that can be labelled or not.</p> <p>Parameters:</p> Name Type Description Default <code>fragment_frame</code> <code>DataFrame</code> <p>DataFrame containing the fragment data.</p> required Source code in <code>spoc/fragments.py</code> <pre><code>class Fragments:\n    \"\"\"Genomic fragments that can be labelled or not.\n\n    Args:\n        fragment_frame (DataFrame): DataFrame containing the fragment data.\n    \"\"\"\n\n    def __init__(self, fragment_frame: DataFrame) -&gt; None:\n        self._data = FragmentSchema.validate(fragment_frame)\n        self._contains_metadata = \"metadata\" in fragment_frame.columns\n\n    @property\n    def data(self) -&gt; DataFrame:\n        \"\"\"Returns the underlying dataframe.\n\n        Returns:\n            DataFrame: Fragment data.\n        \"\"\"\n        return self._data\n\n    @property\n    def contains_metadata(self) -&gt; bool:\n        \"\"\"Returns whether the dataframe contains metadata.\n\n        Returns:\n            bool: Whether the fragment data contains metadata.\n        \"\"\"\n        return self._contains_metadata\n\n    @property\n    def is_dask(self) -&gt; bool:\n        \"\"\"Returns whether the underlying dataframe is dask dataframe.\n\n        Returns:\n            bool: Whether the underlying dataframe is a dask dataframe.\n        \"\"\"\n        return isinstance(self._data, dd.DataFrame)\n</code></pre>"},{"location":"fragments/#spoc.fragments.Fragments.contains_metadata","title":"<code>contains_metadata: bool</code>  <code>property</code>","text":"<p>Returns whether the dataframe contains metadata.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the fragment data contains metadata.</p>"},{"location":"fragments/#spoc.fragments.Fragments.data","title":"<code>data: DataFrame</code>  <code>property</code>","text":"<p>Returns the underlying dataframe.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Fragment data.</p>"},{"location":"fragments/#spoc.fragments.Fragments.is_dask","title":"<code>is_dask: bool</code>  <code>property</code>","text":"<p>Returns whether the underlying dataframe is dask dataframe.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the underlying dataframe is a dask dataframe.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>The easiest way to install spoc is to use pip:</p> <pre><code>pip install git+https://github.com/gerlichlab/spoc\n</code></pre>"},{"location":"io/","title":"IO","text":"<p>Persisting functionality of spoc that manages writing to and reading from the filesystem.</p>"},{"location":"io/#spoc.io.FileManager","title":"<code>FileManager</code>","text":"<p>Is responsible for loading and writing files</p> <p>Parameters:</p> Name Type Description Default <code>data_mode</code> <code>DataMode</code> <p>Data mode. Defaults to DataMode.PANDAS.</p> <code>PANDAS</code> Source code in <code>spoc/io.py</code> <pre><code>class FileManager:\n    \"\"\"Is responsible for loading and writing files\n\n    Args:\n        data_mode (DataMode, optional): Data mode. Defaults to DataMode.PANDAS.\n    \"\"\"\n\n    def __init__(self, data_mode: DataMode = DataMode.PANDAS) -&gt; None:\n        if data_mode == DataMode.DUCKDB:\n            self._parquet_reader_func = partial(\n                duckdb.read_parquet, connection=DUCKDB_CONNECTION\n            )\n        elif data_mode == DataMode.DASK:\n            self._parquet_reader_func = dd.read_parquet\n        elif data_mode == DataMode.PANDAS:\n            self._parquet_reader_func = pd.read_parquet\n        else:\n            raise ValueError(f\"Data mode {data_mode} not supported!\")\n\n    @staticmethod\n    def write_label_library(path: str, data: Dict[str, bool]) -&gt; None:\n        \"\"\"Writes label library to file\n\n        Args:\n            path (str): Path to write the file to.\n            data (Dict[str, bool]): Label library data.\n\n        Returns:\n            None\n        \"\"\"\n        with open(path, \"wb\") as handle:\n            pickle.dump(data, handle)\n\n    @staticmethod\n    def load_label_library(path: str) -&gt; Dict:\n        \"\"\"Load label library\n\n        Args:\n            path (str): Path to the label library file.\n\n        Returns:\n            Dict: Label library data.\n        \"\"\"\n        with open(path, \"rb\") as handle:\n            label_library = pickle.load(handle)\n        return label_library\n\n    def load_fragments(self, path: str) -&gt; Fragments:\n        \"\"\"Load annotated fragments\n\n        Args:\n            path (str): Path to the fragments file.\n\n        Returns:\n            Fragments: Fragments object containing the fragment data.\n\n        \"\"\"\n        data = self._parquet_reader_func(path)\n        return Fragments(data)\n\n    @staticmethod\n    def write_fragments(path: str, fragments: Fragments) -&gt; None:\n        \"\"\"Write annotated fragments\n\n        Args:\n            path (str): Path to write the file to.\n            fragments (Fragments): Fragments object containing the fragment data.\n\n        Returns:\n            None\n\n        \"\"\"\n        # Write fragments\n        fragments.data.to_parquet(path, row_group_size=1024 * 1024)\n\n    @staticmethod\n    def load_chromosome_sizes(path: str) -&gt; pd.DataFrame:\n        \"\"\"Load chromosome sizes\n\n        Args:\n            path (str): Path to the chromosome sizes file.\n\n        Returns:\n            pd.DataFrame: DataFrame containing the chromosome sizes.\n        \"\"\"\n        # TODO: validate schema for this\n        return pd.read_csv(\n            path,\n            sep=\"\\t\",\n            header=None,\n            names=[\"chrom\", \"size\"],\n            index_col=[\"chrom\"],\n            squeeze=True,\n        )\n\n    @staticmethod\n    def _load_metadata(path: str):\n        \"\"\"Load metadata\"\"\"\n        metadata_path = Path(path) / \"metadata.json\"\n        if metadata_path.exists():\n            with open(metadata_path, \"r\", encoding=\"UTF-8\") as f:\n                metadata = json.load(f)\n        else:\n            raise ValueError(f\"Metadata file not found at {metadata_path}\")\n        return metadata\n\n    @staticmethod\n    def list_pixels(path: str) -&gt; List[PixelParameters]:\n        \"\"\"List available pixels\n\n        Args:\n            path (str): Path to the pixel data.\n\n        Returns:\n            List[PixelParameters]: List of PixelParameters objects.\n        \"\"\"\n        # read metadata.json\n        metadata = FileManager._load_metadata(path)\n        # instantiate pixel parameters\n        pixels = [PixelParameters(**params) for params in metadata.values()]\n        return pixels\n\n    @staticmethod\n    def list_contacts(path: str) -&gt; List[ContactsParameters]:\n        \"\"\"List available contacts\n\n        Args:\n            path (str): Path to the contacts data.\n\n        Returns:\n            List[ContactsParameters]: List of ContactsParameters objects.\n        \"\"\"\n        # read metadata.json\n        metadata = FileManager._load_metadata(path)\n        # instantiate pixel parameters\n        contacts = [ContactsParameters(**params) for params in metadata.values()]\n        return contacts\n\n    def _parse_uri(\n        self, uri: str, uri_parameters: List[str], min_fields: int = 2\n    ) -&gt; Tuple[str, Dict[str, str]]:\n        \"\"\"Parse URI\n\n        Args:\n            uri (str): URI to parse.\n            uri_parameters (List[str]): List of URI parameters.\n            min_fields (int, optional): Minimum number of fields that the URI should contain. Defaults to 2.\n        Returns:\n            Tuple(str, Dict[str, str]): Tuple containing the path and a dictionary of parameters.\n        \"\"\"\n        # parse uri\n        uri_arguments = uri.split(\"::\")\n        # validate uri\n        if len(uri_arguments) &lt; min_fields:\n            raise ValueError(\n                f\"Uri: {uri} is not valid. Must contain at least Path, number_fragments and binsize\"\n            )\n        params = dict(zip(uri_parameters, uri_arguments[1:]))\n        # rewrite metadata_combi parameter\n        if \"metadata_combi\" in params.keys() and params[\"metadata_combi\"] != \"None\":\n            params[\"metadata_combi\"] = str(tuple(params[\"metadata_combi\"]))\n        return uri_arguments[0], params\n\n    def _fuzzy_match_parameters(\n        self,\n        target_parameters: Dict[str, str],\n        candidate_parameters: Dict[str, GlobalParameters],\n    ) -&gt; Tuple[str, GlobalParameters]:\n        \"\"\"Fuzzy match parameters\n\n        Args:\n            target_parameters (Dict[str,str]): Target parameters.\n            candidate_parameters (Dict[str,GlobalParameters]): Candidate parameters.\n        Returns:\n            Tuple[str,GlobalParameters]: Tuple containing the path and a dictionary of parameters.\n        \"\"\"\n        # get fuzzy matched parameters\n        matched_parameters = [\n            (path, param)\n            for path, param in candidate_parameters.items()\n            if all(\n                str(value) == str(param.dict()[key])\n                for key, value in target_parameters.items()\n            )\n        ]\n        # check whether there was a unique match\n        if len(matched_parameters) == 0:\n            raise ValueError(f\"No matches found for parameters: {target_parameters}!\")\n        if len(matched_parameters) &gt; 1:\n            raise ValueError(\n                f\"Multiple matches found for parameters: {target_parameters}!\"\n            )\n        return matched_parameters[0]\n\n    def load_pixels(\n        self, path: str, global_parameters: Optional[PixelParameters] = None\n    ) -&gt; Pixels:\n        \"\"\"Loads specific pixels instance based on global parameters.\n        load_dataframe specifies whether the dataframe should be loaded, or whether pixels\n         should be instantiated based on the path alone.\n\n        Args:\n            path (str): Path to the pixel data.\n            global_parameters (PixelParameters): Global parameters.\n\n        Returns:\n            Pixels: Pixels object containing the pixel data.\n\n        \"\"\"\n        # if global parameters is None, path is assumed to be a uri\n        if global_parameters is None:\n            # parse uri\n            path, parsed_parameters = self._parse_uri(\n                path, PixelParameters.get_uri_fields(), min_fields=3\n            )\n        else:\n            parsed_parameters = global_parameters.dict()\n        # get fuzzy matched parameters\n        metadata = {\n            path: PixelParameters(**values)\n            for path, values in self._load_metadata(path).items()\n        }\n        pixel_path, matched_parameters = self._fuzzy_match_parameters(\n            parsed_parameters, metadata\n        )\n        # rewrite path to contain parent folder\n        full_pixel_path = Path(path) / pixel_path\n        df = self._parquet_reader_func(full_pixel_path)\n        return Pixels(df, **matched_parameters.dict())\n\n    def load_contacts(\n        self, path: str, global_parameters: Optional[ContactsParameters] = None\n    ) -&gt; Contacts:\n        \"\"\"Loads specific contacts instance based on global parameters.\n        load_dataframe specifies whether the dataframe should be loaded\n\n        Args:\n            path (str): Path to the contacts data.\n            global_parameters (ContactsParameters): Global parameters.\n        Returns:\n            Contacts: Contacts object containing the contacts data.\n        \"\"\"\n        # if global parameters is None, path is assumed to be a uri\n        if global_parameters is None:\n            # parse uri\n            path, parsed_parameters = self._parse_uri(\n                path, ContactsParameters.get_uri_fields(), min_fields=2\n            )\n        else:\n            parsed_parameters = global_parameters.dict()\n        # get fuzzy matched parameters\n        metadata = {\n            path: ContactsParameters(**values)\n            for path, values in self._load_metadata(path).items()\n        }\n        contacts_path, matched_parameters = self._fuzzy_match_parameters(\n            parsed_parameters, metadata\n        )\n        # rewrite path to contain parent folder\n        full_contacts_path = Path(path) / contacts_path\n        df = self._parquet_reader_func(full_contacts_path)\n        return Contacts(df, **matched_parameters.dict())\n\n    @staticmethod\n    def _get_object_hash_path(path: str, data_object: Union[Pixels, Contacts]) -&gt; str:\n        hash_string = path + json.dumps(\n            data_object.get_global_parameters().dict(),\n            skipkeys=False,\n            ensure_ascii=True,\n            check_circular=True,\n            allow_nan=True,\n            indent=None,\n            sort_keys=False,\n            separators=None,\n            default=None,\n        )\n        return md5(hash_string.encode(encoding=\"utf-8\")).hexdigest() + \".parquet\"\n\n    def write_pixels(self, path: str, pixels: Pixels) -&gt; None:\n        \"\"\"Write pixels\n\n        Args:\n            path (str): Path to write the pixel data to.\n            pixels (Pixels): Pixels object containing the pixel data.\n\n        Returns:\n            None\n\n        \"\"\"\n        # check whether path exists\n        metadata_path = Path(path) / \"metadata.json\"\n        if not Path(path).exists():\n            # we need to create the directory first\n            os.mkdir(path)\n            current_metadata = {}\n        else:\n            current_metadata = FileManager._load_metadata(path)\n        # create new file path -&gt; hash of directory path and parameters\n        write_path = Path(path) / self._get_object_hash_path(path, pixels)\n        # write pixels\n        if pixels.data is None:\n            raise ValueError(\n                \"Writing pixels only suppported for pixels hodling dataframes!\"\n            )\n        pixels.data.to_parquet(write_path, row_group_size=1024 * 1024)\n        # write metadata\n        current_metadata[write_path.name] = pixels.get_global_parameters().dict()\n        with open(metadata_path, \"w\", encoding=\"UTF-8\") as f:\n            json.dump(current_metadata, f)\n\n    def write_contacts(self, path: str, contacts: Contacts) -&gt; None:\n        \"\"\"Write contacts\"\"\"\n        # check whether path exists\n        metadata_path = Path(path) / \"metadata.json\"\n        if not Path(path).exists():\n            # we need to create the directory first\n            os.mkdir(path)\n            current_metadata = {}\n        else:\n            current_metadata = self._load_metadata(path)\n        # create new file path -&gt; hash of directory path and parameters\n        write_path = Path(path) / self._get_object_hash_path(path, contacts)\n        # write contacts\n        if contacts.data is None:\n            raise ValueError(\n                \"Writing contacts only suppported for contacts hodling dataframes!\"\n            )\n        contacts.data.to_parquet(write_path, row_group_size=1024 * 1024)\n        # write metadata\n        current_metadata[write_path.name] = contacts.get_global_parameters().dict()\n        with open(metadata_path, \"w\") as f:\n            json.dump(current_metadata, f)\n</code></pre>"},{"location":"io/#spoc.io.FileManager.list_contacts","title":"<code>list_contacts(path)</code>  <code>staticmethod</code>","text":"<p>List available contacts</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the contacts data.</p> required <p>Returns:</p> Type Description <code>List[ContactsParameters]</code> <p>List[ContactsParameters]: List of ContactsParameters objects.</p> Source code in <code>spoc/io.py</code> <pre><code>@staticmethod\ndef list_contacts(path: str) -&gt; List[ContactsParameters]:\n    \"\"\"List available contacts\n\n    Args:\n        path (str): Path to the contacts data.\n\n    Returns:\n        List[ContactsParameters]: List of ContactsParameters objects.\n    \"\"\"\n    # read metadata.json\n    metadata = FileManager._load_metadata(path)\n    # instantiate pixel parameters\n    contacts = [ContactsParameters(**params) for params in metadata.values()]\n    return contacts\n</code></pre>"},{"location":"io/#spoc.io.FileManager.list_pixels","title":"<code>list_pixels(path)</code>  <code>staticmethod</code>","text":"<p>List available pixels</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the pixel data.</p> required <p>Returns:</p> Type Description <code>List[PixelParameters]</code> <p>List[PixelParameters]: List of PixelParameters objects.</p> Source code in <code>spoc/io.py</code> <pre><code>@staticmethod\ndef list_pixels(path: str) -&gt; List[PixelParameters]:\n    \"\"\"List available pixels\n\n    Args:\n        path (str): Path to the pixel data.\n\n    Returns:\n        List[PixelParameters]: List of PixelParameters objects.\n    \"\"\"\n    # read metadata.json\n    metadata = FileManager._load_metadata(path)\n    # instantiate pixel parameters\n    pixels = [PixelParameters(**params) for params in metadata.values()]\n    return pixels\n</code></pre>"},{"location":"io/#spoc.io.FileManager.load_chromosome_sizes","title":"<code>load_chromosome_sizes(path)</code>  <code>staticmethod</code>","text":"<p>Load chromosome sizes</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the chromosome sizes file.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the chromosome sizes.</p> Source code in <code>spoc/io.py</code> <pre><code>@staticmethod\ndef load_chromosome_sizes(path: str) -&gt; pd.DataFrame:\n    \"\"\"Load chromosome sizes\n\n    Args:\n        path (str): Path to the chromosome sizes file.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the chromosome sizes.\n    \"\"\"\n    # TODO: validate schema for this\n    return pd.read_csv(\n        path,\n        sep=\"\\t\",\n        header=None,\n        names=[\"chrom\", \"size\"],\n        index_col=[\"chrom\"],\n        squeeze=True,\n    )\n</code></pre>"},{"location":"io/#spoc.io.FileManager.load_contacts","title":"<code>load_contacts(path, global_parameters=None)</code>","text":"<p>Loads specific contacts instance based on global parameters. load_dataframe specifies whether the dataframe should be loaded</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the contacts data.</p> required <code>global_parameters</code> <code>ContactsParameters</code> <p>Global parameters.</p> <code>None</code> <p>Returns:     Contacts: Contacts object containing the contacts data.</p> Source code in <code>spoc/io.py</code> <pre><code>def load_contacts(\n    self, path: str, global_parameters: Optional[ContactsParameters] = None\n) -&gt; Contacts:\n    \"\"\"Loads specific contacts instance based on global parameters.\n    load_dataframe specifies whether the dataframe should be loaded\n\n    Args:\n        path (str): Path to the contacts data.\n        global_parameters (ContactsParameters): Global parameters.\n    Returns:\n        Contacts: Contacts object containing the contacts data.\n    \"\"\"\n    # if global parameters is None, path is assumed to be a uri\n    if global_parameters is None:\n        # parse uri\n        path, parsed_parameters = self._parse_uri(\n            path, ContactsParameters.get_uri_fields(), min_fields=2\n        )\n    else:\n        parsed_parameters = global_parameters.dict()\n    # get fuzzy matched parameters\n    metadata = {\n        path: ContactsParameters(**values)\n        for path, values in self._load_metadata(path).items()\n    }\n    contacts_path, matched_parameters = self._fuzzy_match_parameters(\n        parsed_parameters, metadata\n    )\n    # rewrite path to contain parent folder\n    full_contacts_path = Path(path) / contacts_path\n    df = self._parquet_reader_func(full_contacts_path)\n    return Contacts(df, **matched_parameters.dict())\n</code></pre>"},{"location":"io/#spoc.io.FileManager.load_fragments","title":"<code>load_fragments(path)</code>","text":"<p>Load annotated fragments</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the fragments file.</p> required <p>Returns:</p> Name Type Description <code>Fragments</code> <code>Fragments</code> <p>Fragments object containing the fragment data.</p> Source code in <code>spoc/io.py</code> <pre><code>def load_fragments(self, path: str) -&gt; Fragments:\n    \"\"\"Load annotated fragments\n\n    Args:\n        path (str): Path to the fragments file.\n\n    Returns:\n        Fragments: Fragments object containing the fragment data.\n\n    \"\"\"\n    data = self._parquet_reader_func(path)\n    return Fragments(data)\n</code></pre>"},{"location":"io/#spoc.io.FileManager.load_label_library","title":"<code>load_label_library(path)</code>  <code>staticmethod</code>","text":"<p>Load label library</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the label library file.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>Label library data.</p> Source code in <code>spoc/io.py</code> <pre><code>@staticmethod\ndef load_label_library(path: str) -&gt; Dict:\n    \"\"\"Load label library\n\n    Args:\n        path (str): Path to the label library file.\n\n    Returns:\n        Dict: Label library data.\n    \"\"\"\n    with open(path, \"rb\") as handle:\n        label_library = pickle.load(handle)\n    return label_library\n</code></pre>"},{"location":"io/#spoc.io.FileManager.load_pixels","title":"<code>load_pixels(path, global_parameters=None)</code>","text":"<p>Loads specific pixels instance based on global parameters. load_dataframe specifies whether the dataframe should be loaded, or whether pixels  should be instantiated based on the path alone.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the pixel data.</p> required <code>global_parameters</code> <code>PixelParameters</code> <p>Global parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Pixels</code> <code>Pixels</code> <p>Pixels object containing the pixel data.</p> Source code in <code>spoc/io.py</code> <pre><code>def load_pixels(\n    self, path: str, global_parameters: Optional[PixelParameters] = None\n) -&gt; Pixels:\n    \"\"\"Loads specific pixels instance based on global parameters.\n    load_dataframe specifies whether the dataframe should be loaded, or whether pixels\n     should be instantiated based on the path alone.\n\n    Args:\n        path (str): Path to the pixel data.\n        global_parameters (PixelParameters): Global parameters.\n\n    Returns:\n        Pixels: Pixels object containing the pixel data.\n\n    \"\"\"\n    # if global parameters is None, path is assumed to be a uri\n    if global_parameters is None:\n        # parse uri\n        path, parsed_parameters = self._parse_uri(\n            path, PixelParameters.get_uri_fields(), min_fields=3\n        )\n    else:\n        parsed_parameters = global_parameters.dict()\n    # get fuzzy matched parameters\n    metadata = {\n        path: PixelParameters(**values)\n        for path, values in self._load_metadata(path).items()\n    }\n    pixel_path, matched_parameters = self._fuzzy_match_parameters(\n        parsed_parameters, metadata\n    )\n    # rewrite path to contain parent folder\n    full_pixel_path = Path(path) / pixel_path\n    df = self._parquet_reader_func(full_pixel_path)\n    return Pixels(df, **matched_parameters.dict())\n</code></pre>"},{"location":"io/#spoc.io.FileManager.write_contacts","title":"<code>write_contacts(path, contacts)</code>","text":"<p>Write contacts</p> Source code in <code>spoc/io.py</code> <pre><code>def write_contacts(self, path: str, contacts: Contacts) -&gt; None:\n    \"\"\"Write contacts\"\"\"\n    # check whether path exists\n    metadata_path = Path(path) / \"metadata.json\"\n    if not Path(path).exists():\n        # we need to create the directory first\n        os.mkdir(path)\n        current_metadata = {}\n    else:\n        current_metadata = self._load_metadata(path)\n    # create new file path -&gt; hash of directory path and parameters\n    write_path = Path(path) / self._get_object_hash_path(path, contacts)\n    # write contacts\n    if contacts.data is None:\n        raise ValueError(\n            \"Writing contacts only suppported for contacts hodling dataframes!\"\n        )\n    contacts.data.to_parquet(write_path, row_group_size=1024 * 1024)\n    # write metadata\n    current_metadata[write_path.name] = contacts.get_global_parameters().dict()\n    with open(metadata_path, \"w\") as f:\n        json.dump(current_metadata, f)\n</code></pre>"},{"location":"io/#spoc.io.FileManager.write_fragments","title":"<code>write_fragments(path, fragments)</code>  <code>staticmethod</code>","text":"<p>Write annotated fragments</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to write the file to.</p> required <code>fragments</code> <code>Fragments</code> <p>Fragments object containing the fragment data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spoc/io.py</code> <pre><code>@staticmethod\ndef write_fragments(path: str, fragments: Fragments) -&gt; None:\n    \"\"\"Write annotated fragments\n\n    Args:\n        path (str): Path to write the file to.\n        fragments (Fragments): Fragments object containing the fragment data.\n\n    Returns:\n        None\n\n    \"\"\"\n    # Write fragments\n    fragments.data.to_parquet(path, row_group_size=1024 * 1024)\n</code></pre>"},{"location":"io/#spoc.io.FileManager.write_label_library","title":"<code>write_label_library(path, data)</code>  <code>staticmethod</code>","text":"<p>Writes label library to file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to write the file to.</p> required <code>data</code> <code>Dict[str, bool]</code> <p>Label library data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spoc/io.py</code> <pre><code>@staticmethod\ndef write_label_library(path: str, data: Dict[str, bool]) -&gt; None:\n    \"\"\"Writes label library to file\n\n    Args:\n        path (str): Path to write the file to.\n        data (Dict[str, bool]): Label library data.\n\n    Returns:\n        None\n    \"\"\"\n    with open(path, \"wb\") as handle:\n        pickle.dump(data, handle)\n</code></pre>"},{"location":"io/#spoc.io.FileManager.write_pixels","title":"<code>write_pixels(path, pixels)</code>","text":"<p>Write pixels</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to write the pixel data to.</p> required <code>pixels</code> <code>Pixels</code> <p>Pixels object containing the pixel data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spoc/io.py</code> <pre><code>def write_pixels(self, path: str, pixels: Pixels) -&gt; None:\n    \"\"\"Write pixels\n\n    Args:\n        path (str): Path to write the pixel data to.\n        pixels (Pixels): Pixels object containing the pixel data.\n\n    Returns:\n        None\n\n    \"\"\"\n    # check whether path exists\n    metadata_path = Path(path) / \"metadata.json\"\n    if not Path(path).exists():\n        # we need to create the directory first\n        os.mkdir(path)\n        current_metadata = {}\n    else:\n        current_metadata = FileManager._load_metadata(path)\n    # create new file path -&gt; hash of directory path and parameters\n    write_path = Path(path) / self._get_object_hash_path(path, pixels)\n    # write pixels\n    if pixels.data is None:\n        raise ValueError(\n            \"Writing pixels only suppported for pixels hodling dataframes!\"\n        )\n    pixels.data.to_parquet(write_path, row_group_size=1024 * 1024)\n    # write metadata\n    current_metadata[write_path.name] = pixels.get_global_parameters().dict()\n    with open(metadata_path, \"w\", encoding=\"UTF-8\") as f:\n        json.dump(current_metadata, f)\n</code></pre>"},{"location":"load_example_dataset/","title":"TODO","text":""},{"location":"pixels/","title":"Pixels","text":"<p>This part of spoc is responsible for binned, higher order contacts in the form of 'genomic pixels'</p>"},{"location":"pixels/#spoc.pixels.GenomicBinner","title":"<code>GenomicBinner</code>","text":"<p>Bins higher order contacts into genomic bins of fixed size. Is capable of sorting genomic bins along columns based on sister chromatid identity</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>int</code> <p>The size of the genomic bins.</p> required Source code in <code>spoc/pixels.py</code> <pre><code>class GenomicBinner:\n    \"\"\"Bins higher order contacts into genomic bins of fixed size.\n    Is capable of sorting genomic bins along columns based on sister chromatid\n    identity\n\n    Args:\n        bin_size (int): The size of the genomic bins.\n    \"\"\"\n\n    def __init__(self, bin_size: int) -&gt; None:\n        self._bin_size = bin_size\n\n    def _get_assigned_bin_output_structure(self, contact_order: int):\n        columns = [f\"chrom_{index}\" for index in range(1, contact_order + 1)] + [\n            f\"start_{index}\" for index in range(1, contact_order + 1)\n        ]\n        return pd.DataFrame(columns=columns).astype(int)\n\n    def _assign_bins(\n        self, data_frame: pd.DataFrame, contact_order: int\n    ) -&gt; pd.DataFrame:\n        # capture empty dataframe\n        if data_frame.empty:\n            return self._get_assigned_bin_output_structure(contact_order)\n        return data_frame.assign(\n            **{\n                f\"start_{index}\": (data_frame[f\"pos_{index}\"] // self._bin_size)\n                * self._bin_size\n                for index in range(1, contact_order + 1)\n            }\n        ).filter(regex=\"(chrom|start)\")\n\n    def _assign_midpoints(\n        self, contacts: dd.DataFrame, contact_order: int\n    ) -&gt; dd.DataFrame:\n        \"\"\"Collapses start-end to a middle position\"\"\"\n        return contacts.assign(\n            **{\n                f\"pos_{index}\": (contacts[f\"start_{index}\"] + contacts[f\"end_{index}\"])\n                // 2\n                for index in range(1, contact_order + 1)\n            }\n        ).drop(\n            [\n                c\n                for index in range(1, contact_order + 1)\n                for c in [f\"start_{index}\", f\"end_{index}\"]\n            ],\n            axis=1,\n        )\n\n    def bin_contacts(self, contacts: Contacts, same_chromosome: bool = True) -&gt; Pixels:\n        \"\"\"Bins genomic contacts\n\n        Args:\n            contacts (Contacts): The genomic contacts to bin.\n            same_chromosome (bool, optional): Whether to only retain pixels on the same chromosome. Defaults to True.\n\n        Returns:\n            Pixels: The binned genomic pixels.\n\n        \"\"\"\n        contact_order = contacts.number_fragments\n        contacts_w_midpoints = self._assign_midpoints(contacts.data, contact_order)\n        if contacts.data_mode == DataMode.DASK:\n            contact_bins = contacts_w_midpoints.map_partitions(\n                partial(self._assign_bins, contact_order=contact_order),\n                meta=self._get_assigned_bin_output_structure(contact_order),\n            )\n        elif contacts.data_mode == DataMode.PANDAS:\n            contact_bins = self._assign_bins(contacts_w_midpoints, contact_order)\n        else:\n            raise ValueError(f\"Data mode: {contacts.data_mode} not supported!\")\n        pixels = (\n            contact_bins.groupby(\n                [\n                    c\n                    for index in range(1, contact_order + 1)\n                    for c in [f\"chrom_{index}\", f\"start_{index}\"]\n                ],\n                observed=True,\n            )\n            .size()\n            .reset_index()\n            .rename(columns={0: \"count\"})\n        )\n        # only retain pixels on same chromosome\n        if same_chromosome:\n            pixels = (\n                pixels.loc[\n                    (pixels.chrom_1.astype(str) == pixels.chrom_2.astype(str))\n                    &amp; (pixels.chrom_2.astype(str) == pixels.chrom_3.astype(str))\n                ]\n                .drop(\n                    [f\"chrom_{index}\" for index in range(2, contact_order + 1)],\n                    axis=1,\n                )\n                .rename(columns={\"chrom_1\": \"chrom\"})\n            )\n            # sort pixels\n            pixels_sorted = pixels.sort_values(\n                [\"chrom\"] + [f\"start_{index}\" for index in range(1, contact_order + 1)]\n            ).reset_index(drop=True)\n        else:\n            pixels_sorted = pixels.sort_values(\n                [f\"chrom_{index}\" for index in range(1, contact_order + 1)]\n                + [f\"start_{index}\" for index in range(1, contact_order + 1)]\n            ).reset_index(drop=True)\n        # construct pixels and return\n        return Pixels(\n            pixels_sorted,\n            same_chromosome=same_chromosome,\n            number_fragments=contact_order,\n            binsize=self._bin_size,\n            binary_labels_equal=contacts.binary_labels_equal,\n            symmetry_flipped=contacts.symmetry_flipped,\n            metadata_combi=contacts.metadata_combi,\n        )\n</code></pre>"},{"location":"pixels/#spoc.pixels.GenomicBinner.bin_contacts","title":"<code>bin_contacts(contacts, same_chromosome=True)</code>","text":"<p>Bins genomic contacts</p> <p>Parameters:</p> Name Type Description Default <code>contacts</code> <code>Contacts</code> <p>The genomic contacts to bin.</p> required <code>same_chromosome</code> <code>bool</code> <p>Whether to only retain pixels on the same chromosome. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Pixels</code> <code>Pixels</code> <p>The binned genomic pixels.</p> Source code in <code>spoc/pixels.py</code> <pre><code>def bin_contacts(self, contacts: Contacts, same_chromosome: bool = True) -&gt; Pixels:\n    \"\"\"Bins genomic contacts\n\n    Args:\n        contacts (Contacts): The genomic contacts to bin.\n        same_chromosome (bool, optional): Whether to only retain pixels on the same chromosome. Defaults to True.\n\n    Returns:\n        Pixels: The binned genomic pixels.\n\n    \"\"\"\n    contact_order = contacts.number_fragments\n    contacts_w_midpoints = self._assign_midpoints(contacts.data, contact_order)\n    if contacts.data_mode == DataMode.DASK:\n        contact_bins = contacts_w_midpoints.map_partitions(\n            partial(self._assign_bins, contact_order=contact_order),\n            meta=self._get_assigned_bin_output_structure(contact_order),\n        )\n    elif contacts.data_mode == DataMode.PANDAS:\n        contact_bins = self._assign_bins(contacts_w_midpoints, contact_order)\n    else:\n        raise ValueError(f\"Data mode: {contacts.data_mode} not supported!\")\n    pixels = (\n        contact_bins.groupby(\n            [\n                c\n                for index in range(1, contact_order + 1)\n                for c in [f\"chrom_{index}\", f\"start_{index}\"]\n            ],\n            observed=True,\n        )\n        .size()\n        .reset_index()\n        .rename(columns={0: \"count\"})\n    )\n    # only retain pixels on same chromosome\n    if same_chromosome:\n        pixels = (\n            pixels.loc[\n                (pixels.chrom_1.astype(str) == pixels.chrom_2.astype(str))\n                &amp; (pixels.chrom_2.astype(str) == pixels.chrom_3.astype(str))\n            ]\n            .drop(\n                [f\"chrom_{index}\" for index in range(2, contact_order + 1)],\n                axis=1,\n            )\n            .rename(columns={\"chrom_1\": \"chrom\"})\n        )\n        # sort pixels\n        pixels_sorted = pixels.sort_values(\n            [\"chrom\"] + [f\"start_{index}\" for index in range(1, contact_order + 1)]\n        ).reset_index(drop=True)\n    else:\n        pixels_sorted = pixels.sort_values(\n            [f\"chrom_{index}\" for index in range(1, contact_order + 1)]\n            + [f\"start_{index}\" for index in range(1, contact_order + 1)]\n        ).reset_index(drop=True)\n    # construct pixels and return\n    return Pixels(\n        pixels_sorted,\n        same_chromosome=same_chromosome,\n        number_fragments=contact_order,\n        binsize=self._bin_size,\n        binary_labels_equal=contacts.binary_labels_equal,\n        symmetry_flipped=contacts.symmetry_flipped,\n        metadata_combi=contacts.metadata_combi,\n    )\n</code></pre>"},{"location":"pixels/#spoc.pixels.PixelManipulator","title":"<code>PixelManipulator</code>","text":"<p>Has methods to manipulate pixels such as: - Coarsening - Balancing - Transferring weights</p> Source code in <code>spoc/pixels.py</code> <pre><code>class PixelManipulator:\n    \"\"\"Has methods to manipulate pixels such as:\n    - Coarsening\n    - Balancing\n    - Transferring weights\n    \"\"\"\n</code></pre>"},{"location":"pixels/#spoc.pixels.Pixels","title":"<code>Pixels</code>","text":"<p>Genomic pixels of arbitrary order. Contain information about:     - Bin size     - Symmetry (whether contacts that where used to construct them where symmetric)     - Order     - Metadata combination (Whether the pixels represent a certain combination of metadata)     - Whether binary labels are equal (e.g. whether AB pixels also represent BA pixels)</p> Pixels can contain different data sources such as <ul> <li>pandas dataframe</li> <li>dask dataframe</li> <li>path to a parquet file</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pixel_source</code> <code>Union[DataFrame, DataFrame, str]</code> <p>The source of the pixel data.</p> required <code>number_fragments</code> <code>Optional[int]</code> <p>The number of fragments. Defaults to None.</p> required <code>binsize</code> <code>Optional[int]</code> <p>The bin size. Defaults to None.</p> required <code>metadata_combi</code> <code>Optional[List[str]]</code> <p>The metadata combination. Defaults to None.</p> <code>None</code> <code>label_sorted</code> <code>bool</code> <p>Whether the labels are sorted. Defaults to False.</p> <code>False</code> <code>binary_labels_equal</code> <code>bool</code> <p>Whether binary labels are equal. Defaults to False.</p> <code>False</code> <code>symmetry_flipped</code> <code>bool</code> <p>Whether the pixels are symmetry flipped. Defaults to False.</p> <code>False</code> <code>same_chromosome</code> <code>bool</code> <p>Whether the pixels are on the same chromosome. Defaults to True.</p> <code>True</code> Source code in <code>spoc/pixels.py</code> <pre><code>class Pixels:\n    \"\"\"Genomic pixels of arbitrary order.\n    Contain information about:\n        - Bin size\n        - Symmetry (whether contacts that where used to construct them where symmetric)\n        - Order\n        - Metadata combination (Whether the pixels represent a certain combination of metadata)\n        - Whether binary labels are equal (e.g. whether AB pixels also represent BA pixels)\n\n\n    Pixels can contain different data sources such as:\n        - pandas dataframe\n        - dask dataframe\n        - path to a parquet file\n\n\n    Args:\n        pixel_source (Union[pd.DataFrame, dd.DataFrame, str]): The source of the pixel data.\n        number_fragments (Optional[int], optional): The number of fragments. Defaults to None.\n        binsize (Optional[int], optional): The bin size. Defaults to None.\n        metadata_combi (Optional[List[str]], optional): The metadata combination. Defaults to None.\n        label_sorted (bool, optional): Whether the labels are sorted. Defaults to False.\n        binary_labels_equal (bool, optional): Whether binary labels are equal. Defaults to False.\n        symmetry_flipped (bool, optional): Whether the pixels are symmetry flipped. Defaults to False.\n        same_chromosome (bool, optional): Whether the pixels are on the same chromosome. Defaults to True.\n    \"\"\"\n\n    def __init__(\n        self,\n        pixel_source: DataFrame,\n        number_fragments: int,\n        binsize: int,\n        metadata_combi: Optional[List[str]] = None,\n        label_sorted: bool = False,\n        binary_labels_equal: bool = False,\n        symmetry_flipped: bool = False,\n        same_chromosome: bool = True,\n    ):\n        \"\"\"Constructor for genomic pixels. pixel_source\n        can be a pandas or dask dataframe or a path. Caveat is that\n        if pixels are a path, source data is not validated.\"\"\"\n        self._schema = PixelSchema(\n            number_fragments=number_fragments,\n            same_chromosome=same_chromosome,\n            binsize=binsize,\n        )\n        self._same_chromosome = same_chromosome\n        self._number_fragments = number_fragments\n        self._binsize = binsize\n        self._binary_labels_equal = binary_labels_equal\n        self._symmetry_flipped = symmetry_flipped\n        self._metadata_combi = metadata_combi\n        self._label_sorted = label_sorted\n        # get data mode\n        if isinstance(pixel_source, pd.DataFrame):\n            self.data_mode = DataMode.PANDAS\n        elif isinstance(pixel_source, dd.DataFrame):\n            self.data_mode = DataMode.DASK\n        elif isinstance(pixel_source, duckdb.DuckDBPyRelation):\n            self.data_mode = DataMode.DUCKDB\n        else:\n            raise ValueError(\"Unknown data mode!\")\n        self._data = self._schema.validate(pixel_source)\n\n    @staticmethod\n    def from_uri(uri, mode=DataMode.PANDAS) -&gt; \"Pixels\":\n        \"\"\"Construct pixels from uri.\n        Will match parameters based on the following order:\n\n        PATH::number_fragments::binsize::metadata_combi::binary_labels_equal::symmetry_flipped::label_sorted::same_chromosome\n\n        Path, number_fragments and binsize are required. The rest is optional\n        and will be tried to match to the available pixels. If no match is found, or there is no\n         unique match, an error is raised.\n        Mode can be one of pandas|dask|path, which corresponds to the type of the pixel source.\n\n\n        Args:\n            uri (str): The URI to construct the pixels from.\n            mode (str, optional): The mode to use. Defaults to \"path\".\n\n        Returns:\n            Pixels: The constructed pixels.\n\n        \"\"\"\n        # import here to avoid circular imports\n        # pylint: disable=import-outside-toplevel\n        from spoc.io import FileManager\n\n        return FileManager(mode).load_pixels(uri)\n\n    def get_global_parameters(self) -&gt; PixelParameters:\n        \"\"\"Returns global parameters of pixels\n\n        Returns:\n            PixelParameters: The global parameters of the pixels.\n        \"\"\"\n        return PixelParameters(\n            number_fragments=self._number_fragments,\n            binsize=self._binsize,\n            metadata_combi=self._metadata_combi,\n            label_sorted=self._label_sorted,\n            binary_labels_equal=self._binary_labels_equal,\n            symmetry_flipped=self._symmetry_flipped,\n            same_chromosome=self._same_chromosome,\n        )\n\n    @property\n    def data(self) -&gt; DataFrame:\n        \"\"\"Returns pixels as dataframe\n\n        Returns:\n            DataFrame: The pixels as a dataframe.\n\n        \"\"\"\n        return self._data\n\n    @property\n    def number_fragments(self) -&gt; int:\n        \"\"\"Returns number of fragments in pixels\n\n        Returns:\n            int: The number of fragments in the pixels.\n        \"\"\"\n        return self._number_fragments\n\n    @property\n    def binsize(self) -&gt; int:\n        \"\"\"Returns binsize of pixels\n\n        Returns:\n            int: The binsize of the pixels.\n        \"\"\"\n        return self._binsize\n\n    @property\n    def binary_labels_equal(self) -&gt; bool:\n        \"\"\"Returns whether binary labels are equal\n\n        Returns:\n            bool: Whether binary labels are equal.\n        \"\"\"\n        return self._binary_labels_equal\n\n    @property\n    def symmetry_flipped(self) -&gt; bool:\n        \"\"\"Returns whether pixels are symmetry flipped\n\n        Returns:\n            bool: Whether pixels are symmetry flipped.\n        \"\"\"\n        return self._symmetry_flipped\n\n    @property\n    def metadata_combi(self) -&gt; Optional[List[str]]:\n        \"\"\"Returns metadata combination of pixels\n\n        Returns:\n            Optional[List[str]]: The metadata combination of the pixels.\n        \"\"\"\n        return self._metadata_combi\n\n    @property\n    def same_chromosome(self) -&gt; bool:\n        \"\"\"Returns whether pixels are on same chromosome\n\n\n        Returns:\n            bool: Whether pixels are on same chromosome.\n\n        \"\"\"\n        return self._same_chromosome\n\n    def get_schema(self) -&gt; GenomicDataSchema:\n        \"\"\"Returns the schema of the underlying data\"\"\"\n        return self._schema\n</code></pre>"},{"location":"pixels/#spoc.pixels.Pixels.binary_labels_equal","title":"<code>binary_labels_equal: bool</code>  <code>property</code>","text":"<p>Returns whether binary labels are equal</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether binary labels are equal.</p>"},{"location":"pixels/#spoc.pixels.Pixels.binsize","title":"<code>binsize: int</code>  <code>property</code>","text":"<p>Returns binsize of pixels</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The binsize of the pixels.</p>"},{"location":"pixels/#spoc.pixels.Pixels.data","title":"<code>data: DataFrame</code>  <code>property</code>","text":"<p>Returns pixels as dataframe</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The pixels as a dataframe.</p>"},{"location":"pixels/#spoc.pixels.Pixels.metadata_combi","title":"<code>metadata_combi: Optional[List[str]]</code>  <code>property</code>","text":"<p>Returns metadata combination of pixels</p> <p>Returns:</p> Type Description <code>Optional[List[str]]</code> <p>Optional[List[str]]: The metadata combination of the pixels.</p>"},{"location":"pixels/#spoc.pixels.Pixels.number_fragments","title":"<code>number_fragments: int</code>  <code>property</code>","text":"<p>Returns number of fragments in pixels</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The number of fragments in the pixels.</p>"},{"location":"pixels/#spoc.pixels.Pixels.same_chromosome","title":"<code>same_chromosome: bool</code>  <code>property</code>","text":"<p>Returns whether pixels are on same chromosome</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether pixels are on same chromosome.</p>"},{"location":"pixels/#spoc.pixels.Pixels.symmetry_flipped","title":"<code>symmetry_flipped: bool</code>  <code>property</code>","text":"<p>Returns whether pixels are symmetry flipped</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether pixels are symmetry flipped.</p>"},{"location":"pixels/#spoc.pixels.Pixels.__init__","title":"<code>__init__(pixel_source, number_fragments, binsize, metadata_combi=None, label_sorted=False, binary_labels_equal=False, symmetry_flipped=False, same_chromosome=True)</code>","text":"<p>Constructor for genomic pixels. pixel_source can be a pandas or dask dataframe or a path. Caveat is that if pixels are a path, source data is not validated.</p> Source code in <code>spoc/pixels.py</code> <pre><code>def __init__(\n    self,\n    pixel_source: DataFrame,\n    number_fragments: int,\n    binsize: int,\n    metadata_combi: Optional[List[str]] = None,\n    label_sorted: bool = False,\n    binary_labels_equal: bool = False,\n    symmetry_flipped: bool = False,\n    same_chromosome: bool = True,\n):\n    \"\"\"Constructor for genomic pixels. pixel_source\n    can be a pandas or dask dataframe or a path. Caveat is that\n    if pixels are a path, source data is not validated.\"\"\"\n    self._schema = PixelSchema(\n        number_fragments=number_fragments,\n        same_chromosome=same_chromosome,\n        binsize=binsize,\n    )\n    self._same_chromosome = same_chromosome\n    self._number_fragments = number_fragments\n    self._binsize = binsize\n    self._binary_labels_equal = binary_labels_equal\n    self._symmetry_flipped = symmetry_flipped\n    self._metadata_combi = metadata_combi\n    self._label_sorted = label_sorted\n    # get data mode\n    if isinstance(pixel_source, pd.DataFrame):\n        self.data_mode = DataMode.PANDAS\n    elif isinstance(pixel_source, dd.DataFrame):\n        self.data_mode = DataMode.DASK\n    elif isinstance(pixel_source, duckdb.DuckDBPyRelation):\n        self.data_mode = DataMode.DUCKDB\n    else:\n        raise ValueError(\"Unknown data mode!\")\n    self._data = self._schema.validate(pixel_source)\n</code></pre>"},{"location":"pixels/#spoc.pixels.Pixels.from_uri","title":"<code>from_uri(uri, mode=DataMode.PANDAS)</code>  <code>staticmethod</code>","text":"<p>Construct pixels from uri. Will match parameters based on the following order:</p> <p>PATH::number_fragments::binsize::metadata_combi::binary_labels_equal::symmetry_flipped::label_sorted::same_chromosome</p> <p>Path, number_fragments and binsize are required. The rest is optional and will be tried to match to the available pixels. If no match is found, or there is no  unique match, an error is raised. Mode can be one of pandas|dask|path, which corresponds to the type of the pixel source.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The URI to construct the pixels from.</p> required <code>mode</code> <code>str</code> <p>The mode to use. Defaults to \"path\".</p> <code>PANDAS</code> <p>Returns:</p> Name Type Description <code>Pixels</code> <code>Pixels</code> <p>The constructed pixels.</p> Source code in <code>spoc/pixels.py</code> <pre><code>@staticmethod\ndef from_uri(uri, mode=DataMode.PANDAS) -&gt; \"Pixels\":\n    \"\"\"Construct pixels from uri.\n    Will match parameters based on the following order:\n\n    PATH::number_fragments::binsize::metadata_combi::binary_labels_equal::symmetry_flipped::label_sorted::same_chromosome\n\n    Path, number_fragments and binsize are required. The rest is optional\n    and will be tried to match to the available pixels. If no match is found, or there is no\n     unique match, an error is raised.\n    Mode can be one of pandas|dask|path, which corresponds to the type of the pixel source.\n\n\n    Args:\n        uri (str): The URI to construct the pixels from.\n        mode (str, optional): The mode to use. Defaults to \"path\".\n\n    Returns:\n        Pixels: The constructed pixels.\n\n    \"\"\"\n    # import here to avoid circular imports\n    # pylint: disable=import-outside-toplevel\n    from spoc.io import FileManager\n\n    return FileManager(mode).load_pixels(uri)\n</code></pre>"},{"location":"pixels/#spoc.pixels.Pixels.get_global_parameters","title":"<code>get_global_parameters()</code>","text":"<p>Returns global parameters of pixels</p> <p>Returns:</p> Name Type Description <code>PixelParameters</code> <code>PixelParameters</code> <p>The global parameters of the pixels.</p> Source code in <code>spoc/pixels.py</code> <pre><code>def get_global_parameters(self) -&gt; PixelParameters:\n    \"\"\"Returns global parameters of pixels\n\n    Returns:\n        PixelParameters: The global parameters of the pixels.\n    \"\"\"\n    return PixelParameters(\n        number_fragments=self._number_fragments,\n        binsize=self._binsize,\n        metadata_combi=self._metadata_combi,\n        label_sorted=self._label_sorted,\n        binary_labels_equal=self._binary_labels_equal,\n        symmetry_flipped=self._symmetry_flipped,\n        same_chromosome=self._same_chromosome,\n    )\n</code></pre>"},{"location":"pixels/#spoc.pixels.Pixels.get_schema","title":"<code>get_schema()</code>","text":"<p>Returns the schema of the underlying data</p> Source code in <code>spoc/pixels.py</code> <pre><code>def get_schema(self) -&gt; GenomicDataSchema:\n    \"\"\"Returns the schema of the underlying data\"\"\"\n    return self._schema\n</code></pre>"},{"location":"query_engine/","title":"Query Engine","text":"<p>This file contains the classes making up the query engine.</p>"},{"location":"query_engine/#spoc.query_engine.AggregationFunction","title":"<code>AggregationFunction</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enum for aggregation functions. Options are:     SUM: Sum of values.     AVG_WITH_EMPTY: Average of values, empty values are counted as 0.     AVG: Average of values, empty values are not counted.     COUNT: Number of values.</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class AggregationFunction(Enum):\n    \"\"\"Enum for aggregation functions.\n    Options are:\n        SUM: Sum of values.\n        AVG_WITH_EMPTY: Average of values, empty values are counted as 0.\n        AVG: Average of values, empty values are not counted.\n        COUNT: Number of values.\n    \"\"\"\n\n    SUM: str = \"SUM\"\n    AVG_WITH_EMPTY: str = \"AVG_WITH_EMPTY\"\n    AVG: str = \"AVG\"\n    COUNT: str = \"COUNT\"\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.Anchor","title":"<code>Anchor</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents an anchor.</p> <p>Attributes:</p> Name Type Description <code>mode</code> <code>str</code> <p>The mode of the anchor. (Can be \"ANY\" or \"ALL\")</p> <code>anchors</code> <code>Optional[List[int]]</code> <p>The list of anchor values (optional).</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class Anchor(BaseModel):\n    \"\"\"Represents an anchor.\n\n    Attributes:\n        mode (str): The mode of the anchor. (Can be \"ANY\" or \"ALL\")\n        anchors (Optional[List[int]]): The list of anchor values (optional).\n    \"\"\"\n\n    mode: str\n    anchors: Optional[List[int]] = None\n\n    def __repr__(self) -&gt; str:\n        return f\"Anchor(mode={self.mode}, anchors={self.anchors})\"\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceAggregation","title":"<code>DistanceAggregation</code>","text":"<p>Aggregation based on distances from a region. Uses all available distances.</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class DistanceAggregation:\n    \"\"\"Aggregation based on distances from a region. Uses all available distances.\"\"\"\n\n    def __init__(\n        self,\n        value_column: str,\n        function: Union[AggregationFunction, str] = AggregationFunction.AVG,\n        densify_output: bool = True,\n        position_list: Optional[List[int]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the aggregation.\n\n        Args:\n            value_column (str): The name of the column to be aggregated.\n            function (Union[AggregationFunction,str]): The aggregation function to be applied. Defaults to AggregationFunction.AVG.\n            densify_output (bool, optional): Whether to densify the output. Defaults to True.\n                                             This requires a binsize value to be set in the data schema.\n            position_list (Optional[List[int]]): The list of positions to use for aggregations, starting with 1. Defaults to using all positions.\n        \"\"\"\n        if isinstance(function, str):\n            parsed_function = convert_string_to_enum(AggregationFunction, function)\n        else:\n            parsed_function = function\n        self._function = parsed_function\n        self._value_column = value_column\n        self._densify_output = densify_output\n        self._position_list = position_list\n\n    def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n        \"\"\"Validate the aggregation against the data schema\"\"\"\n        # check that at leastl one distance field is present\n        if \"distance_1\" not in data_schema.get_schema().columns:\n            raise ValueError(\"No distance fields in data schema.\")\n        # check that all position fields are present\n        if self._position_list is not None:\n            for position in self._position_list:\n                if position not in data_schema.get_position_fields():\n                    raise ValueError(f\"Position {position} not in data schema.\")\n        # check that value column is present\n        if self._value_column not in data_schema.get_schema().columns:\n            raise ValueError(\"Value column not in data schema.\")\n        # check for binsize -&gt; only pixels have that\n        if data_schema.get_binsize() is None:\n            raise ValueError(\"No binsize specified in data schema.\")\n        # check for window size\n        if data_schema.get_half_window_size() is None:\n            raise ValueError(\"No window size specified in data schema.\")\n\n    def _get_transformed_schema(\n        self,\n        data_frame: duckdb.DuckDBPyRelation,\n        input_schema: GenomicDataSchema,\n        position_fields: Dict[int, List[str]],\n    ) -&gt; GenomicDataSchema:\n        \"\"\"Returns the schema of the transformed data.\"\"\"\n        # construct schema\n        return QueryStepDataSchema(\n            columns=data_frame.columns,\n            position_fields=position_fields,\n            contact_order=len(position_fields),\n            binsize=input_schema.get_binsize(),\n        )\n\n    def _aggregate_distances(\n        self,\n        data_frame: duckdb.DuckDBPyRelation,\n        input_schema: GenomicDataSchema,\n        position_fields: Dict[int, List[str]],\n    ) -&gt; duckdb.DuckDBPyRelation:\n        \"\"\"Aggregates the distances.\"\"\"\n        # get distance columns\n        distance_columns = [\n            f\"distance_{position}\" for position in position_fields.keys()\n        ]\n        # construct aggregation\n        if self._function == AggregationFunction.COUNT:\n            aggregation_string = (\n                f\"COUNT(*) as {self._value_column}_{self._function.name.lower()}\"\n            )\n        elif self._function == AggregationFunction.AVG_WITH_EMPTY:\n            # For average, we need to sum up the values and divide by the number of regions\n            aggregation_string = f\"SUM({self._value_column})::float/{input_schema.get_region_number()} as {self._value_column}_{self._function.name.lower()}\"\n        else:\n            aggregation_string = f\"{self._function.name}({self._value_column}) as {self._value_column}_{self._function.name.lower()}\"\n        data_frame = (\n            data_frame.set_alias(\"data\")\n            .aggregate(\n                \",\".join(distance_columns + [aggregation_string]),\n            )\n            .order(\",\".join(distance_columns))\n        )\n        return data_frame\n\n    def _create_empty_dense_output(\n        self,\n        input_schema: GenomicDataSchema,\n        position_fields: Dict[int, List[str]],\n    ) -&gt; duckdb.DuckDBPyRelation:\n        \"\"\"Create dense value columns for all distances.\"\"\"\n        binsize: Optional[int] = input_schema.get_binsize()\n        if binsize is None:\n            raise ValueError(\"No binsize specified in data schema.\")\n        int_binsize: int = binsize\n        windowsize: Optional[int] = input_schema.get_half_window_size()\n        if windowsize is None:\n            raise ValueError(\"No window size specified in data schema.\")\n        int_windowsize: int = windowsize\n        # create combinations of distances\n        distance_combinations = pd.DataFrame(\n            product(\n                np.arange(\n                    -(np.floor(int_windowsize / int_binsize) * int_binsize),\n                    (np.floor(int_windowsize / int_binsize) * int_binsize) + 1,\n                    int_binsize,\n                ),\n                repeat=len(position_fields.keys()),\n            ),\n            columns=[f\"distance_{i}\" for i in position_fields.keys()],\n        )\n        # fill value\n        if self._function in (\n            AggregationFunction.COUNT,\n            AggregationFunction.SUM,\n            AggregationFunction.AVG_WITH_EMPTY,\n        ):\n            distance_combinations[\"fill_value\"] = 0\n        else:\n            distance_combinations[\"fill_value\"] = np.nan\n        return duckdb.from_df(distance_combinations, connection=DUCKDB_CONNECTION)\n\n    def _fill_empty_output(\n        self,\n        data_frame: duckdb.DuckDBPyRelation,\n        empty_dense_output: duckdb.DuckDBPyRelation,\n        position_fields: Dict[int, List[str]],\n    ) -&gt; duckdb.DuckDBPyRelation:\n        \"\"\"Fill empty output with values from dense output.\"\"\"\n        # get distance columns\n        distance_columns = [f\"distance_{i}\" for i in position_fields.keys()]\n        # construct join and coalesce output\n        data_frame = (\n            data_frame.set_alias(\"data\")\n            .join(\n                empty_dense_output.set_alias(\"empty_dense_output\"),\n                \",\".join(distance_columns),\n                how=\"right\",\n            )\n            .project(\n                \",\".join(distance_columns)\n                + f\", COALESCE(data.{self._value_column}_{self._function.name.lower()}, empty_dense_output.fill_value) as {self._value_column}\"\n            )\n            .set_alias(\"filled\")\n            .order(\",\".join([f\"filled.{col}\" for col in distance_columns]))\n        )\n        return data_frame\n\n    def __call__(self, genomic_data: GenomicData) -&gt; GenomicData:\n        \"\"\"Apply the aggregation to the data\"\"\"\n        # get input schema\n        input_schema = genomic_data.get_schema()\n        # bring input to duckdb dataframe\n        if isinstance(genomic_data.data, duckdb.DuckDBPyRelation):\n            genomic_df = genomic_data.data\n        else:\n            genomic_df = duckdb.from_df(genomic_data.data, connection=DUCKDB_CONNECTION)\n        # get position columns\n        position_fields = input_schema.get_position_fields()\n        if self._position_list is not None:\n            position_fields = {\n                position: position_fields[position] for position in self._position_list\n            }\n        # construct transformation\n        aggregated_data = self._aggregate_distances(\n            genomic_df, input_schema, position_fields\n        )\n        if self._densify_output:\n            empty_dense_output = self._create_empty_dense_output(\n                input_schema, position_fields\n            )\n            aggregated_data = self._fill_empty_output(\n                aggregated_data, empty_dense_output, position_fields\n            )\n        return QueryPlan(\n            aggregated_data,\n            self._get_transformed_schema(\n                aggregated_data, input_schema, position_fields\n            ),\n        )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceAggregation.__call__","title":"<code>__call__(genomic_data)</code>","text":"<p>Apply the aggregation to the data</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def __call__(self, genomic_data: GenomicData) -&gt; GenomicData:\n    \"\"\"Apply the aggregation to the data\"\"\"\n    # get input schema\n    input_schema = genomic_data.get_schema()\n    # bring input to duckdb dataframe\n    if isinstance(genomic_data.data, duckdb.DuckDBPyRelation):\n        genomic_df = genomic_data.data\n    else:\n        genomic_df = duckdb.from_df(genomic_data.data, connection=DUCKDB_CONNECTION)\n    # get position columns\n    position_fields = input_schema.get_position_fields()\n    if self._position_list is not None:\n        position_fields = {\n            position: position_fields[position] for position in self._position_list\n        }\n    # construct transformation\n    aggregated_data = self._aggregate_distances(\n        genomic_df, input_schema, position_fields\n    )\n    if self._densify_output:\n        empty_dense_output = self._create_empty_dense_output(\n            input_schema, position_fields\n        )\n        aggregated_data = self._fill_empty_output(\n            aggregated_data, empty_dense_output, position_fields\n        )\n    return QueryPlan(\n        aggregated_data,\n        self._get_transformed_schema(\n            aggregated_data, input_schema, position_fields\n        ),\n    )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceAggregation.__init__","title":"<code>__init__(value_column, function=AggregationFunction.AVG, densify_output=True, position_list=None)</code>","text":"<p>Initialize the aggregation.</p> <p>Parameters:</p> Name Type Description Default <code>value_column</code> <code>str</code> <p>The name of the column to be aggregated.</p> required <code>function</code> <code>Union[AggregationFunction, str]</code> <p>The aggregation function to be applied. Defaults to AggregationFunction.AVG.</p> <code>AVG</code> <code>densify_output</code> <code>bool</code> <p>Whether to densify the output. Defaults to True.                              This requires a binsize value to be set in the data schema.</p> <code>True</code> <code>position_list</code> <code>Optional[List[int]]</code> <p>The list of positions to use for aggregations, starting with 1. Defaults to using all positions.</p> <code>None</code> Source code in <code>spoc/query_engine.py</code> <pre><code>def __init__(\n    self,\n    value_column: str,\n    function: Union[AggregationFunction, str] = AggregationFunction.AVG,\n    densify_output: bool = True,\n    position_list: Optional[List[int]] = None,\n) -&gt; None:\n    \"\"\"Initialize the aggregation.\n\n    Args:\n        value_column (str): The name of the column to be aggregated.\n        function (Union[AggregationFunction,str]): The aggregation function to be applied. Defaults to AggregationFunction.AVG.\n        densify_output (bool, optional): Whether to densify the output. Defaults to True.\n                                         This requires a binsize value to be set in the data schema.\n        position_list (Optional[List[int]]): The list of positions to use for aggregations, starting with 1. Defaults to using all positions.\n    \"\"\"\n    if isinstance(function, str):\n        parsed_function = convert_string_to_enum(AggregationFunction, function)\n    else:\n        parsed_function = function\n    self._function = parsed_function\n    self._value_column = value_column\n    self._densify_output = densify_output\n    self._position_list = position_list\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceAggregation.validate","title":"<code>validate(data_schema)</code>","text":"<p>Validate the aggregation against the data schema</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n    \"\"\"Validate the aggregation against the data schema\"\"\"\n    # check that at leastl one distance field is present\n    if \"distance_1\" not in data_schema.get_schema().columns:\n        raise ValueError(\"No distance fields in data schema.\")\n    # check that all position fields are present\n    if self._position_list is not None:\n        for position in self._position_list:\n            if position not in data_schema.get_position_fields():\n                raise ValueError(f\"Position {position} not in data schema.\")\n    # check that value column is present\n    if self._value_column not in data_schema.get_schema().columns:\n        raise ValueError(\"Value column not in data schema.\")\n    # check for binsize -&gt; only pixels have that\n    if data_schema.get_binsize() is None:\n        raise ValueError(\"No binsize specified in data schema.\")\n    # check for window size\n    if data_schema.get_half_window_size() is None:\n        raise ValueError(\"No window size specified in data schema.\")\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceMode","title":"<code>DistanceMode</code>","text":"<p>             Bases: <code>Enum</code></p> <p>Enum for distance modes.</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class DistanceMode(Enum):\n    \"\"\"Enum for distance modes.\"\"\"\n\n    LEFT: str = \"LEFT\"\n    RIGHT: str = \"RIGHT\"\n    BOTH: str = \"BOTH\"\n    MIDPOINT: str = \"MIDPOINT\"\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceTransformation","title":"<code>DistanceTransformation</code>","text":"<p>Adds distance columns for each position field relative to required region_columns.</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class DistanceTransformation:\n    \"\"\"Adds distance columns for each position field relative\n    to required region_columns.\"\"\"\n\n    def __init__(\n        self, distance_mode: Union[DistanceMode, str] = DistanceMode.LEFT\n    ) -&gt; None:\n        \"\"\"Initialize the transformation.\n\n        Args:\n            distance_mode (Union[DistanceMode,str]): The distance mode to be used. Defaults to DistanceMode.MIDPOINT.\n                                      Specifies how the distance is calculated relative to the region midpoint.\n                                      Note that the distance is always calculated relative to the midpoint of the region.\n                                      If a binsize is specificed in the data schema, this needs to be set to LEFT.\n        \"\"\"\n        if isinstance(distance_mode, str):\n            distance_mode = convert_string_to_enum(DistanceMode, distance_mode)\n        self._distance_mode = distance_mode\n\n    def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n        \"\"\"Validate the transformation against the data schema\"\"\"\n        # check that there are position fields and region columns\n        if not data_schema.get_position_fields():\n            raise ValueError(\"No position fields in data schema.\")\n        schema_columns = data_schema.get_schema().columns\n        required_columns = [\"region_chrom\", \"region_start\", \"region_end\"]\n        if not all(column in schema_columns for column in required_columns):\n            raise ValueError(\"No region columns in data schema.\")\n        if (\n            self._distance_mode != DistanceMode.LEFT\n            and data_schema.get_binsize() is not None\n        ):\n            raise ValueError(\n                \"Binsize specified in data schema, but distance mode is not set to LEFT.\"\n            )\n\n    def _create_transform_columns(\n        self, genomic_df: duckdb.DuckDBPyRelation, input_schema: GenomicDataSchema\n    ) -&gt; duckdb.DuckDBPyRelation:\n        \"\"\"Creates the transform columns for the given position fields\"\"\"\n        # position fields\n        position_fields = input_schema.get_position_fields()\n        # get existing columns\n        transform_strings = [f\"data.{column}\" for column in genomic_df.columns]\n        # check whether binsize is specified\n        if input_schema.get_binsize() is not None:\n            binsize = input_schema.get_binsize()\n        else:\n            binsize = 1\n        # create transform columns\n        for position_field, fields in position_fields.items():\n            _, start, end = fields\n            if self._distance_mode == DistanceMode.MIDPOINT:\n                output_string = f\"\"\"(FLOOR((data.{start} + data.{end})/2) - FLOOR((data.region_start + data.region_end)/2))\n                                         as distance_{position_field}\"\"\"\n            if self._distance_mode == DistanceMode.LEFT:\n                output_string = f\"\"\"data.{start} - FLOOR((FLOOR(data.region_start/{binsize}) * {binsize}\n                                    + FLOOR(data.region_end/{binsize}) * {binsize})/2) as distance_{position_field}\"\"\"\n            if self._distance_mode == DistanceMode.RIGHT:\n                output_string = f\"\"\"data.{end} - FLOOR((data.region_start + data.region_end)/2) as distance_{position_field}\"\"\"\n            if self._distance_mode == DistanceMode.BOTH:\n                output_string = f\"\"\"data.{start} - FLOOR((data.region_start + data.region_end)/2) as start_distance_{position_field},\n                                    data.{end} - FLOOR((data.region_start + data.region_end)/2) as end_distance_{position_field}\"\"\"\n            transform_strings.append(output_string)\n        return \",\".join(transform_strings)\n\n    def _get_transformed_schema(\n        self, data_frame: duckdb.DuckDBPyRelation, input_schema: GenomicDataSchema\n    ) -&gt; GenomicDataSchema:\n        \"\"\"Returns the schema of the transformed data.\"\"\"\n        # construct schema\n        return QueryStepDataSchema(\n            columns=data_frame.columns,\n            position_fields=input_schema.get_position_fields(),\n            contact_order=input_schema.get_contact_order(),\n            binsize=input_schema.get_binsize(),\n            region_number=input_schema.get_region_number(),\n            half_window_size=input_schema.get_half_window_size(),\n        )\n\n    def __call__(self, genomic_data: GenomicData) -&gt; GenomicData:\n        \"\"\"Apply the transformation to the data\"\"\"\n        # get input schema\n        input_schema = genomic_data.get_schema()\n        # bring input to duckdb dataframe\n        if isinstance(genomic_data.data, duckdb.DuckDBPyRelation):\n            genomic_df = genomic_data.data\n        else:\n            genomic_df = duckdb.from_df(genomic_data.data, connection=DUCKDB_CONNECTION)\n        # construct transformation\n        transformed_df = genomic_df.set_alias(\"data\").project(\n            self._create_transform_columns(genomic_df, input_schema)\n        )\n        return QueryPlan(\n            transformed_df, self._get_transformed_schema(transformed_df, input_schema)\n        )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceTransformation.__call__","title":"<code>__call__(genomic_data)</code>","text":"<p>Apply the transformation to the data</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def __call__(self, genomic_data: GenomicData) -&gt; GenomicData:\n    \"\"\"Apply the transformation to the data\"\"\"\n    # get input schema\n    input_schema = genomic_data.get_schema()\n    # bring input to duckdb dataframe\n    if isinstance(genomic_data.data, duckdb.DuckDBPyRelation):\n        genomic_df = genomic_data.data\n    else:\n        genomic_df = duckdb.from_df(genomic_data.data, connection=DUCKDB_CONNECTION)\n    # construct transformation\n    transformed_df = genomic_df.set_alias(\"data\").project(\n        self._create_transform_columns(genomic_df, input_schema)\n    )\n    return QueryPlan(\n        transformed_df, self._get_transformed_schema(transformed_df, input_schema)\n    )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceTransformation.__init__","title":"<code>__init__(distance_mode=DistanceMode.LEFT)</code>","text":"<p>Initialize the transformation.</p> <p>Parameters:</p> Name Type Description Default <code>distance_mode</code> <code>Union[DistanceMode, str]</code> <p>The distance mode to be used. Defaults to DistanceMode.MIDPOINT.                       Specifies how the distance is calculated relative to the region midpoint.                       Note that the distance is always calculated relative to the midpoint of the region.                       If a binsize is specificed in the data schema, this needs to be set to LEFT.</p> <code>LEFT</code> Source code in <code>spoc/query_engine.py</code> <pre><code>def __init__(\n    self, distance_mode: Union[DistanceMode, str] = DistanceMode.LEFT\n) -&gt; None:\n    \"\"\"Initialize the transformation.\n\n    Args:\n        distance_mode (Union[DistanceMode,str]): The distance mode to be used. Defaults to DistanceMode.MIDPOINT.\n                                  Specifies how the distance is calculated relative to the region midpoint.\n                                  Note that the distance is always calculated relative to the midpoint of the region.\n                                  If a binsize is specificed in the data schema, this needs to be set to LEFT.\n    \"\"\"\n    if isinstance(distance_mode, str):\n        distance_mode = convert_string_to_enum(DistanceMode, distance_mode)\n    self._distance_mode = distance_mode\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.DistanceTransformation.validate","title":"<code>validate(data_schema)</code>","text":"<p>Validate the transformation against the data schema</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n    \"\"\"Validate the transformation against the data schema\"\"\"\n    # check that there are position fields and region columns\n    if not data_schema.get_position_fields():\n        raise ValueError(\"No position fields in data schema.\")\n    schema_columns = data_schema.get_schema().columns\n    required_columns = [\"region_chrom\", \"region_start\", \"region_end\"]\n    if not all(column in schema_columns for column in required_columns):\n        raise ValueError(\"No region columns in data schema.\")\n    if (\n        self._distance_mode != DistanceMode.LEFT\n        and data_schema.get_binsize() is not None\n    ):\n        raise ValueError(\n            \"Binsize specified in data schema, but distance mode is not set to LEFT.\"\n        )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.GenomicData","title":"<code>GenomicData</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Protocol for genomic data to be used in the query engine</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class GenomicData(Protocol):\n    \"\"\"Protocol for genomic data\n    to be used in the query engine\"\"\"\n\n    @property\n    def data(self) -&gt; Union[pd.DataFrame, duckdb.DuckDBPyRelation, dd.DataFrame]:\n        \"\"\"Return the data in the object\"\"\"\n\n    def get_schema(self) -&gt; GenomicDataSchema:\n        \"\"\"Return the schema of the underlying data\"\"\"\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.GenomicData.data","title":"<code>data: Union[pd.DataFrame, duckdb.DuckDBPyRelation, dd.DataFrame]</code>  <code>property</code>","text":"<p>Return the data in the object</p>"},{"location":"query_engine/#spoc.query_engine.GenomicData.get_schema","title":"<code>get_schema()</code>","text":"<p>Return the schema of the underlying data</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def get_schema(self) -&gt; GenomicDataSchema:\n    \"\"\"Return the schema of the underlying data\"\"\"\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.Overlap","title":"<code>Overlap</code>","text":"<p>This class represents an overlap calculation used for contact and pixel selection. It provides methods to validate the filter against a data schema, convert data to a duckdb relation, construct a filter string, and apply the filter to the data.</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class Overlap:\n    \"\"\"\n    This class represents an overlap calculation used for contact and pixel selection.\n    It provides methods to validate the filter against a data schema,\n    convert data to a duckdb relation, construct a filter string,\n    and apply the filter to the data.\n    \"\"\"\n\n    def __init__(\n        self,\n        regions: pd.DataFrame,\n        anchor_mode: Union[Anchor, Tuple[str, List[int]]],\n        half_window_size: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the Overlap object.\n\n        Args:\n            regions (pd.DataFrame): A DataFrame containing the regions data.\n            anchor_mode (Union[Anchor,Tuple[str,List[int]]]): The anchor mode to be used.\n            half_window_size (Optional[int]): The window size the regions should be expanded to. Defaults to None and is inferred from the data.\n\n        Returns:\n            None\n        \"\"\"\n        # add ids to regions if they don't exist\n        if \"id\" not in regions.columns:\n            regions[\"id\"] = range(len(regions))\n        if half_window_size is not None:\n            expanded_regions = regions.copy()\n            # create midpoint\n            expanded_regions[\"midpoint\"] = (\n                expanded_regions[\"start\"] + expanded_regions[\"end\"]\n            ) // 2\n            # expand regions\n            expanded_regions[\"start\"] = expanded_regions[\"midpoint\"] - half_window_size\n            expanded_regions[\"end\"] = expanded_regions[\"midpoint\"] + half_window_size\n            # drop midpoint\n            expanded_regions = expanded_regions.drop(columns=[\"midpoint\"])\n            self._regions = RegionSchema.validate(\n                expanded_regions.add_prefix(\"region_\")\n            )\n            self._half_window_size = half_window_size\n        else:\n            self._regions = RegionSchema.validate(regions.add_prefix(\"region_\"))\n            # infer window size -&gt; variable regions will have largest possible window size\n            self._half_window_size = int(\n                (self._regions[\"region_end\"] - self._regions[\"region_start\"]).max() // 2\n            )\n        if isinstance(anchor_mode, tuple):\n            self._anchor_mode = Anchor(mode=anchor_mode[0], anchors=anchor_mode[1])\n        else:\n            self._anchor_mode = anchor_mode\n\n    def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n        \"\"\"Validate the filter against the data schema\"\"\"\n        # check whether an anchor is specified that is not in the data\n        if self._anchor_mode.anchors is not None:\n            if not all(\n                anchor in data_schema.get_position_fields().keys()\n                for anchor in self._anchor_mode.anchors\n            ):\n                raise ValueError(\n                    \"An anchor is specified that is not in the data schema.\"\n                )\n\n    def _convert_to_duckdb(\n        self,\n        data: Union[pd.DataFrame, dd.DataFrame],\n    ) -&gt; duckdb.DuckDBPyRelation:\n        \"\"\"\n        Converts the data to a duckdb relation.\n\n        Parameters:\n            data (Union[pd.DataFrame, dd.DataFrame, duckdb.DuckDBPyRelation]): The input data to be converted.\n\n        Returns:\n            duckdb.DuckDBPyRelation: The converted duckdb relation.\n        \"\"\"\n        if isinstance(data, dd.DataFrame):\n            data = data.compute()\n        return duckdb.from_df(data, connection=DUCKDB_CONNECTION)\n\n    def _contstruct_filter(self, position_fields: Dict[int, List[str]]) -&gt; str:\n        \"\"\"Constructs the filter string.\n\n        Args:\n            position_fields (List[str]): List of position fields.\n\n        Returns:\n            str: The constructed filter string.\n\n        Raises:\n            NotImplementedError: If the length of fields is not equal to 3.\n        \"\"\"\n        query_strings = []\n        join_string = \" or \" if self._anchor_mode.mode == \"ANY\" else \" and \"\n        # subset on anchor regions\n        if self._anchor_mode.anchors is not None:\n            subset_positions = [\n                position_fields[anchor] for anchor in self._anchor_mode.anchors\n            ]\n        else:\n            subset_positions = list(position_fields.values())\n        for fields in subset_positions:\n            chrom, start, end = fields\n            output_string = f\"\"\"(data.{chrom} = regions.region_chrom and\n                                    (\n                                        data.{start} between regions.region_start and regions.region_end or \n                                        data.{end} between regions.region_start and regions.region_end or\n                                        regions.region_start between data.{start} and data.{end}\n                                    )\n                                )\"\"\"\n            query_strings.append(output_string)\n        return join_string.join(query_strings)\n\n    def _get_transformed_schema(\n        self,\n        data_frame: duckdb.DuckDBPyRelation,\n        input_schema: GenomicDataSchema,\n        position_fields: Dict[int, List[str]],\n    ) -&gt; GenomicDataSchema:\n        \"\"\"Returns the schema of the transformed data.\"\"\"\n        # construct schema\n        return QueryStepDataSchema(\n            columns=data_frame.columns,\n            position_fields=position_fields,\n            contact_order=input_schema.get_contact_order(),\n            binsize=input_schema.get_binsize(),\n            region_number=len(self._regions),\n            half_window_size=self._half_window_size,\n        )\n\n    def _add_end_position(\n        self,\n        data_frame: duckdb.DuckDBPyRelation,\n        bin_size: Optional[int],\n        position_fields: Dict[int, List[str]],\n    ) -&gt; duckdb.DuckDBPyRelation:\n        \"\"\"Adds an end position column to the dataframe\"\"\"\n        position_columns = {j for i in position_fields.values() for j in i}\n        non_position_columns = [\n            column for column in data_frame.columns if column not in position_columns\n        ]\n        new_position_clause = [\n            f\"data.chrom as chrom_{position}, data.start_{position} as start_{position}, data.start_{position} + {bin_size} as end_{position}\"\n            for position in position_fields.keys()\n        ]\n        # add end position\n        return data_frame.set_alias(\"data\").project(\n            \",\".join(new_position_clause + non_position_columns)\n        )\n\n    def __repr__(self) -&gt; str:\n        return f\"Overlap(anchor_mode={self._anchor_mode})\"\n\n    def __call__(self, genomic_data: GenomicData) -&gt; GenomicData:\n        \"\"\"Apply the filter to the data\"\"\"\n        # get input schema\n        input_schema = genomic_data.get_schema()\n        # bring input to duckdb dataframe\n        if isinstance(genomic_data.data, duckdb.DuckDBPyRelation):\n            genomic_df = genomic_data.data\n        else:\n            genomic_df = self._convert_to_duckdb(genomic_data.data)\n        regions = self._convert_to_duckdb(self._regions)\n        # get position columns and construct filter\n        position_fields = input_schema.get_position_fields()\n        # add end position if not present\n        if len(position_fields[1]) == 2:\n            genomic_df = self._add_end_position(\n                genomic_df, input_schema.get_binsize(), position_fields\n            )\n            position_fields = {\n                position: [f\"chrom_{position}\", f\"start_{position}\", f\"end_{position}\"]\n                for position in position_fields.keys()\n            }\n        # construct query\n        snipped_df = genomic_df.set_alias(\"data\").join(\n            regions.set_alias(\"regions\"), self._contstruct_filter(position_fields)\n        )\n        return QueryPlan(\n            snipped_df,\n            self._get_transformed_schema(snipped_df, input_schema, position_fields),\n        )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.Overlap.__call__","title":"<code>__call__(genomic_data)</code>","text":"<p>Apply the filter to the data</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def __call__(self, genomic_data: GenomicData) -&gt; GenomicData:\n    \"\"\"Apply the filter to the data\"\"\"\n    # get input schema\n    input_schema = genomic_data.get_schema()\n    # bring input to duckdb dataframe\n    if isinstance(genomic_data.data, duckdb.DuckDBPyRelation):\n        genomic_df = genomic_data.data\n    else:\n        genomic_df = self._convert_to_duckdb(genomic_data.data)\n    regions = self._convert_to_duckdb(self._regions)\n    # get position columns and construct filter\n    position_fields = input_schema.get_position_fields()\n    # add end position if not present\n    if len(position_fields[1]) == 2:\n        genomic_df = self._add_end_position(\n            genomic_df, input_schema.get_binsize(), position_fields\n        )\n        position_fields = {\n            position: [f\"chrom_{position}\", f\"start_{position}\", f\"end_{position}\"]\n            for position in position_fields.keys()\n        }\n    # construct query\n    snipped_df = genomic_df.set_alias(\"data\").join(\n        regions.set_alias(\"regions\"), self._contstruct_filter(position_fields)\n    )\n    return QueryPlan(\n        snipped_df,\n        self._get_transformed_schema(snipped_df, input_schema, position_fields),\n    )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.Overlap.__init__","title":"<code>__init__(regions, anchor_mode, half_window_size=None)</code>","text":"<p>Initialize the Overlap object.</p> <p>Parameters:</p> Name Type Description Default <code>regions</code> <code>DataFrame</code> <p>A DataFrame containing the regions data.</p> required <code>anchor_mode</code> <code>Union[Anchor, Tuple[str, List[int]]]</code> <p>The anchor mode to be used.</p> required <code>half_window_size</code> <code>Optional[int]</code> <p>The window size the regions should be expanded to. Defaults to None and is inferred from the data.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def __init__(\n    self,\n    regions: pd.DataFrame,\n    anchor_mode: Union[Anchor, Tuple[str, List[int]]],\n    half_window_size: Optional[int] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the Overlap object.\n\n    Args:\n        regions (pd.DataFrame): A DataFrame containing the regions data.\n        anchor_mode (Union[Anchor,Tuple[str,List[int]]]): The anchor mode to be used.\n        half_window_size (Optional[int]): The window size the regions should be expanded to. Defaults to None and is inferred from the data.\n\n    Returns:\n        None\n    \"\"\"\n    # add ids to regions if they don't exist\n    if \"id\" not in regions.columns:\n        regions[\"id\"] = range(len(regions))\n    if half_window_size is not None:\n        expanded_regions = regions.copy()\n        # create midpoint\n        expanded_regions[\"midpoint\"] = (\n            expanded_regions[\"start\"] + expanded_regions[\"end\"]\n        ) // 2\n        # expand regions\n        expanded_regions[\"start\"] = expanded_regions[\"midpoint\"] - half_window_size\n        expanded_regions[\"end\"] = expanded_regions[\"midpoint\"] + half_window_size\n        # drop midpoint\n        expanded_regions = expanded_regions.drop(columns=[\"midpoint\"])\n        self._regions = RegionSchema.validate(\n            expanded_regions.add_prefix(\"region_\")\n        )\n        self._half_window_size = half_window_size\n    else:\n        self._regions = RegionSchema.validate(regions.add_prefix(\"region_\"))\n        # infer window size -&gt; variable regions will have largest possible window size\n        self._half_window_size = int(\n            (self._regions[\"region_end\"] - self._regions[\"region_start\"]).max() // 2\n        )\n    if isinstance(anchor_mode, tuple):\n        self._anchor_mode = Anchor(mode=anchor_mode[0], anchors=anchor_mode[1])\n    else:\n        self._anchor_mode = anchor_mode\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.Overlap.validate","title":"<code>validate(data_schema)</code>","text":"<p>Validate the filter against the data schema</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n    \"\"\"Validate the filter against the data schema\"\"\"\n    # check whether an anchor is specified that is not in the data\n    if self._anchor_mode.anchors is not None:\n        if not all(\n            anchor in data_schema.get_position_fields().keys()\n            for anchor in self._anchor_mode.anchors\n        ):\n            raise ValueError(\n                \"An anchor is specified that is not in the data schema.\"\n            )\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.Query","title":"<code>Query</code>","text":"<p>Basic query engine that runs a query plan on the data</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class Query:\n    \"\"\"Basic query engine that runs a query plan on the data\"\"\"\n\n    def __init__(self, query_steps: List[QueryStep]) -&gt; None:\n        self._query_steps = query_steps\n\n    def build(self, input_data: GenomicData) -&gt; QueryPlan:\n        \"\"\"Runs the query on the data and returns the result\"\"\"\n        # instantiate query result\n        query_plan = QueryPlan(input_data.data, input_data.get_schema())\n        # run query\n        for step in self._query_steps:\n            # validate schema\n            step.validate(query_plan.get_schema())\n            # apply step\n            query_plan = step(query_plan)\n        return query_plan\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.Query.build","title":"<code>build(input_data)</code>","text":"<p>Runs the query on the data and returns the result</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def build(self, input_data: GenomicData) -&gt; QueryPlan:\n    \"\"\"Runs the query on the data and returns the result\"\"\"\n    # instantiate query result\n    query_plan = QueryPlan(input_data.data, input_data.get_schema())\n    # run query\n    for step in self._query_steps:\n        # validate schema\n        step.validate(query_plan.get_schema())\n        # apply step\n        query_plan = step(query_plan)\n    return query_plan\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.QueryPlan","title":"<code>QueryPlan</code>","text":"<p>Result of a query</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class QueryPlan:\n    \"\"\"Result of a query\"\"\"\n\n    def __init__(\n        self,\n        data: Union[pd.DataFrame, duckdb.DuckDBPyRelation],\n        schema: GenomicDataSchema,\n    ) -&gt; None:\n        self._data = data\n        self._schema = schema\n\n    @property\n    def data(self) -&gt; Union[pd.DataFrame, duckdb.DuckDBPyRelation]:\n        \"\"\"Returns the result as a dataframe object, either in memory or as a relation object\"\"\"\n        return self._data\n\n    def compute(self) -&gt; pd.DataFrame:\n        \"\"\"Loads the result into memory\"\"\"\n        if isinstance(self._data, duckdb.DuckDBPyRelation):\n            return self._data.to_df()\n        return self._data\n\n    def get_schema(self) -&gt; GenomicDataSchema:\n        \"\"\"Returns the schema of the result\"\"\"\n        return self._schema\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.QueryPlan.data","title":"<code>data: Union[pd.DataFrame, duckdb.DuckDBPyRelation]</code>  <code>property</code>","text":"<p>Returns the result as a dataframe object, either in memory or as a relation object</p>"},{"location":"query_engine/#spoc.query_engine.QueryPlan.compute","title":"<code>compute()</code>","text":"<p>Loads the result into memory</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def compute(self) -&gt; pd.DataFrame:\n    \"\"\"Loads the result into memory\"\"\"\n    if isinstance(self._data, duckdb.DuckDBPyRelation):\n        return self._data.to_df()\n    return self._data\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.QueryPlan.get_schema","title":"<code>get_schema()</code>","text":"<p>Returns the schema of the result</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def get_schema(self) -&gt; GenomicDataSchema:\n    \"\"\"Returns the schema of the result\"\"\"\n    return self._schema\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.QueryStep","title":"<code>QueryStep</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Protocol for query steps</p> Source code in <code>spoc/query_engine.py</code> <pre><code>class QueryStep(Protocol):\n    \"\"\"Protocol for query steps\"\"\"\n\n    def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n        \"\"\"Validate the query step against the data schema\"\"\"\n\n    def __call__(self, *args: Any, **kwds: Any) -&gt; \"QueryPlan\":\n        \"\"\"Apply the query step to the data\"\"\"\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.QueryStep.__call__","title":"<code>__call__(*args, **kwds)</code>","text":"<p>Apply the query step to the data</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def __call__(self, *args: Any, **kwds: Any) -&gt; \"QueryPlan\":\n    \"\"\"Apply the query step to the data\"\"\"\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.QueryStep.validate","title":"<code>validate(data_schema)</code>","text":"<p>Validate the query step against the data schema</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def validate(self, data_schema: GenomicDataSchema) -&gt; None:\n    \"\"\"Validate the query step against the data schema\"\"\"\n</code></pre>"},{"location":"query_engine/#spoc.query_engine.convert_string_to_enum","title":"<code>convert_string_to_enum(enum_class, string)</code>","text":"<p>Converts a string to an enum value</p> Source code in <code>spoc/query_engine.py</code> <pre><code>def convert_string_to_enum(enum_class: Callable[[str], T], string: str) -&gt; T:\n    \"\"\"Converts a string to an enum value\"\"\"\n    try:\n        return enum_class(string.upper())\n    except ValueError as exc:\n        raise ValueError(f\"Invalid value for {enum_class.__name__}: {string}\") from exc\n</code></pre>"},{"location":"query_engine_interface/","title":"Interface query","text":"<p>This document defines the high-level interface of the spoc query engine.</p>"},{"location":"query_engine_interface/#class-relationships","title":"Class relationships","text":"<pre><code>classDiagram\n\n    class gDataProtocol{\n        &lt;&lt;Protocol&gt;&gt;\n        +DataFrame data\n        +get_schema(): gDataSchema\n    }\n\n    class gDataSchema{\n        &lt;&lt;Protocol&gt;&gt;\n        +get_position_coordinates(): Maping~int~:~str~\n        +get_contact_order(): int\n        +get_schema(): pandera.DataFrameSchema\n        +get_binsize(): int\n        +get_region_number(): int\n    }\n\n    class QueryStep{\n        &lt;&lt;Protocol&gt;&gt;\n        +validate_schema(schema)\n        -__call__() gDataProtocol\n    }\n\n    class Query\n        Query: +List~QueryStep~ query_plan\n        Query: +compose_with(Query) Query\n        Query: +query(gDataProtocol)\n\n    Query --&gt; QueryResult : query result\n    Query \"1\" --* \"*\" Overlap\n    Query \"1\" --* \"*\" Aggregation\n    Query \"1\" --* \"*\" Transformation\n    gDataProtocol --&gt; Query : takes input\n    gDataProtocol ..|&gt; Pixels : realization\n    gDataProtocol ..|&gt; Contacts : realization\n    gDataProtocol ..|&gt; QueryResult : realization\n    QueryStep ..|&gt; Overlap\n    QueryStep ..|&gt; Transformation\n    QueryStep ..|&gt; Aggregation\n\n\n    class Overlap {\n        +pd.DataFrame~Regions~\n        +String anchor_mode\n        +validate_schema(schema)\n        -__call__(data) gDataProtocol\n    }\n\n\n    class Aggregation{\n        +validate_schema(schema)\n        -__call__() gDataProtocol\n    }\n\n    class Transformation{\n        +validate_schema(schema)\n        -__call__() gDataProtocol\n    }\n\n    class QueryResult\n        QueryResult: +DataFrame data\n        QueryResult: get_schema() pandera.Schema\n        QueryResult: compute() gDataProtocol\n</code></pre>"},{"location":"query_engine_interface/#description","title":"Description","text":"<ul> <li>gDataProtocol: Protocol class that defines the interface of genomic data that can be accepted by <code>Query</code>. Implements a method to get it's schema as well as a parameter to get the underlying data</li> <li>gDataSchema: Schema protocol that incorporates interfaces to getting information about the genomic data.</li> <li>Query: Central query class that encapsulates querying an object that implements the <code>gDataProtocol</code>. Holds references to a query plan, which is a list of filters, aggregations and transformations that are executed in order and specify the filtering, aggregation and transformation operations. Is composable with other basic query instances to capture more complex queries. Performs checks on the proposed operations based on the <code>get_schema()</code> method and the requested filters and aggregations.</li> <li>QueryResult: Result of a Query that implements the <code>gDataProtocol</code> and can either be computed, which manifests the query in memory, or passed to basic query again.</li> <li>Filter: Interface of a filter that is accepted by <code>Query</code> and encapsulates filtering along rows of genomic data.</li> <li>Overlap: A filter that filters for overlap with specific genomic regions that are passed to the constructor. Anchor refers to the way that the genomic regions are overlapped (e.g. at least one, exactly one, all, the first contact etc.)</li> </ul>"},{"location":"query_engine_interface/#conceptual-examples","title":"Conceptual examples","text":"<p>Example pseudocode for selected usecases that are planned to be implemented. For usecases that have already been implemented see the usage description of the query engine.</p>"},{"location":"query_engine_interface/#subsetting-triplets-3-way-contacts-on-multiple-overlaps","title":"Subsetting triplets (3-way contacts) on multiple overlaps","text":"<p>This use cases has the goal of counting the number of cis-sister contacts that form a loop, while simultaneously contacting their respective loop base on the other sister chromatid. This is a very complex use case and can be broken down into smaller steps as follows:</p> <ul> <li>Load a target set of contacts containing the required fragment level information</li> <li>In this case, we are interested in triplets that have their sister identify annotated, and where binary labels have been equated (see data structures page for more info). We are interested both in all cis contacts (AAA labeling state) and triplets containing a trans contact (AAB labeling state) since we want to quantify the number of target triplets amongst all triplets</li> <li>We now split the different labeling state into two analysis flows that run in a paraelel:</li> <li>The case for AAA<ul> <li>We perform multi-region annotation on the entire contacts, where we annotate the contacts on whether they overlap with loop bases (the two regions used are the left and the right loop bases).</li> <li>We filter the contacts on whether they overlap at least with one loop base</li> <li>We add a contact level annotation on whether a loop is present (overlap count =2)</li> <li>We add a contact level annotation that indicates \"no-trans interaction\"</li> </ul> </li> <li>The case for AAB<ul> <li>We perform the same analysis as for AAA, only for the contacts AA.</li> <li>We add a contact level annotation on whether a loop is present (overlap count =2)</li> <li>We add a contact level annotation that indicates \"trans interaction\"</li> </ul> </li> <li>We concatenate the two contacts</li> <li>We aggregate by the contact level annotations on whether a loop is present and whether a a trans-contact has been observed and count the contacts an all the 4 combinations.</li> </ul> <pre><code>from spoc.query_engine import (\n    Overlap,\n    MultiAnchor,\n    Anchor,\n    Query,\n    MultiOverlap,\n    FieldLiteral,\n    Concatenate,\n    Aggregate,\n    AggregationFunction\n)\nfrom spoc.contacts import Contacts\nimport pandas as pd\n\n# load contacts\n\ncis_triplets = Contacts.from_uri(\"contacts::2::AAA\")\ntrans_trans = Contacts.from_uri(\"contacts::2::AAB\")\n\n# load loop bases\n\nloop_bases_left = pd.read_csv('loop_base_left.bed', sep=\"\\t\")\nloop_bases_right = pd.read_csv('loop_base_right.bed', sep=\"\\t\")\n\n# analysis cis_triplets\n\nquery_plan_cis = [\n    MultiOverlap(\n        regions=[loop_bases_left, loop_bases_right], # Regions to overlap\n        add_overlap_columns=False, # Whether to add the regions as additional columns\n        anchor=MultiAnchor(\n            fragment_mode=\"ANY\",\n            region_mode=\"ANY\", positions=[0,1,2]\n        ), # At least one fragment needs to overlap at least one region\n    ),\n    FieldLiteral(field_name=\"is_trans\", value=False)\n]\n\ncis_filtered = Query(query_plan_cis)\\\n                        .build(cis_triplets)\n\n# analysis trans_triplets\n\nquery_plan_trans = [\n    MultiOverlap(\n        regions=[loop_bases_left, loop_bases_right], # Regions to overlap\n        add_overlap_columns=False, # Whether to add the regions as additional columns\n        anchor=MultiAnchor(\n            fragment_mode=\"ANY\",\n            region_mode=\"ANY\", positions=[0,1]\n        ), # At least one fragment needs to overlap at least one region\n    ),\n    FieldLiteral(field_name=\"is_trans\", value=True)\n]\n\ntrans_filtered = Query(query_plan_trans)\\\n                        .build(trans_triplets)\n\n\n# merge contacts and aggregate\n\nquery_plan_aggregate = [\n    Concatenate(trans_filtered),\n    Aggregate(fields=['overlap_count', 'is_trans'], function=AggregationFunction.Count)\n]\n\nresult = Query(query_plan_aggregate)\\\n                  .build(cis_filtered)\\\n                  .compute()\n&gt;\n-----------------------------------\n| overlap_count | is_trans | count |\n|---------------|----------|-------|\n| 1             | 0        | 1000  |\n| 1             | 1        | 5000  |\n| 2             | 0        | 100   |\n| 2             | 1        | 50000 |\n-----------------------------------\n</code></pre>"},{"location":"query_engine_usage/","title":"Query engine","text":"<p>This technical document describes the spoc query engine, a set of classes that implements spoc's interface for querying multi-dimensional genomic data.</p>"},{"location":"query_engine_usage/#principles","title":"Principles","text":""},{"location":"query_engine_usage/#composable-pieces","title":"Composable pieces","text":"<p>Spoc's query engine consists of composable pieces that can be combined to produce an expressive query language. These pieces represent basic operations on genomic data that are easily implemented and understood on their own. This allows a great degree of flexibility, while also allowing predefined recipes that less experienced users can get started with.</p>"},{"location":"query_engine_usage/#lazy-evaluation","title":"Lazy evaluation","text":"<p>The spoc query engine is designed with lazy evaluation as a guiding principle. This means that data queries are only executed when they are needed to minimize loading data into memory and computational overhead. To enable this, spoc queries have a construction phase, which specifies the operations to be executed and an execution phase, that actually executes the query.</p>"},{"location":"query_engine_usage/#query-plans-and-query-steps","title":"Query plans and query steps","text":"<p>The most important ingredient in this query language is a class that implements the <code>QueryStep</code> protocol. This protocol serves two purposes:</p> <ul> <li>It exposes a way to validate the data schema during query building</li> <li>It implements adding itself to a query</li> </ul> <p>This way, query steps can be combined into a query plan that specifies the analysis to be executed. In generaly, spoc supports the following types of query steps:</p> <ul> <li>Overlap: Overlaps genomic data with a set of genomic intervals</li> <li>Aggregation: Aggregates the data passed into the query using an aggregation function</li> <li>Transform: Adds columns to query based on a fixed or custom computation</li> </ul> <p>Specific examples of query steps are:</p> <ul> <li>DistanceTransformation: Example of a transfomration. Adds distance of genomic positions to regions added by Overlap</li> <li>DistanceAggregation: Exaple of an aggregation. Aggregates the distances to genomic regions using an aggregation function.</li> </ul>"},{"location":"query_engine_usage/#input-and-output-of-query-steps","title":"Input and output of query steps","text":"<p>A query step takes as input a class that implements the <code>GenomicData</code> protocol. This protocol allows retrievel of the data schema (a thin wrapper over a pandera dataframe schema) as well as the data itself. The output of a query step is again a class that ipmlements the <code>GenomicData</code> protocol to allow composition. Specific examples of possible inputs are:</p> <ul> <li>Pixels: Represents input pixels</li> <li>Contacts: Represents input contacts</li> <li>QueryPlan: The result of a query step</li> </ul>"},{"location":"query_engine_usage/#composition-of-query-steps","title":"Composition of query steps","text":"<p>To allow specifying complex queries, query steps need to be combined. This is done using the <code>Query</code> class. It takes a query plan (a list of <code>QueryStep</code> instances) as input, exposes the <code>build</code> method, which takes input data, validates all query steps and adds them to the resulting <code>QueryPlan</code> instance that is returned.</p>"},{"location":"query_engine_usage/#manifestation-of-results","title":"Manifestation of results","text":"<p>So far, we have only talked about specifying the query to be executed, but not how to actually execute it. A <code>QueryPlan</code> has a <code>compute()</code> method that returns the manifested dataframe as a <code>pd.DataFrame</code> instance. This is the step that actually executes the specified query.</p>"},{"location":"query_engine_usage/#examples","title":"Examples","text":""},{"location":"query_engine_usage/#selecting-a-subset-of-contacts-at-a-single-genomic-position","title":"Selecting a subset of contacts at a single genomic position","text":"<p>In this example, we want to select a subset of genomic contacts at a single location. For this, we first load the required input data:</p> <pre><code>from spoc.query_engine import Overlap, Anchor, Query\nfrom spoc.contacts import Contacts\nimport pandas as pd\n\ncontacts = Contacts.from_uri(\"../tests/test_files/contacts_unlabelled_2d_v2.parquet::2\")\n</code></pre> <p>Then we specify a target region</p> <pre><code>target_region = pd.DataFrame({\n    \"chrom\": ['chr1'],\n    \"start\": [100],\n    \"end\": [400],\n})\n</code></pre> <p>First, we want to select all contacts where any of the fragments constituting the contact overlaps the target region. To perform this action, we use the Overlap class and pass the target region as well as an instance of the <code>Anchor</code> class. The <code>Anchor</code> dataclass allows us to specify how we want to filter contacts for region overlap. It has two attributes <code>mode</code> and <code>anchors</code>. <code>Anchors</code> indicates the positions we want to filter on (default is all positions) and <code>mode</code> specifies whether we require all positions to overlap or any position to overlap. So for example, if we want all of our two-way contacts for which any of the positions overlap, we would use <code>Anchor(mode='ANY', anchors=[1,2])</code>.</p> <pre><code>query_steps = [\n    Overlap(target_region, anchor_mode=Anchor(mode=\"ANY\", anchors=[1,2]))\n]\n</code></pre> <p>A query plan is a list of qury steps that can be used in the basic query class</p> <pre><code>query = Query(query_steps=query_steps)\n</code></pre> <p>The <code>.build</code> method executes the query plan and retuns a <code>QueryPlan</code> object</p> <pre><code>result = query.build(contacts)\nresult\n</code></pre> <pre><code>&lt;spoc.query_engine.QueryPlan at 0x1804b711a00&gt;\n</code></pre> <p>The <code>.load_result</code> method of the <code>QueryResult</code> object can be executed using <code>.load_result</code>, which returns a <code>pd.DataFrame</code>. The resulting dataframe has additional columns that represent the regions, with which the input contacts overlapped.</p> <pre><code>df = result.compute()\nprint(type(df))\ndf.filter(regex=r\"chrom|start|end|id\")\n</code></pre> <pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;\n</code></pre> chrom_1 start_1 end_1 chrom_2 start_2 end_2 region_chrom region_start region_end region_id 0 chr1 100 200 chr1 1000 2000 chr1 100 400 0 1 chr1 2000 3000 chr1 200 300 chr1 100 400 0 2 chr1 3000 4000 chr1 300 400 chr1 100 400 0 <p>We can also restrict the positions to filter on, by passing different anchor parameters. For example, we can filter for contacts, where the first position overlaps with our target:</p> <pre><code>query_steps = [\n    Overlap(target_region, anchor_mode=Anchor(mode=\"ANY\", anchors=[1]))\n]\nQuery(query_steps=query_steps)\\\n    .build(contacts)\\\n    .compute()\\\n    .filter(regex=r\"chrom|start|end|id\")\n</code></pre> chrom_1 start_1 end_1 chrom_2 start_2 end_2 region_chrom region_start region_end region_id 0 chr1 100 200 chr1 1000 2000 chr1 100 400 0 <p>This time, only the first contact overlaps.</p> <p>The same functionality is implemented also for the Pixels class</p>"},{"location":"query_engine_usage/#selecting-a-subset-of-contacts-at-multiple-genomic-regions","title":"Selecting a subset of contacts at multiple genomic regions","text":"<p>The Overlap class is also capable of selecting contacts at multiple genomic regions. Here, the behavior of <code>Overlap</code> deviates from a simple filter, because if a given contact overlaps with multiple regions, it will be returned multiple times.</p> <p>Specify target regions</p> <pre><code>target_regions = pd.DataFrame({\n    \"chrom\": ['chr1', 'chr1'],\n    \"start\": [100, 150],\n    \"end\": [400, 200],\n})\n</code></pre> <pre><code>query_steps = [\n    Overlap(target_regions, anchor_mode=Anchor(mode=\"ANY\", anchors=[1]))\n]\nQuery(query_steps=query_steps)\\\n    .build(contacts)\\\n    .compute()\\\n    .filter(regex=r\"chrom|start|end|id\")\n</code></pre> chrom_1 start_1 end_1 chrom_2 start_2 end_2 region_chrom region_start region_end region_id 0 chr1 100 200 chr1 1000 2000 chr1 100 400 0 1 chr1 100 200 chr1 1000 2000 chr1 150 200 1 <p>In this example, the contact overlapping both regions is duplicated.</p> <p>The same functionality is implemented also for the pixels class.</p>"},{"location":"query_engine_usage/#calculating-the-distance-to-a-target-region-and-aggregating-the-result","title":"Calculating the distance to a target region and aggregating the result","text":"<p>In this example, we calculate the distance of pixels to target regions and aggregate based on the distances. This is a very common use case in so-called pileup analyses, where we want to investigate the average behavior around regions of interest.</p> <pre><code>from spoc.pixels import Pixels\nfrom spoc.query_engine import DistanceTransformation, DistanceMode\nimport pandas as pd\nimport numpy as np\nfrom itertools import product\n</code></pre> <p>First we define a set of target pixels</p> <pre><code>def complete_synthetic_pixels():\n    \"\"\"Pixels that span two regions densely\"\"\"\n    np.random.seed(42)\n    # genomic region_1\n    pixels_1 = [\n        {\n            \"chrom\": tup[0],\n            \"start_1\": tup[1],\n            \"start_2\": tup[2],\n            \"start_3\": tup[3],\n            \"count\": np.random.randint(0, 10),\n        }\n        for tup in product(\n            [\"chr1\"],\n            np.arange(900_000, 1_150_000, 50_000),\n            np.arange(900_000, 1_150_000, 50_000),\n            np.arange(900_000, 1_150_000, 50_000),\n        )\n    ]\n    # genomic region_2\n    pixels_2 = [\n        {\n            \"chrom\": tup[0],\n            \"start_1\": tup[1],\n            \"start_2\": tup[2],\n            \"start_3\": tup[3],\n            \"count\": np.random.randint(0, 10),\n        }\n        for tup in product(\n            [\"chr2\"],\n            np.arange(900_000, 1_150_000, 50_000),\n            np.arange(900_000, 1_150_000, 50_000),\n            np.arange(900_000, 1_150_000, 50_000),\n        )\n    ]\n    return pd.concat((pd.DataFrame(pixels_1), pd.DataFrame(pixels_2)))\n</code></pre> <pre><code>pixels = Pixels(complete_synthetic_pixels(), number_fragments=3, binsize=50_000)\n</code></pre> <p>Then we define the target regions we are interested in.</p> <pre><code>target_regions = pd.DataFrame(\n        {\n            \"chrom\": [\"chr1\", \"chr2\"],\n            \"start\": [900_000, 900_000],\n            \"end\": [1_100_000, 1_100_000],\n        }\n    )\n</code></pre> <p>We are then interested in selecting all contacts that are contained within these pixels and then calculate the distance to them. The selection step can be done with the <code>Overlap</code> class that we described above. The distance transformation can be done with the <code>DistanceTransformation</code> query step. This query step takes an instance of genomic data that contains regions (as defined by it's schema) and calculates the distance to all position columns. All distances are calculated with regards to the center of each assigned region. Since genomic positions are defined by a start and end,the <code>DistanceTransformation</code> query step has a <code>DistanceMode</code> parameter that defines whether we would like to calculate the distance with regard to the start of a genomic position, the end or it's center.</p> <pre><code>query_steps = [\n    Overlap(target_regions, anchor_mode=Anchor(mode=\"ANY\")),\n    DistanceTransformation(\n        distance_mode=DistanceMode.LEFT,\n    ),\n]\n</code></pre> <p>We can then execute this query plan using the Query class. This well add an distance column to the genomic dataset returned.</p> <pre><code>Query(query_steps=query_steps)\\\n    .build(pixels)\\\n    .compute()\\\n    .filter(regex=r\"chrom|distance\")\n</code></pre> chrom_1 chrom_2 chrom_3 region_chrom distance_1 distance_2 distance_3 0 chr1 chr1 chr1 chr1 -100000.0 -100000.0 -100000.0 1 chr1 chr1 chr1 chr1 -100000.0 -100000.0 -50000.0 2 chr1 chr1 chr1 chr1 -100000.0 -100000.0 0.0 3 chr1 chr1 chr1 chr1 -100000.0 -100000.0 50000.0 4 chr1 chr1 chr1 chr1 -100000.0 -100000.0 100000.0 ... ... ... ... ... ... ... ... 245 chr2 chr2 chr2 chr2 100000.0 100000.0 -100000.0 246 chr2 chr2 chr2 chr2 100000.0 100000.0 -50000.0 247 chr2 chr2 chr2 chr2 100000.0 100000.0 0.0 248 chr2 chr2 chr2 chr2 100000.0 100000.0 50000.0 249 chr2 chr2 chr2 chr2 100000.0 100000.0 100000.0 <p>250 rows \u00d7 7 columns</p>"},{"location":"query_engine_usage/#aggregating-genomic-data-based-on-its-distance-to-a-target-region","title":"Aggregating genomic data based on it's distance to a target region","text":"<p>In this example, we extend the above use-case to aggregate the results based on the distance columns added. This is a common use-case to calculate aggregate statistics for different distance levels. To achieve this, we employ the same query plan as above and extend it using the <code>DistanceAggregation</code> query step.</p> <pre><code>from spoc.query_engine import DistanceAggregation, AggregationFunction\n</code></pre> <p>The <code>DistanceAggregation</code> class requires the following parameters: - <code>value_columns</code>: Thie specifies the value to aggregate - <code>function</code>: The aggregation function to use. This is the enumerated type <code>AggregationFunction</code> - <code>densify_output</code>: Whether missing distance values should be filled with empty values (specific empty value depends on the aggregation function)</p> <p>Note that there are two different average functions available, <code>AVG</code> and <code>AVG_WITH_EMPTY</code>. <code>AVG</code> performs and average over all available columns, where as <code>AVG_WITH_EMPTY</code> counts missing distances per regions as 0.</p> <pre><code>query_steps = [\n    Overlap(target_regions, anchor_mode=Anchor(mode=\"ALL\")),\n    DistanceTransformation(),\n    DistanceAggregation(\n        value_column='count',\n        function=AggregationFunction.AVG,\n    ),\n]\n</code></pre> <pre><code>Query(query_steps=query_steps)\\\n    .build(pixels)\\\n    .compute()\n</code></pre> distance_1 distance_2 distance_3 count 0 -100000.0 -100000.0 -100000.0 4.5 1 -100000.0 -100000.0 -50000.0 3.0 2 -100000.0 -100000.0 0.0 5.5 3 -100000.0 -100000.0 50000.0 5.0 4 -100000.0 -100000.0 100000.0 6.0 ... ... ... ... ... 120 100000.0 100000.0 -100000.0 8.0 121 100000.0 100000.0 -50000.0 4.5 122 100000.0 100000.0 0.0 4.5 123 100000.0 100000.0 50000.0 4.5 124 100000.0 100000.0 100000.0 0.0 <p>125 rows \u00d7 4 columns</p> <p>In addition, we can also aggregate on a subset of distance positions, using the <code>position_list</code> parameter:</p> <pre><code>query_steps = [\n    Overlap(target_regions, anchor_mode=Anchor(mode=\"ALL\")),\n    DistanceTransformation(),\n    DistanceAggregation(\n        value_column='count',\n        function=AggregationFunction.AVG,\n        position_list=[1,2]\n    ),\n]\n</code></pre> <pre><code>Query(query_steps=query_steps)\\\n    .build(pixels)\\\n    .compute()\n</code></pre> distance_1 distance_2 count 0 -100000.0 -100000.0 4.8 1 -100000.0 -50000.0 4.5 2 -100000.0 0.0 5.3 3 -100000.0 50000.0 4.7 4 -100000.0 100000.0 5.3 5 -50000.0 -100000.0 4.8 6 -50000.0 -50000.0 4.4 7 -50000.0 0.0 4.5 8 -50000.0 50000.0 5.4 9 -50000.0 100000.0 3.4 10 0.0 -100000.0 2.0 11 0.0 -50000.0 3.5 12 0.0 0.0 4.4 13 0.0 50000.0 5.4 14 0.0 100000.0 4.3 15 50000.0 -100000.0 5.3 16 50000.0 -50000.0 4.7 17 50000.0 0.0 4.0 18 50000.0 50000.0 4.2 19 50000.0 100000.0 6.1 20 100000.0 -100000.0 5.4 21 100000.0 -50000.0 2.8 22 100000.0 0.0 3.6 23 100000.0 50000.0 5.2 24 100000.0 100000.0 4.3"},{"location":"transformation_pipeline/","title":"TODO","text":""}]}